{"meta":{"title":"Arvon's Blog","subtitle":null,"description":"不要为了看别人而走错了自己脚下的路","author":"Arvon","url":"http://arvon.top"},"pages":[{"title":"","date":"2016-07-29T03:18:23.000Z","updated":"2017-05-27T05:57:59.000Z","comments":false,"path":"about/index.html","permalink":"http://arvon.top/about/index.html","excerpt":"","text":"主人很懒，什么也没有留下。。。"},{"title":"search","date":"2017-05-26T13:17:57.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"search/index.html","permalink":"http://arvon.top/search/index.html","excerpt":"","text":""},{"title":"","date":"2016-07-29T02:54:15.000Z","updated":"2017-05-26T13:17:57.000Z","comments":false,"path":"tags/index.html","permalink":"http://arvon.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Docker备份还原简介","slug":"Docker备份还原简介","date":"2018-03-06T08:50:00.000Z","updated":"2018-03-06T08:59:41.000Z","comments":true,"path":"2018/03/06/Docker备份还原简介/","link":"","permalink":"http://arvon.top/2018/03/06/Docker备份还原简介/","excerpt":"一般而言，使用docker远端镜像仓库的话，基本不需要使用备份还原，但再某些情况下也是需要这些操作的，如对网络受限的主机进行部署或者基于安全考虑对本地仓库进行备份，就需要这些操作，总之Docker的备份还原还是十分简便的，这里也做一下记录。","text":"一般而言，使用docker远端镜像仓库的话，基本不需要使用备份还原，但再某些情况下也是需要这些操作的，如对网络受限的主机进行部署或者基于安全考虑对本地仓库进行备份，就需要这些操作，总之Docker的备份还原还是十分简便的，这里也做一下记录。 备份镜像一般迁移时使用，有图可能更直观，如下,可以使用镜像名+TAG，也可以使用镜像ID12345docker images #list imagesdocker history d86649f09ddb #查看这个镜像的历史，如果有需要可以导出之前版本的镜像docker save -o /data/image_bak.tar docker.taiyouxi.net/ticore/testlink:1.0.1#save usage: docker save -o /down_dir/your_tar_file_name image:TAG#OR：docker save image:TAG &gt; /down_dir/your_tar_file_name 备份容器备份容器跟备份镜像有些细微的差别，首先需要容器commit成为image，如果必要可以对容器的启动配置也进行备份，然后就跟直接对镜像备份一样对新生成的镜像备份即可123docker ps #list Running containerdocker commit $&#123;CONTAINER ID&#125; new_images_name:version #将容器提交为镜像docker rmi image_name:version #删除某个镜像 另外可以备份容器的运行配置1234docker inspect d7796307dfb7 &gt; /data/bak_container_config.json #备份配置#PS：附送命令docker inspect --format='&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;' $INSTANCE_ID #获取IP地址docker inspect --format='&#123;&#123;range $p, $conf := .NetworkSettings.Ports&#125;&#125; &#123;&#123;$p&#125;&#125; -&gt; &#123;&#123;(index $conf 0).HostPort&#125;&#125; &#123;&#123;end&#125;&#125;' $INSTANCE_ID #获取端口映射 还原镜像还原也十分简单，在具备docker环境并运行docker的主机上，执行导入镜像命令即可1234docker load -i /data/image_bak.tar#ORdocker load &lt; /data/image_bak.tardocker images","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://arvon.top/tags/Docker/"}]},{"title":"关于Fluentd使用中filter及ES插件的问题记录","slug":"关于Fluentd使用中filter及ES插件的问题记录","date":"2018-03-01T03:00:00.000Z","updated":"2018-03-07T03:30:20.000Z","comments":true,"path":"2018/03/01/关于Fluentd使用中filter及ES插件的问题记录/","link":"","permalink":"http://arvon.top/2018/03/01/关于Fluentd使用中filter及ES插件的问题记录/","excerpt":"背景介绍：线上需求将业务log数据清洗后导入kibana，原始log也要一同导入，从服务直接输出两份log肯定是不合理的，所以考虑从fluentd收集的时候进行处理，先将原始数据复制为2份，然后一份直接导入kibana，另一份通过fluentd的filter进行过滤筛选后导入kibana。其中filter遇到无法筛选nested类型数据的问题，升级版本后并更改写法后解决；升级后导致ES的index无法按原来的方法命名的问题，通过更换ES插件解决，具体见以下记录。 测试环境：OS: Amazon Linux AMI release 2015.03Fluentd: td-agent 0.12.20","text":"背景介绍：线上需求将业务log数据清洗后导入kibana，原始log也要一同导入，从服务直接输出两份log肯定是不合理的，所以考虑从fluentd收集的时候进行处理，先将原始数据复制为2份，然后一份直接导入kibana，另一份通过fluentd的filter进行过滤筛选后导入kibana。其中filter遇到无法筛选nested类型数据的问题，升级版本后并更改写法后解决；升级后导致ES的index无法按原来的方法命名的问题，通过更换ES插件解决，具体见以下记录。 测试环境：OS: Amazon Linux AMI release 2015.03Fluentd: td-agent 0.12.20 问题1：filter过滤条件不支持nested问题描述 原始数据如下需要两个过滤条件type_name以及info.Type（这是一个nested键值） 1234567891011121314151617181920&#123; \"Level\": \"Error\", \"logtime\": 1519444062811430400, \"@timestamp\": \"2018-02-24T03:47:42.811Z\", \"utc8\": \"2018-02-24 11:47:42\", \"type_name\": \"CostCurrency\", \"gid\": 202, \"sid\": 3258, \"avatar\": \"4\", \"corplvl\": 44, \"channel\": \"130134001232\", \"info\": &#123; \"Reason\": \"AbstractCancelCost\", \"Type\": \"VI_HC\", \"Value\": 20, \"BefValue\": 3139, \"AftValue\": 3119, \"VIP\": 5 &#125;&#125; 首先查找文档找到了filter_grep插件经过测试，regexp1是可以的，但regexp2不行，提了Issues答复是因为这个插件不支持nested，目前已经被弃用了。测试配置如下 1234567891011121314151617181920212223&lt;match logics.**&gt; @type copy &lt;store&gt; @type elasticsearch ... &lt;/store&gt; &lt;store&gt; @type relabel @label @CostCurrency &lt;/store&gt;&lt;/match&gt;&lt;label @CostCurrency&gt; &lt;filter logics.**&gt; @type grep regexp1 type_name CostCurrency regexp2 info.Type VI_HC &lt;/filter&gt; &lt;match logics.**&gt; @type elasticsearch ... &lt;/match&gt;&lt;/label&gt;#td-agent --dry-run -c /etc/td-agent/td-agent.conf #测试配置文件命令 解决方法 升级fluentd版本安装版本&gt;=0.14.19 从这个版本作者支持了nested升级后修改配置文件如下，就可以支持nested过滤了123456789101112&lt;filter logics.**&gt; @type grep #regexp1 type_name CostCurrency #这个在v0.12版本是可用的 &lt;regexp&gt; key type_name pattern CostCurrency &lt;/regexp&gt; &lt;regexp&gt; key $.info.Type pattern ^VI_HC$ &lt;/regexp&gt; &lt;/filter&gt; 问题2：升级v0.14之后无法使用${tag_parts[-1]}这种方式命名index问题描述 目前环境中client TD发来的数据tag类似part1.xxx.part2,我希望拥有相同part1和part2的使用同一个index，所以有了这个需求升级版本之后，之前使用的logstash_dateformat logics-${tag_parts[-1]}.%Y.%m.%d这种方式不生效了，经验证，只能使用${tag}这一种方式，但并达不到我需要的效果，经过查证官方文档,使用elasticsearch_dynamic插件替代原来的elasticsearch插件，就可以正常使用了。不过官方警告如下,所以目前考虑是不是需要更改log收集的思路，目前先按这个执行测试，后续有问题再改进 Please note, this uses Ruby’s eval for every message, so there are performance and security implications. 升级后配置如下 12345678910111213 &lt;store&gt;# @type elasticsearch @type elasticsearch_dynamic host data1.elasticsearch.qa.net port 9200 request_timeout 15s #defaults to 5s reload_connections false reload_on_failure true # defaults to false logstash_format true logstash_prefix bilogs-$&#123;tag_parts[0]&#125;-$&#123;tag_parts[2]&#125; logstash_dateformat %Y.%m.%d time_key time &lt;/store&gt; 完整配置 完整中转配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889&lt;source&gt; @type forward #port 24224&lt;/source&gt;&lt;match debug.**&gt; @type stdout&lt;/match&gt;&lt;source&gt; @type debug_agent bind 127.0.0.1 port 24230&lt;/source&gt;##################################################################################&lt;match logics.**&gt; @type copy #@type forest #subtype copy &lt;store&gt;# @type elasticsearch @type elasticsearch_dynamic host data1.elasticsearch.taiqa.net port 9200 request_timeout 15s #defaults to 5s reload_connections false reload_on_failure true # defaults to false logstash_format true logstash_prefix bilogs-$&#123;tag_parts[0]&#125;-$&#123;tag_parts[2]&#125; logstash_dateformat %Y.%m.%d time_key time &lt;buffer&gt; @type file path /var/log/td-agent/buffer/td-gamex-buffer chunk_limit_size 512MB #Default: 8MB (memory) / 256MB (file) total_limit_size 32GB #Default: 512MB (memory) / 64GB (file) chunk_full_threshold 0.9 #output plugin will flush the chunk when actual size reaches chunk_limit_size * chunk_full_threshold compress text #The option to specify compression of each chunks, during events are buffered flush_mode default flush_interval 15s #Default: 60s flush_thread_count 1 #Default: 1 The number of threads of output plugins, which is used to write chunks in parallel delayed_commit_timeout 60 #The timeout seconds until output plugin decides that async write operation fails overflow_action throw_exception retry_timeout 10m &lt;/buffer&gt; &lt;/store&gt; &lt;store&gt; @type relabel @label @CostCurrency &lt;/store&gt;&lt;/match&gt;&lt;label @CostCurrency&gt; &lt;filter logics.**&gt; @type grep #regexp1 type_name CostCurrency &lt;regexp&gt; key type_name pattern CostCurrency &lt;/regexp&gt; &lt;regexp&gt; key $.info.Type pattern ^VI_HC$ &lt;/regexp&gt; &lt;/filter&gt; &lt;match logics.**&gt; @type elasticsearch host data1.elasticsearch.taiqa.net port 9200 request_timeout 15s #defaults to 5s reload_connections false reload_on_failure true # defaults to false logstash_format true logstash_prefix cost logstash_dateformat currency-hc.%Y.%m.%d time_key time &lt;buffer&gt; @type file path /var/log/td-agent/buffer/td-cost-buffer chunk_limit_size 512MB #Default: 8MB (memory) / 256MB (file) total_limit_size 32GB #Default: 512MB (memory) / 64GB (file) chunk_full_threshold 0.9 #output plugin will flush the chunk when actual size reaches chunk_limit_size * chunk_full_threshold compress text #The option to specify compression of each chunks, during events are buffered flush_mode default flush_interval 15s #Default: 60s flush_thread_count 1 #Default: 1 The number of threads of output plugins, which is used to write chunks in parallel delayed_commit_timeout 60 #The timeout seconds until output plugin decides that async write operation fails overflow_action throw_exception retry_timeout 10m &lt;/buffer&gt; &lt;/match&gt;&lt;/label&gt; 参考文档关于tag或字段rewriteES插件文档","categories":[],"tags":[{"name":"踩坑指南","slug":"踩坑指南","permalink":"http://arvon.top/tags/踩坑指南/"},{"name":"ELK","slug":"ELK","permalink":"http://arvon.top/tags/ELK/"}]},{"title":"Td-agent配置说明","slug":"Td-agent配置说明","date":"2018-02-23T13:00:00.000Z","updated":"2018-02-23T08:14:41.000Z","comments":true,"path":"2018/02/23/Td-agent配置说明/","link":"","permalink":"http://arvon.top/2018/02/23/Td-agent配置说明/","excerpt":"简介：生产环境并没有使用传统ELK,而是使用tdagent来代替Logstash作日志收集。关于td-agent和Fluentd的关系可以引用官网的描述：&quot;In one word, td-agent is a stable distribution package of Fluentd.&quot; update：2017-05-20 初次修改 一般架构","text":"简介：生产环境并没有使用传统ELK,而是使用tdagent来代替Logstash作日志收集。关于td-agent和Fluentd的关系可以引用官网的描述：&quot;In one word, td-agent is a stable distribution package of Fluentd.&quot; update：2017-05-20 初次修改 一般架构 特性记录 版本2017 年 12 月的时候，fluentd 发布了 v1.0 版本，也就是 td-agent v3 版。 性能“a regular PC box can handle 18,000 messages/second with a single process.”即一般来说，fluentd 单节点的吞吐量大概是 18w/sec 左右。要想提高性能的话，可以在输出端（match）指定 num_threads 来提高并发，在输入端安装 fluent-plugin-multiprocess 插件来提高 CPU 的利用率（Ruby 也有 GIL 问题）。 指令说明 指令 功能 source 决定从哪里读取日志，关键字type指定启用插件后配置相关参数 match 设定当满足指定条件时如何处理日志的方法，在source指令追加的标签(tag)满足match指令的条件时，该日志将被指定插件处理。定义输出的目标，如写入文件，或者发送到指定 filter 过滤，也即事件处理流水线，可在输入和输出之间运行 system 系统级别的设置 label 定义一组操作，实现复用及内部路由 @include 引入其他文件，和python的import类似 source指令 sourceTips:每个 source 指令必须包括 “type” 参数，指定使用那种插件 Routing（路由）：source 把事件提交到 fluentd 的路由引擎中。一个事件由三个实体组成：tag、time 和 record。tag：是一个通过 “.” 来分离的字符串（e.g. myapp.access），用作 Fluentd 内部路由引擎的方向time：时间字段由输入插件指定，并且必须为 Unix 时间格式。record：一个 JSON 对象。 1234567891011121314151617181920&lt;source&gt; type forward #使 fluentd 转变为一个 TCP 端点，以接受 TCP 报文，监听24224端口 port 24224&lt;/source&gt;&lt;source&gt; @type http #使fluentd转变为一个httpd端点以接受进入的 http 报文,监听7777端口 port 7777&lt;/source&gt;#可以使用curl -X POST -d 'json=&#123;\"json\":\"message\"&#125;' http://localhost:7777/debug.test测试，可以在/var/log/td-agent/td-agent.log看到输入内容,这个例子中tag就是debug.test，时间就是current time，record就是&#123;\"json\":\"message\"&#125;，这个url还可以写成http://localhost:7777/debug.test?json=&#123;\"json\":\"message\"&#125;&lt;source&gt; type tail #tail方式是 Fluentd 内置的输入方式，其原理是不停地从源文件中获取增量日志，与linx命令tail相似，也可以使用其他输入方式如http、forward等输入，也可以使用输入插件，将 tail 改为相应的插件名称 如： type tail_ex format json #指定json格式解析，也可使用apache格式（apache为fluentd内置的日志解析器） time_key time time_format %N pos_file /var/log/td-agent/logics_5001.log.pos #优化参数（将access_log上次的读取长度写入到该文件，主要保证在fluentd服务宕机重启后能够继续收集，避免日志数据收集丢失，保证数据收集的完整性），注意此文件的权限 path /opt/supervisor/log/logics_shard5001.%d.%m.%Y.log #指定收集日志文件的位置 tag logics.5001.205 #指定标签，用来对不同的日志进行分类，与match操作相匹配&lt;/source&gt; match指令 matchTips: match指令查询匹配tags事件并处理他们。match 命令的最常见用法是将事件输出到其他系统（因此，与 match 命令对应的插件称为 “输出插件”）。 Fluentd 的标准输出插件包括 file 和 forward。每个 match 指令必须包括一个匹配模式和 type 参数。只有与模式匹配的 “tags” 事件才会发送到输出目标（在上面的示例中，只有标记 “myapp.access” 的事件匹配），Fluentd 尝试按照它们在配置文件中出现的顺序，从上到下来进行 “tags” 匹配，如上一条已经匹配那么下面的将不会被匹配 。type 参数指定使用哪种输出插件 *：匹配单个 tag 部分。例：a.，匹配 a.b，但不匹配 a 或者 a.b.c**：匹配 0 或 多个 tag 部分。例：a.**，匹配 a、a.b 和 a.b.c{X,Y,Z}：匹配 X、Y 或 Z，其中 X、Y 和 Z 是匹配模式。可以和 和 ** 模式组合使用当多个模式列在一个 标签（由一个或多个空格分隔）内时，它匹配任何列出的模式 12345678910111213141516171819202122232425&lt;match logics.**&gt; #配置输出数据流的匹配规则及匹配成功后所需要执行的动作，匹配logics标签成功的数据执行转发操作 type forward # forward模式，转发给其他服务器处理（file类型 会将数据写入到路径文件中） send_timeout 60s #发送事件日志的超时时间，默认60s recover_wait 10s #接受服务器故障恢复之前等待时间，默认10s heartbeat_interval 1s #心跳时间刷新频率 phi_threshold 16 #用于检测服务器故障的阈值参数。 默认值为16。 hard_timeout 60s #用于检测服务器故障的硬超时。 默认值等于send_timeout参数。 heartbeat_type tcp #用于心跳的传输协议默认UDP连接，这里为tcp连接方式 slow_flush_log_threshold 300.0 #用于检查块冲洗性能的阈值。默认值为20.0秒。注意，参数类型是float，而不是时间。如果chunk flush需要比这个阈值更长的时间，fluentd日志警告消息如下：2016-12-19 12:00:00 +0000 [warn]：缓冲区刷新花费的时间比slow_flush_log_threshold更长：elapsed_time = 15.0031226690043695 slow_flush_log_threshold = 10.0 num_threads 2 #default 1 buffer_chunk_limit 16M #default 8M buffer_queue_limit 256 #default 256 flush_interval 5s #default 60s &lt;server&gt; name logics.shard host tdagent.test.net port 24224 weight 1 &lt;/server&gt; &lt;secondary&gt; #所有服务器不可用时使用的备份策略，这里是直接生成文件到本地目录 type file path /var/log/td-agent/logics-forward-failed &lt;/secondary&gt;&lt;/match&gt; filter指令 filterTips：“filter” 指令具有与 “match” 相同的语法，但是 filter 可以串联成 pipeline，对数据进行串行处理，最终再交给 match 输出。 使用 fliters，事件流如下： 下面例子里，filter 获取数据后，调用原生的 @type record_transformer 插件，在事件的 record 里插入了新的字段 host_param，然后再交给 match 输出。filter 匹配顺序与 match 相同，我们应该在 之前放置 12345678910111213141516#Input -&gt; filter 1 -&gt; ... -&gt; filter N -&gt; Output（Match tag）### http://this.host:9880/myapp.access?json=&#123;\"event\":\"data\"&#125;&lt;source&gt; @type http port 9880&lt;/source&gt;&lt;filter myapp.access&gt; @type record_transformer &lt;record&gt; host_param \"#&#123;Socket.gethostname&#125;\" &lt;/record&gt;&lt;/filter&gt;&lt;match myapp.access&gt; @type file path /var/log/fluent/access&lt;/match&gt; system指令 systemTips:fluentd的相关设置，也可以在配置文件里设置。包含 log_level suppress_repeated_stacktrace emit_error_log_interval suppress_config_dump without_source 12345678&lt;system&gt; # equal to -qq option log_level error #启动配置 # equal to --without-source option without_source #启动配置 # ... process_name fluentd1 #服务进程名，可通过ps查看到&lt;/system&gt; label指令 labelTips:label用于将任务进行分组，方便复杂任务的管理。可以在 source 里指定 @label @，这个 source 所触发的事件就会被发送给指定的 label 所包含的任务，而不会被后续的其他任务获取到。用来接收插件通过调用 emit_error_event API 抛出的异常，使用方法和 label 一样，通过设定 就可以接收到相关的异常。12345678910111213141516171819202122232425262728293031&lt;source&gt; @type forward&lt;/source&gt;&lt;source&gt; ### 这个任务指定了 label 为 @SYSTEM ### 会被发送给 &lt;label @SYSTEM&gt; ### 而不会被发送给下面紧跟的 filter 和 match @type tail @label @SYSTEM&lt;/source&gt;&lt;filter access.**&gt; @type record_transformer &lt;record&gt; # ... &lt;/record&gt;&lt;/filter&gt;&lt;match **&gt; @type elasticsearch # ...&lt;/match&gt;&lt;label @SYSTEM&gt; ### 将会接收到上面 @type tail 的 source event &lt;filter var.log.middleware.**&gt; @type grep # ... &lt;/filter&gt; &lt;match **&gt; @type s3 # ... &lt;/match&gt;&lt;/label&gt; include指令 includeTips:使用include指令可以导入其他独立的配置文件中的指令，这些文件可以使用相对路径、绝对路径及HTTP的URL Fluentd插件Tips：插件有6种类型 input：输入 output：输出 Buffer：缓冲区 filter：过滤器 Parset：解析器 Formatter：格式化器 Fluentd安装 安装步骤Tips:这里安装的td-agent是fluentd的易安装版本，也是业界流行的的安装版本，点击查看版本下载页12cat /etc/issue #这里实验机型为Amazon Linux AMI release 2016.09curl -L https://toolbelt.treasuredata.com/sh/install-redhat-td-agent2.sh | sh 参考说明Plugins 参照：https://blog.mallux.me/2017/02/04/fluentd/","categories":[],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://arvon.top/tags/ELK/"}]},{"title":"消息队列介绍及常用MQ对比","slug":"消息队列介绍及常用MQ对比","date":"2018-02-23T07:00:00.000Z","updated":"2018-02-23T08:03:37.000Z","comments":true,"path":"2018/02/23/消息队列介绍及常用MQ对比/","link":"","permalink":"http://arvon.top/2018/02/23/消息队列介绍及常用MQ对比/","excerpt":"什么是消息队列概述： 消息队列（Message Queue）一般大家习惯简称为MQ。主要特点为异步处理,也就是说消息的发送者和接收者不需要同时与消息队列交互。消息会保存在队列中，直到接收者取回它。消息队列和信号相比，能够传递更多的信息。与管道相比，消息队列提供了有格式的数据。 举一个直观的小例子姐姐小红希望弟弟小明多读书，经常拿好书给小明看。之前的方式是先问小明什么时候有空，然后把书给小明送过去，监督小明把书读完。后来两人都觉得太麻烦，就换了一个方式。买了一个书架，小红一有好书就放到书架上，小明看到书架上有书就取着读。这里，书架其实就是一个消息队列，小红就是生产者，小明就是消费者。这样的好处是：1.小红想给小明书的时候不必问小明什么时候有时间，直接把书放在书架上就行了，小红和小明的时间都更自由2.小红相信小明的读书自觉和读书能力，不必亲眼观察小明的读书过程，小红只要做一个放书的动作，很节省时间。3.当明天有另一个爱读书的小伙伴小强加入，小红仍旧只需要把书放到书架上，小明和小强从书架上取书即可4.书在书架上，小明读的慢就晚点看完，读的快就早点看完，小明的压力会小点。对应可以总结为：解耦、提速、广播、削峰相应缺点就是：1.引入复杂度2.暂时的不一致性PS：以上内容引自知乎祁达方","text":"什么是消息队列概述： 消息队列（Message Queue）一般大家习惯简称为MQ。主要特点为异步处理,也就是说消息的发送者和接收者不需要同时与消息队列交互。消息会保存在队列中，直到接收者取回它。消息队列和信号相比，能够传递更多的信息。与管道相比，消息队列提供了有格式的数据。 举一个直观的小例子姐姐小红希望弟弟小明多读书，经常拿好书给小明看。之前的方式是先问小明什么时候有空，然后把书给小明送过去，监督小明把书读完。后来两人都觉得太麻烦，就换了一个方式。买了一个书架，小红一有好书就放到书架上，小明看到书架上有书就取着读。这里，书架其实就是一个消息队列，小红就是生产者，小明就是消费者。这样的好处是：1.小红想给小明书的时候不必问小明什么时候有时间，直接把书放在书架上就行了，小红和小明的时间都更自由2.小红相信小明的读书自觉和读书能力，不必亲眼观察小明的读书过程，小红只要做一个放书的动作，很节省时间。3.当明天有另一个爱读书的小伙伴小强加入，小红仍旧只需要把书放到书架上，小明和小强从书架上取书即可4.书在书架上，小明读的慢就晚点看完，读的快就早点看完，小明的压力会小点。对应可以总结为：解耦、提速、广播、削峰相应缺点就是：1.引入复杂度2.暂时的不一致性PS：以上内容引自知乎祁达方Update: 2017-12-11 编写 常见消息队列（MQ）RabbitMQRabbitMQ是使用Erlang编写的一个开源的消息队列，本身支持很多的协议：AMQP，XMPP, SMTP, STOMP，也正因如此，它非常重量级，更适合于企业级的开发。同时实现了Broker构架，这意味着消息在发送给客户端时先在中心队列排队。对路由，负 载均衡或者数据持久化都有很好的支持。 RedisRedis是一个基于Key-Value对的NoSQL数据库，开发维护很活跃。虽然它是一个Key-Value数据库存储系统，但它本身支持MQ功能， 所以完全可以当做一个轻量级的队列服务来使用。对于RabbitMQ和Redis的入队和出队操作，各执行100万次，每10万次记录一次执行时间。测试 数据分为128Bytes、512Bytes、1K和10K四个不同大小的数据。实验表明：入队时，当数据比较小时Redis的性能要高于 RabbitMQ，而如果数据大小超过了10K，Redis则慢的无法忍受；出队时，无论数据大小，Redis都表现出非常好的性能，而RabbitMQ 的出队性能则远低于Redis。 Kafka/JafkaKafka是Apache下的一个子项目，是一个高性能跨语言分布式Publish/Subscribe消息队列系统，而Jafka是在Kafka之上孵 化而来的，即Kafka的一个升级版。具有以下特性：快速持久化，可以在O(1)的系统开销下进行消息持久化；高吞吐，在一台普通的服务器上既可以达到 10W/s的吞吐速率；完全的分布式系统，Broker、Producer、Consumer都原生自动支持分布式，自动实现复杂均衡；支持Hadoop 数据并行加载，对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka通过Hadoop的并行 加载机制来统一了在线和离线的消息处理。Apache Kafka相对于ActiveMQ是一个非常轻量级的消息系统，除了性能非常好之外，还是一个工作良好的分布式系统。 ZeroMQZeroMQ号称最快的消息队列系统，尤其针对大吞吐量的需求场景。ZMQ能够实现RabbitMQ不擅长的高级/复杂的队列，但是开发人员需要自己组合 多种技术框架，技术上的复杂度是对这MQ能够应用成功的挑战。ZeroMQ具有一个独特的非中间件的模式，你不需要安装和运行一个消息服务器或中间件，因 为你的应用程序将扮演了这个服务角色。你只需要简单的引用ZeroMQ程序库，可以使用NuGet安装，然后你就可以愉快的在应用程序之间发送消息了。但 是ZeroMQ仅提供非持久性的队列，也就是说如果down机，数据将会丢失。其中，Twitter的Storm中默认使用ZeroMQ作为数据流的传 输。 ActiveMQActiveMQ是Apache下的一个子项目。 类似于ZeroMQ，它能够以代理人和点对点的技术实现队列。同时类似于RabbitMQ，它少量代码就可以高效地实现高级应用场景。","categories":[],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"http://arvon.top/tags/消息队列/"}]},{"title":"Jenkins使用GitSubmodule实现代码库调用","slug":"Jenkins使用GitSubmodule实现代码库调用","date":"2017-11-29T03:00:00.000Z","updated":"2017-11-29T04:01:09.000Z","comments":true,"path":"2017/11/29/Jenkins使用GitSubmodule实现代码库调用/","link":"","permalink":"http://arvon.top/2017/11/29/Jenkins使用GitSubmodule实现代码库调用/","excerpt":"背景：在实际项目构建中有时会用到调用其他代码库的情况，例如调用公共代码库和基础代码库，此时就可以用git submodule这个模块实现这个需求，git submodule可以将一个git仓库以目录的方式作为另一个git仓库的子目录。Jenkins用来做项目的持续集成，如果使用了Git Submodule,每次Jenkins更新版本库的时候还需要更新submodule的内容。","text":"背景：在实际项目构建中有时会用到调用其他代码库的情况，例如调用公共代码库和基础代码库，此时就可以用git submodule这个模块实现这个需求，git submodule可以将一个git仓库以目录的方式作为另一个git仓库的子目录。Jenkins用来做项目的持续集成，如果使用了Git Submodule,每次Jenkins更新版本库的时候还需要更新submodule的内容。 Jenkis中的使用方法 版本需求注意：使用此功能，需要Jenkins的部分插件达到指定版本，我的版本如下： Git clent plugin == 2.6.0 Git Plugin == 3.6.4 Credentials Plugin == 2.1.16 SSH Credentials Plugin == 1.13 安装以上版本或更高版本插件安装完成后需要重启Jenkins，此时就可以使用了 在Jenkins上设置其中User credentials from default remote of parent repository意思是Git Submodule的repository会使用和主repository一样的验证，就是主repository使用的身份，低版本不会有这个勾选项，还会报错。 升级插件的方法 1.使用Jenins默认方法进入Jenkins管理页面即 Jenkins–&gt;插件管理–&gt;高级 如下：Jenkins默认更新的URL为http://updates.jenkins-ci.org/experimental/update-center.json ，但是由于网络问题推荐使用http://mirror.xmission.com/jenkins/updates/experimental/update-center.json,另外有条件也可以使用这个页面上面的网络代理选项 2.直接下载插件通过web页面进行上传依然在Jenkins的管理页面高级中，通过自己下载，然后上传即可，部分插件安装后需要重启Jenkins生效 1234https://wiki.jenkins.io/display/JENKINS/Git+Plugin#git 3.6.4https://wiki.jenkins.io/display/JENKINS/Git+Client+Plugin#git-client 2.6.0 关于报错 1.Permission报错 123456789101112131415FATAL: Command &quot;git submodule update --init --recursive&quot; returned status code 128:stdout:stderr: Cloning into &apos;plxxm/pxxx&apos;...Permission denied (publickey).fatal: Could not read from remote repository.Please make sure you have the correct access rightsand the repository exists.fatal: clone of &apos;git@xxx.net:platform/planx.git&apos; into submodule path &apos;plxxm/pxxx&apos; failedhudson.plugins.git.GitException: Command &quot;git submodule update --init --recursive&quot; returned status code 128:stdout:stderr: Cloning into &apos;plxxm/pxxx&apos;...Permission denied (publickey).fatal: Could not read from remote repository.Please make sure you have the correct access rightsand the repository exists. 原因：git submodule使用的是ssh方式管理repository,没有找到可以使用的ssh key做submodule的身份认证。按上面的方法升级Git插件版本后，就可以通过和主版本库一样的Credentials进行代码获取了","categories":[],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://arvon.top/tags/Jenkins/"}]},{"title":"collectd使用exec模块进行自定义监控","slug":"collectd使用exec模块进行自定义监控","date":"2017-11-28T07:00:00.000Z","updated":"2017-11-28T07:10:09.000Z","comments":true,"path":"2017/11/28/collectd使用exec模块进行自定义监控/","link":"","permalink":"http://arvon.top/2017/11/28/collectd使用exec模块进行自定义监控/","excerpt":"使用Grafana进行服务监控的明显优点就是监控出图漂亮，而且拥有丰富的插件，可以直接监控系统资源和负荷信息以及常见的服务进程状态，不过还是会有一些场景直接使用模块是不太容易进行监控的，这个时候更适合使用编写脚本进行数据收集和监控，而exec模块就可以满足这样的需求，下面就列举一个小栗子。","text":"使用Grafana进行服务监控的明显优点就是监控出图漂亮，而且拥有丰富的插件，可以直接监控系统资源和负荷信息以及常见的服务进程状态，不过还是会有一些场景直接使用模块是不太容易进行监控的，这个时候更适合使用编写脚本进行数据收集和监控，而exec模块就可以满足这样的需求，下面就列举一个小栗子。 步骤 开启exec模块(/etc/collectd.conf)开启exec模块并指定自定义脚本位置 123456&lt;Plugin exec&gt;# Exec \"user:group\" \"/path/to/exec\" Exec \"ec2-user:ec2-user\" \"/opt/collectd/plugin/check_notice.sh\"# NotificationExec \"user:group\" \"/path/to/exec\"&lt;/Plugin&gt;LoadPlugin exec 自定义脚本 1234567891011HOSTNAME=\"$&#123;COLLECTD_HOSTNAME:-vpc1-208-jenkins&#125;\"INTERVAL=\"$&#123;COLLECTD_INTERVAL:-10&#125;\"while sleep \"$INTERVAL\"; do RES=$(curl -s http://1.2.3.4:8083/notice/v1/getnotice?gid=5\\&amp;version=4.0.500 |grep Endpoint |wc -l) echo \"PUTVAL \\\"$HOSTNAME/health/gauge-notice\\\" interval=$INTERVAL N:$RES\"done#$HOSTNAME/health/gauge-notice对应下面的#&lt;instance-id&gt;/&lt;plugin&gt;-&lt;plugin_instance&gt;/&lt;type&gt;-&lt;type_instance&gt;#gauge是表示type的一种，不能自己编着写哦#脚本输出如：PUTVAL \"vpc1-208-jenkins/health/gauge-notice\" interval=10 N:1 在Grafana上添加监控 参考 官方介绍 Github地址 timo的博客","categories":[],"tags":[{"name":"Grafana","slug":"Grafana","permalink":"http://arvon.top/tags/Grafana/"},{"name":"运维监控","slug":"运维监控","permalink":"http://arvon.top/tags/运维监控/"}]},{"title":"Elasticsearch默认fields1000报错解决","slug":"Elasticsearch默认fields1000报错解决","date":"2017-11-24T08:11:16.000Z","updated":"2017-11-24T08:19:12.000Z","comments":true,"path":"2017/11/24/Elasticsearch默认fields1000报错解决/","link":"","permalink":"http://arvon.top/2017/11/24/Elasticsearch默认fields1000报错解决/","excerpt":"背景：由于日志输出调整，ES出现了很多如下的报错，这个issue可以在这个github地址找到,另外还有5.0版本关于这个问题的说明。我的ES版本为：5.0.0(直接curl yourip:9200就可以看到) This is to prevent mapping explosion when dynamic keys such as UUID are used as field names. index.mapping.total_fields.limit specifies the total number of fields an index can have. An exception will be thrown when the limit is reached. The default limit is 1000. Value 0 means no limit. This setting is runtime adjustable– – – 以上摘自yanjunh对于该issue的答复","text":"背景：由于日志输出调整，ES出现了很多如下的报错，这个issue可以在这个github地址找到,另外还有5.0版本关于这个问题的说明。我的ES版本为：5.0.0(直接curl yourip:9200就可以看到) This is to prevent mapping explosion when dynamic keys such as UUID are used as field names. index.mapping.total_fields.limit specifies the total number of fields an index can have. An exception will be thrown when the limit is reached. The default limit is 1000. Value 0 means no limit. This setting is runtime adjustable– – – 以上摘自yanjunh对于该issue的答复 报错如下： 报错信息 [2017-11-18T00:00:03,102][DEBUG][o.e.a.b.TransportShardBulkAction] [vpc1-ip-1] [bilogs-logics-202.2017.11.17][2] failed to execute bulk item (index) …omitted…java.lang.IllegalArgumentException: Limit of total fields [1000] in index [bilogs-logics-log] has been exceeded 解决方案：说明：对于已经建立的索引可以通过设置fields进行修复，对于之后的将建立的索引通过设置template进行设置 对于已经建立的索引 123456789101112curl -XPUT yourEShost:port/your_index_name/_settings -d '&#123;\"index.mapping.total_fields.limit\": 0&#125;'#以上表示对于‘your_index_name’这个索引设置fields为无限制，默认为1000curl -XPUT 10.0.1.1:9200/*/_settings -d '&#123;\"index.mapping.total_fields.limit\": 50000&#125;'#这个表示对所有index的fields的limit设置为50000curl 10.0.1.1:9200/_cat/indices/*?pretty#查看所有索引,查看指定索引将*换为索引名称即可curl -XGET 10.0.1.1:9200/_all/_settings?pretty#查看所有索引的设置curl 10.0.1.1:9200/bilogs-logics-202.2017.11.21/_settings?pretty#查看单个索引的设置curl 10.0.1.1:9200/bilogs-logics-202.2017.11.23/_mapping?pretty#查看单个索引的map 对于未创建的索引，可以通过模板设置 1234567891011curl -XPUT '10.0.1.1:9200/_template/all ' -d '&#123; \"template\": \"*\", \"settings\": &#123; \"index.mapping.total_fields.limit\": 50000, \"refresh_interval\": \"30s\" &#125;&#125;'#设置template的setting，curl -XGET 10.33.3.191:9200/_template/*?pretty#查看所有模板的设置，使用了*匹配，如果看指定的模板将*换成对应模板名即可，另外这里可以看到每个模本都有一个\"order\"字段，这个字段的数值越低，优先级越高，优先级高的模板会覆盖优先级低的模板","categories":[],"tags":[{"name":"踩坑指南","slug":"踩坑指南","permalink":"http://arvon.top/tags/踩坑指南/"},{"name":"ELK","slug":"ELK","permalink":"http://arvon.top/tags/ELK/"}]},{"title":"关于Kafka分布式消息队列","slug":"关于Kafka分布式消息队列","date":"2017-11-05T13:00:00.000Z","updated":"2017-11-06T10:12:45.000Z","comments":true,"path":"2017/11/05/关于Kafka分布式消息队列/","link":"","permalink":"http://arvon.top/2017/11/05/关于Kafka分布式消息队列/","excerpt":"背景: 直接使用EFK进行日志收集，在大规模高压力的情况下Elasticsearch会存在丢数据的情况，现在考虑使用MQ（Message Queue）进行缓冲，达到不丢数据的目的。由于对于日志收集响应速度并不是十分高，并且对日志的可靠性要求较高，最终选择Kafka来充当消息队列而非官方推荐的redis。这里着重进行kafka介绍，之后会整合EFK+kafka的应用落地记录。","text":"背景: 直接使用EFK进行日志收集，在大规模高压力的情况下Elasticsearch会存在丢数据的情况，现在考虑使用MQ（Message Queue）进行缓冲，达到不丢数据的目的。由于对于日志收集响应速度并不是十分高，并且对日志的可靠性要求较高，最终选择Kafka来充当消息队列而非官方推荐的redis。这里着重进行kafka介绍，之后会整合EFK+kafka的应用落地记录。 关于Kafka的基本原理基本介绍 Kafka是由LinkedIn使用Scala开发的一个分布式的消息系统。最初用作LinkedIn的活动流（Activity Stream）和运营数据处理管道（Pipeline），Linkedin于2010年贡献给了Apache基金会并成为顶级开源项目。kafka是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统(也可以当做MQ系统)，常见可以用于web/nginx日志、访问日志，消息服务等等。 基本组成Tips：主要由四部分组成，Topic（话题）、Producer（生产者）、Broker（服务节点）、Consumer（消费者） Broker 已发布的消息保存在一组服务器中，它们被称为代理（Broker）或Kafka集群。组成kafka集群的每个服务器，都称为是Broker。Broker可以容纳多个Topic Topic 是特定类型的消息流。消息是字节的有效负载（Payload），话题是消息的分类名或种子（Feed）名。每条发送到kafka的消息都有一个类别，这个类别就叫做Topic。可以理解为一个消息队列（Message Queue）的名称。（物理上不同 Topic 的消息分开存储，逻辑上一个 Topic 的消息虽然保存于一个或多个 broker 上，但用户只需指定消息的 Topic 即可生产或消费数据而不必关心数据存于何处）。Partition parition是物理上的概念，每个topic包含一个或多个partition，创建topic时可指定parition数量。每个partition对应于一个文件夹，该文件夹下存储该partition的数据和索引文件 partition分区数，控制topic将分片成多少个log。可以显示指定，如果不指定则会使用broker(server.properties)中的num.partitions配置的数量 为了实现扩展性，一个非常大的topic可以分布到多个 broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。 partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体 （多个partition间）的顺序。 也就是说，一个topic在集群中可以有多个partition，那么分区的策略是什么？(消息发送到哪个分区上，有两种基本的策略，一是采用Key Hash算法，一是采用Round Robin算法) Offset kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka Producer 消息生产者，就是向kafka broker发消息的客户端。负责发布消息到Kafka broker。 Consumer 消息消费者，向 Kafka broker 读取消息的客户端。每个consumer属于一个特定的consuer group（可为每个consumer指定group name，若不指定group name则属于默认的group）。使用consumer high level API时，同一topic的一条消息只能被同一个consumer group内的一个consumer消费，但多个consumer group可同时消费这一消息。Consumer Group 每个 Consumer 属于一个特定的 Consumer Group（可为每个 Consumer 指定 group name，若不指定 group name 则属于默认的 group） 消息系统有两类，一是广播，二是订阅发布。广播是把消息发送给所有的消费者；发布订阅是把消息只发送给订阅者。Kafka通过Consumer Group组合实现了这两种机制： 实现一个topic消息广播（发给所有的consumer）和单播（发给任意一个consumer）。一个topic可以有多个Consumer Group。 topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个CG只会把消息发给该CG中的一个 consumer（这是实现一个Topic多Consumer的关键点：为一个Topic定义一个CG，CG下定义多个Consumer）。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。 典型的应用场景是，多个Consumer来读取一个Topic(理想情况下是一个Consumer读取Topic的一个Partition）,那么可以让这些Consumer属于同一个Consumer Group即可实现消息的多Consumer并行处理，原理是Kafka将一个消息发布出去后，ConsumerGroup中的Consumers可以通过Round Robin的方式进行消费(Consumers之间的负载均衡使用Zookeeper来实现) 搭建kafka集群 Tips：这里使用的zk及kafka版本如下zookeeper:3.4.10kafka:2.11-0.10.2.0 安装配置zookeeper Install 12345678wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.10/zookeeper-3.4.10.tar.gztar xvf zookeeper-3.4.10.tar.gzcp zookeeper-3.4.10 /usr/local/zookeeper/ -rcd /usr/local/zookeeper/cp conf/zoo_sample.cfg conf/zoo.cfgmkdir -p /var/zookeeper/dataecho 1 &gt;/var/zookeeper/data/myid#注意，这里要与下面config中server.1的配置一致，此server为10.17.0.112，所以echo 1，同理226这台server就echo 2 Config 123456789#cat /usr/local/zookeeper/conf/zoo.cfgtickTime=2000initLimit=10syncLimit=5dataDir=/var/zookeeper/dataclientPort=2181server.1=10.17.0.112:2888:3888server.2=10.17.0.226:2888:3888server.3=10.17.0.211:2888:3888 Start 123cd /usr/local/zookeeper./bin/zkServer.sh start./bin/zkServer.sh status 123456789101112#cat /etc/rc.d/init.d/zookeeper#!/bin/bash#chkconfig:2345 20 90#description:zookeeper#processname:zookeepercase $1 in start) /usr/local/zookeeper/bin/zkServer.sh start;; stop) /usr/local/zookeeper/bin/zkServer.sh stop;; status) /usr/local/zookeeper/bin/zkServer.sh status;; restart) /usr/local/zookeeper/bin/zkServer.sh restart;; *) echo \"require start|stop|status|restart\";;esac 安装配置kafka Install 1234wget http://mirror.bit.edu.cn/apache/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgztar xvf kafka_2.11-0.10.2.0.tgzcp kafka_2.11-0.10.2.0 /usr/local/kafka/ -rcd /usr/local/kafka/ Config（需要改server和consumer配置） 12345678910111213141516171819#cat /usr/local/kafka/config/server.properties |egrep -v \"^#|^$\"broker.id=2#唯一值，我这里按zk的serverID进行了配置listeners = PLAINTEXT://10.17.0.226:9092#这个地方在这个版本及以后需要打开注释填写本机地址，之前的版本需配置host.namenum.network.threads=3num.io.threads=8socket.send.buffer.bytes=102400socket.receive.buffer.bytes=102400socket.request.max.bytes=104857600log.dirs=/tmp/kafka-logsnum.partitions=1num.recovery.threads.per.data.dir=1log.retention.hours=168log.segment.bytes=1073741824log.retention.check.interval.ms=300000zookeeper.connect=10.17.0.112:2181,10.17.0.226:2181,10.17.0.221:2181#这里填写zookeeper的地址，另外消费配置上也需要做对应修改zookeeper.connection.timeout.ms=6000 1234#cat /usr/local/kafka/config/consumer.properties |egrep -v \"^#|^$\"zookeeper.connect=10.17.0.112:2181,10.17.0.226:2181,10.17.0.221:2181zookeeper.connection.timeout.ms=6000group.id=test-consumer-group Start 12345cd /usr/local/kafka/bin/kafka-server-start.sh config/server.properties &amp;#后台执行，退出终端后终止bin/kafka-server-start.sh -daemon config/server.properties &amp;#后台执行，退出终端后不终止 1234567891011121314#cat /etc/rc.d/init.d/kafka#!/bin/bash#chkconfig:2345 30 80#description:kafka#processname:kafkacase $1 in start) /usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties &amp;;; stop) /usr/local/kafka/bin/kafka-server-stop.sh /usr/local/kafka/config/server.properties;; restart) /usr/local/kafka/bin/kafka-server-stop.sh /usr/local/kafka/config/server.properties /usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties &amp; ;; *) echo \"require start|stop|restart\" ;;esac 安装配置kafka监控程序 目前常用有三种：Kafka Web Console：监控功能较为全面，可以预览消息，监控Offset、Lag等信息，但存在bug，不建议在生产环境中使用。Kafka Manager：偏向Kafka集群管理，若操作不当，容易导致集群出现故障。对Kafka实时生产和消费消息是通过JMX实现的。没有记录Offset、Lag等信息。KafkaOffsetMonitor：程序一个jar包的形式运行，部署较为方便。只有监控功能，使用起来也较为安全。这里使用第三种KafkaOffsetMonitor install 12wget https://github.com/quantifind/KafkaOffsetMonitor/releases/download/v0.2.1/KafkaOffsetMonitor-assembly-0.2.1.jar#下载即可 start 123456java -cp KafkaOffsetMonitor-assembly-0.2.1.jar \\ com.quantifind.kafka.offsetapp.OffsetGetterWeb \\ --zk 10.17.0.112:2181,10.17.0.226:2181,10.17.0.221:2181 \\ --port 8089 \\ --refresh 10.seconds \\ --retain 2.days &amp; 常用kafka命令 创建及查看Topic 123456bin/kafka-topics.sh --create --zookeeper 10.17.0.211:2181 --replication-factor 3 --partitions 2 --topic prod-test#创建一个名为prod-test的topic，有3个副本（即控制消息保存在3个broker上），2个分区bin/kafka-topics.sh --list --zookeeper 10.17.0.211:2181#list topic，列出所有topicbin/kafka-topics.sh --describe --zookeeper 10.17.0.211:2181 --topic prod-test#查看某个topic的具体信息 生产消费数据（可以测试集群搭建是否成功） 1234bin/kafka-console-producer.sh --broker-list 10.17.0.211:9092 --topic prod-test#从控制台向topic生产数据bin/kafka-console-consumer.sh --zookeeper 10.17.0.226:2181 --topic prod-test --from-beginning#从控制台消费topic prod-test的数据，可以使用任意集群中的地址进行测试 查看topic某分区偏移量最大（小）值 1234bin/kafka-run-class.sh kafka.tools.GetOffsetShell --topic prod-test --time -2 --broker-list 10.17.0.211:9092 --partitions 1#查看prod-test这个topic分区1的分组最小偏移值（time -2 表示最小偏移量）bin/kafka-run-class.sh kafka.tools.GetOffsetShell --topic prod-test --time -1 --broker-list 10.17.0.211:9092 --partitions 1#查看prod-test这个topic分区0的分组最大偏移值（time -1 表示最大偏移量 增加topic分区数 12bin/kafka-topics.sh --zookeeper 10.17.0.211:2181 --alter --topic prod-test --partitions 5#增加prod-test的这个topic的分区数到5个（比如之前是3个，就是再增加2个），这个数字只能比现在已有的分区数大。 查看topic消费进度 12#consumer group可以从zk中查看bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --group console-consumer-50561 --zookeeper 10.17.0.226:2181","categories":[],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://arvon.top/tags/ELK/"},{"name":"消息队列","slug":"消息队列","permalink":"http://arvon.top/tags/消息队列/"}]},{"title":"Etcd及相关组件安装配置说明","slug":"etcd及相关组件安装配置说明","date":"2017-10-13T13:00:00.000Z","updated":"2017-10-13T08:58:01.000Z","comments":true,"path":"2017/10/13/etcd及相关组件安装配置说明/","link":"","permalink":"http://arvon.top/2017/10/13/etcd及相关组件安装配置说明/","excerpt":"简介：Etcd是CentOS公司发起的一个开源项目，灵感来自于 ZooKeeper 和 Doozer，它使用Go语言编写，并通过Raft一致性算法处理日志复制以保证强一致性（Raft是一个来自Stanford的新的一致性算法，适用于分布式系统的日志复制，Raft通过选举的方式来实现一致性，在Raft中，任何一个节点都可能成为Leader。），主要用于共享配置和服务发现的分布式、一致性的KV存储系统。除了常见的共享配置及服务发现，还可以使用Etcd结合SkyDNS提供内网域名解析服务。这里主要涉及etcd安装配置说明、etcd-view安装配置、skydns安装配置","text":"简介：Etcd是CentOS公司发起的一个开源项目，灵感来自于 ZooKeeper 和 Doozer，它使用Go语言编写，并通过Raft一致性算法处理日志复制以保证强一致性（Raft是一个来自Stanford的新的一致性算法，适用于分布式系统的日志复制，Raft通过选举的方式来实现一致性，在Raft中，任何一个节点都可能成为Leader。），主要用于共享配置和服务发现的分布式、一致性的KV存储系统。除了常见的共享配置及服务发现，还可以使用Etcd结合SkyDNS提供内网域名解析服务。这里主要涉及etcd安装配置说明、etcd-view安装配置、skydns安装配置 优势分析： 简单：基于HTTP+JSON的API，可以直接用curl命令和Pyhton的URL方法轻松测试使用，相比ZK方便运维 可靠：使用Raft算法保证强一致性，并可靠的实现了分布式，集群具备一定的容错能力。即使集群中出现部分节点故障、网络故障等问题，仍可保证其余大多数节点正确的步进。甚至当更多的节点（一般来说超过集群节点总数的一半）出现故障而导致集群不可用时，依然可以保证节点中的数据不会出现错误的结果。 安全： 快速：按照官网给出的[Benchmark], 在2CPU，1.8G内存，SSD磁盘这样的配置下，单节点的写性能可以达到16K QPS, 而先写后读也能达到12K QPS。性能相当可观 项目活跃度：有大量资料，开发社区活跃，对比ZK好特别多。Google的容器集群管理系统Kubernetes、开源PaaS平台Cloud Foundry和CoreOS的Fleet都广泛使用了etcd。 原理分析： a.)ETCD使用Raft协议来维护集群内各个节点状态的一致性。简单说，ETCD集群是一个分布式系统，由多个节点相互通信构成整体对外服务，每个节点都存储了完整的数据，并且通过Raft协议保证每个节点维护的数据是一致的。每个ETCD节点都维护了一个状态机，并且，任意时刻至多存在一个有效的主节点。主节点处理所有来自客户端写操作，通过Raft协议保证写操作对状态机的改动会可靠的同步到其他节点。 b.)ETCD工作原理核心部分在于Raft协议,Raft协议主要分为三个部分：选主，日志复制，安全性。 选主：一组服务节点构成一个集群，并且有一个主节点来对外提供服务。当集群初始化，或者主节点挂掉后，面临一个选主问题。集群中每个节点，任意时刻处于Leader, Follower, Candidate这三个角色之一。当集群初始化时候，每个节点都是Follower角色，当Follower在一定时间内没有收到来自主节点的心跳，会将自己角色改变为Candidate，并发起一次选主投票；当收到包括自己在内超过半数节点赞成后，选举成功；当收到票数不足半数选举失败，或者选举超时。若本轮未选出主节点，将进行下一轮选举（出现这种情况，是由于多个节点同时选举，所有节点均为获得过半选票）。Candidate节点收到来自主节点的信息后，会立即终止选举过程，进入Follower角色。为了避免陷入选主失败循环，每个节点未收到心跳发起选举的时间是一定范围内的随机值，这样能够避免2个节点同时发起选主。 日志复制：所谓日志复制，是指主节点将每次操作形成日志条目，并持久化到本地磁盘，然后通过网络IO发送给其他节点。其他节点根据日志的逻辑时钟(TERM)和日志编号(INDEX)来判断是否将该日志记录持久化到本地。当主节点收到包括自己在内超过半数节点成功返回，那么认为该日志是可提交的(committed），并将日志输入到状态机，将结果返回给客户端。注意每次选主都会形成一个唯一的TERM编号，相当于逻辑时钟。每一条日志都有全局唯一的编号。 安全性：Raft在选主逻辑中，对能够成为主的节点加以限制，确保选出的节点已定包含了集群已经提交的所有日志。如果新选出的主节点已经包含了集群所有提交的日志，那就不需要从和其他节点比对数据了。简化了流程，缩短了集群恢复服务的时间。 应用场景： 配置管理 服务注册发现 选主 应用调度 分布式队列 分布式锁 安装 软件获取地址： Etcd Github地址 Etcd-viewer nikfoundas.github.ioNeed: java &gt;= 1.7官网地址 maven &gt;= 3.0.5 官网地址 skydns Gitbub地址 我的安装版本 Etcd:__V3.2.7 Etcd-viewer:__V1.1 JDK1.8 Maven3.5.0 sykdns:__V2.5.3a Etcd接口支持 支持HTTP的PUT/GET/DELETE接口 通过http long poll支持WATCH接口（服务注册与发现） 支持key具有TTL属性 CAS（compare and swap）操作 支持多key的事务操作 支持目录操作 常用命令 启动参数 12345678910 /usr/local/bin/etcd \\-name ip-10-222-0-218-2 \\--data-dir /opt/etcd/etcd-data \\-initial-advertise-peer-urls http://10.222.0.218:12380 \\-listen-peer-urls http://10.222.0.218:12380 \\-listen-client-urls http://10.222.0.218:2379,http://127.0.0.1:2379 \\-advertise-client-urls http://10.222.0.218:2379 \\-initial-cluster-token etcd-cluster-arvon \\-initial-cluster ip-10-222-0-218=http://10.222.0.218:12380,ip-10-222-0-218=http://10.222.0.218:2380 \\-initial-cluster-state new 常用命令 123456789101112131415161718192021222324etcdctl member list#查看集群状况etcdctl rm hello#检查集群监控状态etcdctl set test-key1 value1#设置一个键值etcdctl get test-key1#获取指定key的valueetcdctl mkdir /dir1/dir2/etcdctl mkdir /dir3/#创建一个空目录,可递归创建set /dir4/test-key2 value2#跨目录设置key，会自动创建目录etcdctl get /dir4/test-key2#获取目录下的keyetcdctl lsetcdctl ls dir1/dir2#查看注册在根目录下的文件（目录及key）etcdctl rm test-key1#删除指定keyetcdctl rmdir dir3#删除指定空目录（只能删除空目录）etcdctl rm -r dir1#递归删除指定目录，也能删除key HTTP接口常用命令 123456789101112131415161718192021222324252627curl http://10.222.0.80:2379/version#查看版本curl http://10.222.0.80:2379/v2/members#查看集群节点curl http://10.222.0.80:2379/v2/stats/store#查看集群运行状态curl http://10.222.0.80:2379/v2/stats/leader#查看选举的leadercurl http://10.222.0.80:2379/v2/stats/self#查看节点自身信息curl http://10.222.0.80:2379/v2/keys#查看键curl -XPUT http://10.222.0.80:2379/v2/keys/test-key11 -d value=\"value11\"curl -XPUT http://10.222.0.80:2379/v2/keys/dir3/dir33/test-key333 -d value=\"value333\"#创建一个键值curl http://10.222.0.80:2379/v2/keys/cdir1 -XPUT -d dir=truecurl http://10.222.0.80:2379/v2/keys/cdir2/cdir3 -XPUT -d dir=true#创建一个目录curl http://10.222.0.80:2379/v2/keys/tvalue1 -XPUT -d value=\"t1\" -d ttl=10#创建一个带ttl的键值，单位为秒，即10s后此键值自动消失curl http://10.222.0.80:2379/v2/keys/seqvar -XPOST -d value=\"s01\"curl http://10.222.0.80:2379/v2/keys/seqvar -XPOST -d value=\"s02\"curl http://10.222.0.80:2379/v2/keys/seqvar -XPOST -d value=\"s03\"curl http://10.222.0.80:2379/v2/keys/seqvar#创建有序键值（例如类似DNS轮训注册）curl http://10.222.0.80:2379/v2/keys/value111 -XDELETE#删除一个键值 备份还原策略 数据备份 1234ps axu | grep etcd|sed \"s/ -/\\n/g\" |grep \"data-dir\" |awk '&#123;print $2&#125;'#获取data目录etcdctl backup --data-dir=/opt/etcd/etcd-data --backup-dir=/tmp/etcd-backup-`date +%y%m%d%H`#备份数据目录到tmp下 备份还原（单机情况） 1234#1. 关闭etcd#2. 将备份数据拷贝至datadir#3. 启动配置需加上--force-new-cluster参数#4. 启动etcd 说明 1.使用 –force-new-cluster 参数启动Etcd服务。这个参数会重置集群ID和集群的所有成员信息，其中节点的监听地址会被重置为localhost:2379, 表示集群中只有一个节点。 附录1：结合SkyDNS实现内网域名解析说明：使用skydns+etcd可以搭建一个内网的域名服务，对于业务的部署连接及扩展会非常有帮助。 设置步骤如下： 安装etcd服务 安装skydns服务 初始化skydns服务 修改dhcp选项中的DNS解析地址，或手动修改/etc/resolve.conf 配置完成（附录2中有完整的ansible脚本） 附录2：Ansible脚本安装部署脚本地址：包括以下 安装etcd 安装etcd-view 安装skydns 安装supervisor 附录3：参考链接EtcdEtcd不错的图文应用介绍Etcd常用场景分析Etcd原理及ZK对比","categories":[],"tags":[{"name":"配置管理","slug":"配置管理","permalink":"http://arvon.top/tags/配置管理/"},{"name":"运维架构","slug":"运维架构","permalink":"http://arvon.top/tags/运维架构/"}]},{"title":"Ansible部署NFS服务","slug":"Ansible部署NFS服务","date":"2017-09-07T12:30:00.000Z","updated":"2017-09-07T12:36:57.000Z","comments":true,"path":"2017/09/07/Ansible部署NFS服务/","link":"","permalink":"http://arvon.top/2017/09/07/Ansible部署NFS服务/","excerpt":"虽然很简单，写的比较low，不过还是记下吧，下次就可以直接拿着用了。很久以前写过一篇如何配置，这个安装基本是把原来的给改成playbook了，附原来手动配置的链接。另附官方模块地址： http://docs.ansible.com/ansible/latest/mount_module.html","text":"虽然很简单，写的比较low，不过还是记下吧，下次就可以直接拿着用了。很久以前写过一篇如何配置，这个安装基本是把原来的给改成playbook了，附原来手动配置的链接。另附官方模块地址： http://docs.ansible.com/ansible/latest/mount_module.html Server Role目录结构 12345678910111213nfs/├── nfs-client│ ├── defaults│ │ └── main.yml│ └── tasks│ └── main.yml└── nfs-server ├── defaults │ └── main.yml ├── tasks │ └── main.yml └── templates └── exports.j2 exportcat nfs/nfs-server/templates/exports.j2 12#/data/nfsd/deploy 192.168.1.2/32(rw,root_squash,all_squash)/data/ops_nfs/deploy &#123;&#123;ip_range&#125;&#125;(rw,root_squash,all_squash) NFS-servercat nfs/nfs-server/tasks/main.yml 12345678910111213141516171819202122232425262728---- name: install nfs pkg yum: name=nfs-utils state=present- name: set starting up with service rpcbind and nfs command: chkconfig rpcbind on- command: chkconfig nfs on- name: create /data/ops_nfs dir file: path: /data/ops_nfs/deploy state: directory owner: nfsnobody group: nfsnobody mode: 0755- name: modify exports config template: src=exports.j2 dest=/etc/exports- name: start nfs service service: name=rpcbind state=restarted- service: name=nfs state=restarted#- name: debug rpc result# command: rpcinfo -p# command: exportfs Client NFS-client cat nfs/nfs-client/tasks/main.yml 123456789101112131415---- name: set starting up with service rpcbind and nfs command: chkconfig rpcbind on- name: start rpcbind service service: name=rpcbind state=restarted- name: mount nfs mount: name: /data/deploy src: \"&#123;&#123; nfs_server_address &#125;&#125;\" fstype: nfs4 opts: rw fstab: /etc/fstab state: mounted Other hosts123456789[nfs-client]192.168.0.1192.168.0.2192.168.0.3[nfs-server]192.168.0.3[nfs:chidren]nfs-clientnfs-server 以上","categories":[],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"http://arvon.top/tags/Ansible/"}]},{"title":"vsftp安装配置记录","slug":"vsftp安装配置记录","date":"2017-08-13T13:00:00.000Z","updated":"2017-08-15T02:26:28.000Z","comments":true,"path":"2017/08/13/vsftp安装配置记录/","link":"","permalink":"http://arvon.top/2017/08/13/vsftp安装配置记录/","excerpt":"ftp服务在运维工作中还是十分常用的。下面记录下安装配置过程。关于主动被动原理可以看一下我的wikivsftp是一个主打安全的ftp服务，所以配置文件会复杂一些。下面是适用于一般场景的配置，使用虚拟用户进行登录操作，更多定制需要需要自行进行修改。","text":"ftp服务在运维工作中还是十分常用的。下面记录下安装配置过程。关于主动被动原理可以看一下我的wikivsftp是一个主打安全的ftp服务，所以配置文件会复杂一些。下面是适用于一般场景的配置，使用虚拟用户进行登录操作，更多定制需要需要自行进行修改。 安装 My Env AWS AMI YUM Repo: epel Red Hat Enterprise Linux Server release 7.3 (Maipo) Check and Install vsftp pkg 12rpm -q vsftpdyum install vsftpd -y Stop Firewall and SElinux 123456setenforce 0# forver stopvi /etc/selinux/config #SELINUX=disabled# Stop firewallsystemctl stop firewalld.servicesystemctl disable firewalld.service 配置修改 主配置文件修改如下 123456789101112131415161718192021222324252627anonymous_enable=Nolocal_enable=YESallow_writeable_chroot=YESwrite_enable=YESlocal_umask=022dirmessage_enable=YESxferlog_enable=YESconnect_from_port_20=YESxferlog_std_format=YESchroot_local_user=YESchroot_list_enable=YESlisten=YESpam_service_name=vsftpduserlist_enable=YEStcp_wrappers=YESlisten_port=21idle_session_timeout=300data_connection_timeout=120guest_enable=YESguest_username=ftpuseruser_config_dir=/etc/vsftpd/vuser_confvirtual_use_local_privs=YESpasv_address=\"Your Public IP\"pasv_min_port=10060pasv_max_port=10090accept_timeout=5connect_timeout=5 添加ftp用户 123useradd -g root -M -d /data/ftp_server -s /sbin/nologin ftpuserecho \"rPUdeubeKCPwRAhpVt8GWLpG\" |passwd --stdin ftpuserchown -R ftpuser.root /data/ftp_server/ Config virture User 123456789touch /etc/vsftpd/vuser_passwdvi /etc/vsftpd/vuser_passwd#user#passworddb_load -T -t hash -f /etc/vsftpd/vuser_passwd /etc/vsftpd/vuser_passwd.dbchmod 600 /etc/vsftpd/vuser_passwd.dbmkdir /etc/vsftpd/vuser_confcd /etc/vsftpd/vuser_conftouch tai-yunying 修改pam.d下的ftp配置 12345678910#%PAM-1.0#session optional pam_keyinit.so force revoke#auth required pam_listfile.so item=user sense=deny file=/etc/vsftpd/ftpusers onerr=succeed#auth required pam_shells.so#auth include password-auth#account include password-auth#session required pam_loginuid.so#session include password-authauth required /lib64/security/pam_userdb.so db=/etc/vsftpd/vuser_passwdaccount required /lib64/security/pam_userdb.so db=/etc/vsftpd/vuser_passwd 启动服务 1systemctl restart vsftpd.service About Base Yum Base Yum Repo123456789101112131415161718192021222324252627282930[base]name=CentOS-$7 - Base - 163.com#mirrorlist=http://mirrorlist.centos.org/?release=$7&amp;arch=$basearch&amp;repo=osbaseurl=http://mirrors.163.com/centos/7/os/$basearch/gpgcheck=1gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7#released updates[updates]name=CentOS-$7 - Updates - 163.com#mirrorlist=http://mirrorlist.centos.org/?release=$7&amp;arch=$basearch&amp;repo=updatesbaseurl=http://mirrors.163.com/centos/7/updates/$basearch/gpgcheck=1gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7#additional packages that may be useful[extras]name=CentOS-$7 - Extras - 163.com#mirrorlist=http://mirrorlist.centos.org/?release=$7&amp;arch=$basearch&amp;repo=extrasbaseurl=http://mirrors.163.com/centos/7/extras/$basearch/gpgcheck=1gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7#additional packages that extend functionality of existing packages[centosplus]name=CentOS-$7 - Plus - 163.combaseurl=http://mirrors.163.com/centos/7/centosplus/$basearch/gpgcheck=1enabled=0gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7 报错解决 227 Entering Passive Mode http://wrj1982.blog.51cto.com/1131419/420537 Unit NetworkManager-dispatcher.service has begun starting up. Jul 28 06:12:45 ip-10-222-0-61.taiyouxi.net dbus[501]: [system] Successfully activated service ‘org.freedesktop.nm_dispatcher’centos7中vsftp的配置文件默认将 listen_ipv6=YES 这一行没有注释掉，而我们目前的网络环境还不支持ipv6，从而导致出现错误无法启动，所以解决方法是将 listen_ipv6=YES更改为：listen_ipv6=NO，或将这一行注释掉 被动模式报错 425 Security: Bad IP connecting.解决：修改vsftp服务器端主配置文件，添加字段（原因，被动模式客户端拿到了服务器端的内网地址所以无法建立连接）pasv_address＝61.52.112.30 配置默认目录说明 配置文件说明|目录文件| 说明||—|—|—||/usr/sbin/vsftpd| VSFTPD的主程序||/etc/rc.d/init.d/vsftpd| 启动脚本||/etc/vsftpd/vsftpd.conf| 主配置文件||/etc/pam.d/vsftpd| PAM认证文件||/etc/vsftpd.ftpusers| 禁止使用VSFTPD的用户列表文件||/etc/vsftpd.user_list| 禁止或允许使用VSFTPD的用户列表文件||/var/ftp| 匿名用户主目录||/var/ftp/pub| 匿名用户的下载目录| FTP命令 命令详解12345678910111213141516171819202122232425262728293031323334ftp&gt; ascii # 设定以ASCII方式传送文件(缺省值)ftp&gt; bell # 每完成一次文件传送,报警提示.ftp&gt; binary # 设定以二进制方式传送文件.ftp&gt; bye # 终止主机FTP进程,并退出FTP管理方式.ftp&gt; case # 当为ON时,用MGET命令拷贝的文件名到本地机器中,全部转换为小写字母.ftp&gt; cd # 同UNIX的CD命令.ftp&gt; cdup # 返回上一级目录.ftp&gt; chmod # 改变远端主机的文件权限.ftp&gt; close # 终止远端的FTP进程,返回到FTP命令状态, 所有的宏定义都被删除.ftp&gt; delete # 删除远端主机中的文件.ftp&gt; dir [remote-directory] [local-file] # 列出当前远端主机目录中的文件.如果有本地文件,就将结果写至本地文件.ftp&gt; get [remote-file] [local-file] # 从远端主机中传送至本地主机中.ftp&gt; help [command] # 输出命令的解释.ftp&gt; lcd # 改变当前本地主机的工作目录,如果缺省,就转到当前用户的HOME目录.ftp&gt; ls [remote-directory] [local-file] # 同DIR.ftp&gt; macdef # 定义宏命令.ftp&gt; mdelete [remote-files] # 删除一批文件.ftp&gt; mget [remote-files] # 从远端主机接收一批文件至本地主机.ftp&gt; mkdir directory-name # 在远端主机中建立目录.ftp&gt; mput local-files # 将本地主机中一批文件传送至远端主机.ftp&gt; open host [port] # 重新建立一个新的连接.ftp&gt; prompt # 交互提示模式.ftp&gt; put local-file [remote-file] # 将本地一个文件传送至远端主机中.ftp&gt; pwd # 列出当前远端主机目录.ftp&gt; quit # 同BYE.ftp&gt; recv remote-file [local-file] # 同GET.ftp&gt; rename [from] [to] # 改变远端主机中的文件名.ftp&gt; rmdir directory-name # 删除远端主机中的目录.ftp&gt; send local-file [remote-file] # 同PUT.ftp&gt; status # 显示当前FTP的状态.ftp&gt; system # 显示远端主机系统类型.ftp&gt; user user-name [password] [account] # 重新以别的用户名登录远端主机.ftp&gt; ? [command] # 同HELP. [command]指定需要帮助的命令名称。如果没有指定 command，ftp 将显示全部命令的列表。ftp&gt; ! # 从 ftp 子系统退出到外壳。 参考连接FTP安装配置清水的博客 以上","categories":[],"tags":[{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"},{"name":"网络协议","slug":"网络协议","permalink":"http://arvon.top/tags/网络协议/"}]},{"title":"Shell编程多线程并发实践","slug":"Shell编程多线程并发实践","date":"2017-08-02T14:15:00.000Z","updated":"2017-08-04T04:06:39.000Z","comments":true,"path":"2017/08/02/Shell编程多线程并发实践/","link":"","permalink":"http://arvon.top/2017/08/02/Shell编程多线程并发实践/","excerpt":"Tips：正常来说shell是不具备多线程能力的，也就是说shell本身是按顺序进行执行指令的，并不能并发执行。但是可以换个思路，通过遍历+后台执行的方式进行模拟多线程的方式。但是还有个问题就是如何去控制并发数量，比如你要批量往1w台机器上copy一个文件，如果不设置并发数，估计直接就带宽跑满game over了，不过也有对应的解决方案，往下看吧。","text":"Tips：正常来说shell是不具备多线程能力的，也就是说shell本身是按顺序进行执行指令的，并不能并发执行。但是可以换个思路，通过遍历+后台执行的方式进行模拟多线程的方式。但是还有个问题就是如何去控制并发数量，比如你要批量往1w台机器上copy一个文件，如果不设置并发数，估计直接就带宽跑满game over了，不过也有对应的解决方案，往下看吧。 并发方案模型 无并发设置 12345678910#!/bin/bashecho `date`echo \"-----------Arvon.top Loop Begin-------------\"for i in `seq 3`;do sleep 10 echo $i echo `date`doneecho \"-----------Arvon.top Loop End-------------\"echo `date` 输出 12345678910Wed Aug 2 12:53:25 UTC 2017-----------Arvon.top Loop Begin-------------1Wed Aug 2 12:53:35 UTC 20172Wed Aug 2 12:53:45 UTC 20173Wed Aug 2 12:53:55 UTC 2017-----------Arvon.top Loop End-------------Wed Aug 2 12:53:55 UTC 2017 无并发数设置Tips:敲黑板，注意wait这个指令，需要在如果需要循环体内所有任务执行完成后再执行其他指令，需要在循坏外紧接着加上 12345678910111213#!/bin/bashecho `date`echo \"-----------Arvon.top Loop Begin-------------\"for i in `seq 3`;do&#123; sleep 10 echo $i echo `date`&#125;&amp;donewaitecho \"-----------Arvon.top Loop End-------------\"echo `date` 输出 12345678910Wed Aug 2 13:04:34 UTC 2017-----------Arvon.top Loop Begin-------------123Wed Aug 2 13:04:44 UTC 2017Wed Aug 2 13:04:44 UTC 2017Wed Aug 2 13:04:44 UTC 2017-----------Arvon.top Loop End-------------Wed Aug 2 13:04:44 UTC 2017 有并发数设置Tips：这里需要用到一种特殊的文件名称为有名管道（FIFO），这个具体会另写一篇，注意&amp;6和read这些地方 123456789101112131415161718192021222324252627282930313233#!/bin/bashserver_list=`seq 10`thread=3# Create FIFOtmp_fifofile=\"/tmp/$$.fifo\"mkfifo $tmp_fifofileexec 6&lt;&gt; $tmp_fifofilerm $tmp_fifofilefor ((i=0;i&lt;$thread;i++)); doechodone &gt;&amp; 6# user operationfunction user_scripts()&#123; sleep 10 echo `date`&#125;echo `date`echo \"-----------Arvon.top Loop Begin-------------\"for i in $&#123;server_list&#125;;doread -u6&#123; user_scripts echo &gt;&amp;6&#125;&amp;done &lt;&amp;6waitexec 6&gt;&amp;-echo \"-----------Arvon.top Loop End-------------\"echo `date` 输出 1234567891011121314Wed Aug 2 14:12:03 UTC 2017-----------Arvon.top Loop Begin-------------Wed Aug 2 14:12:13 UTC 2017Wed Aug 2 14:12:13 UTC 2017Wed Aug 2 14:12:13 UTC 2017Wed Aug 2 14:12:23 UTC 2017Wed Aug 2 14:12:23 UTC 2017Wed Aug 2 14:12:23 UTC 2017Wed Aug 2 14:12:33 UTC 2017Wed Aug 2 14:12:33 UTC 2017Wed Aug 2 14:12:33 UTC 2017Wed Aug 2 14:12:43 UTC 2017-----------Arvon.top Loop End-------------Wed Aug 2 14:12:43 UTC 2017 实战小栗子 例子1：需要 本机上进行并行运行脚本 每次并行需要不同的配置文件 代码如下： 12345678910111213#!/bin/bash# cat server.csv | grep gid &gt; confd/for i in `cat server.csv | egrep -v \"gid\"`;do sid=`echo $i |awk -F',' '&#123;print $2&#125;'` cat server.csv | grep gid &gt; confd/$&#123;sid&#125;_server.csv echo $i &gt;&gt; confd/$&#123;sid&#125;_server.csv &#123; echo \"./tool_refresh_gs -save -config confd/$&#123;sid&#125;_server.csv -out $&#123;sid&#125;\" #./tool_refresh_gs -save -config confd/$&#123;sid&#125;_server.csv -out $&#123;sid&#125; &#125;&amp;donewait 不错的写的很不错","categories":[],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://arvon.top/tags/Shell/"}]},{"title":"Rsync服务安装配置记录","slug":"Rsync服务安装配置记录","date":"2017-07-24T14:00:00.000Z","updated":"2017-07-25T06:12:21.000Z","comments":true,"path":"2017/07/24/Rsync服务安装配置记录/","link":"","permalink":"http://arvon.top/2017/07/24/Rsync服务安装配置记录/","excerpt":"关于RSYNC，个人理解传送大量资源文件或较多小文件时适合使用传输，具有数据验证、断点续传、增量传输、差异传输等特性。优于scp及ftp等工具。 rsync是unix/linux下同步文件的一个高效算法，它能同步更新两处计算机的文件与目录，并适当利用查找文件中的不同块以减少数据传输。rsync中一项与其他大部分类似程序或协定中所未见的重要特性是镜像是只对有变更的部分进行传送。rsync可拷贝／显示目录属性，以及拷贝文件，并可选择性的压缩以及递归拷贝。rsync利用由Andrew Tridgell发明的算法… 摘自Rsync的核心算法","text":"关于RSYNC，个人理解传送大量资源文件或较多小文件时适合使用传输，具有数据验证、断点续传、增量传输、差异传输等特性。优于scp及ftp等工具。 rsync是unix/linux下同步文件的一个高效算法，它能同步更新两处计算机的文件与目录，并适当利用查找文件中的不同块以减少数据传输。rsync中一项与其他大部分类似程序或协定中所未见的重要特性是镜像是只对有变更的部分进行传送。rsync可拷贝／显示目录属性，以及拷贝文件，并可选择性的压缩以及递归拷贝。rsync利用由Andrew Tridgell发明的算法… 摘自Rsync的核心算法 服务器端 安装服务 1234567891011yum install xinetd -yyum install rsync -ymkdir /etc/rsyncdtouch /etc/rsyncd/rsyncd.pwdchmod 600 /etc/rsyncd/rsyncd.pwd#创建密码文件并更改权限touch /etc/rsyncd/rsyncd.motdecho \"---Hello It's Rsync---\" &gt; /etc/rsyncd/rsyncd.motd#创建欢迎信息touch /etc/rsyncd/rsyncd.conf#创建配置文件 配置文件实例 123456789101112131415161718192021222324252627282930#vim /etc/rsyncd/rsyncd.confpid file = /var/run/rsyncd.pidport = 873#address = 192.168.1.11 #监听地址，可不填uid = rootgid = root#为避免权限问题，此处使用root，此用户操作的是xinteduse chroot = yesread only = no#只读选择，只让客户端从服务器上读取文件#write only = yes #只写选择，只让客户端到服务器上写入hosts allow = xxxxxxxx/24max connections = 5motd file = /etc/rsyncd/rsyncd.motdlog file = /var/log/rsync.logtransfer logging = yeslog format = %t %a %m %f %bsyslog facility = local3timeout = 0# 设置为0为无限制，简易为600#----Mode rsync---[binlogshome]path = /data/bilogs_S3_haiwai/userinfo_guildinfo_zip/list = no#当查看服务器上提供了哪些目录时是否列出来，no比较安全ignore errors#忽略I/O错误secrets file = /etc/rsyncd/rsyncd.pwd#exclude = error_log httpd.pid #忽略的文件或目录#comment this is my log #本模块注释 编辑密码文件 12#vim /etc/rsyncd/rsyncd.pwdarvon123:A35q3FhoXTJ4FRMqm 启动Rsync服务 命令行启动 1/usr/bin/rsync --daemon --config=/etc/rsyncd/rsyncd.conf xinetd方式启动 1234567891011121314#vim /etc/xinetd.d/rsync#service rsync#&#123;# disable = no# socket_type = stream# wait = no# user = root# server = /usr/bin/rsync# server_args = --daemon# log_on_failure += USERID#&#125;ln -s /etc/rsyncd/rsyncd.conf /etc/rsyncd.conf#xinted 默认会去/etc下查找配置，所以做个软链就可以了/etc/init.d/xinted start 客服端命令 安装rsync命令 设置密码文件 1echo \"A35q3FhoXTJ4FRMqm\" &gt;/etc/rsync.password 同步命令 1rsync -avzP arvon123@xxxxxx::binlogshome /data/bilogs_S3_haiwai/ --password-file=/etc/rsync.password 常用优化相关 传输大量备份文件 半夜花了一个多小时查看了rsync的文档，发现有一个参数能快速恢复大文件的增量同步，–append。设置–append参数会在增量同步时计算文件大小并直接追加新的数据到文件，这样就省了费IO校验的过程。不过这个参数最好只在源文件和目标文件都不会更改的时候使用比较安全，比如备份的文件。 使源目录保存较少文件 这是一个传统优化办法，因为rsync虽然是同步所有文件，但和同步最近更新的文件是一个道理，因此将源服务器上的目录删除，仅仅保持最近更新的文件，文件数量就变得不但很少，而且是稳定的，随着时间推移，这数量也不会涨得很快。但这样做有个缺点，就是rsync不能使用删除模式，如果有文件要删除，可以将其弄成空文件，假如有更严格要求，可以另一个程序来删除。 参考文献关于rsync命令John_ABC的博客传输优化相关常见错误处理","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"},{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"}]},{"title":"弱网终端替代工具Mosh","slug":"弱网终端替代工具Mosh","date":"2017-07-17T15:00:45.000Z","updated":"2017-11-28T10:08:39.000Z","comments":true,"path":"2017/07/17/弱网终端替代工具Mosh/","link":"","permalink":"http://arvon.top/2017/07/17/弱网终端替代工具Mosh/","excerpt":"应用背景：当有海外服务器维护需求的时候，常常会遇到使用ssh终端延时高并经常性假死的情况，非常影响维护效率。然后一个能缓解延时假死的小工具就是十分有用的，Mosh正是这样一个工具。Mosh是一个用于从客户端跨互联网连接远程服务器的命令行工具，程序最初由Keith Winstein 编写，用于类Unix的操作系统中，发布于GNU GPL V3协议下，基于UDP方式进行数据传输。","text":"应用背景：当有海外服务器维护需求的时候，常常会遇到使用ssh终端延时高并经常性假死的情况，非常影响维护效率。然后一个能缓解延时假死的小工具就是十分有用的，Mosh正是这样一个工具。Mosh是一个用于从客户端跨互联网连接远程服务器的命令行工具，程序最初由Keith Winstein 编写，用于类Unix的操作系统中，发布于GNU GPL V3协议下，基于UDP方式进行数据传输。 Mosh的优缺点： 优点：1.回话保持（会话的中断不会导致当前正在前端执行的命令中断；会话在中断过后，会自动在当前会话重新连接）2.基本支持全平台（Linux、FreeBSD、Solaris、Mac OS X和Android）3.切换网络/暂时断网后回话自动连接 缺点：1.需要开启额外UDP端口2.在网络状况良好的状况下不宜使用3.状态同步、维持心跳、协助预测的ACK包都增加了传输的数据量 Mosh Server安装配置Tips：我的环境为aws的ec2，另外需要开放一些UDP端口，官方默认是让开60000-61000端口，生产环境下最好对端口进行更改 安装环境依赖包 1yum install -y autoconf automake libtool curl make g++ unzip zlib zlib-devel ncurses-devel openssl-devel 安装protobuf的依赖 12345678910wget https://github.com/google/protobuf/releases/download/v3.3.0/protobuf-cpp-3.3.0.tar.gztar xvf protobuf-cpp-3.3.0.tar.gzcd protobuf-cpp-3.3.0#./configure --prefix=/usr/local/protobuf./configure – prefix=/usr – libdir=/usr/lib64makemake checkmake install#echo \"/usr/local/protobuf/lib/\" &gt;&gt; /etc/ld.so.conf#ldconfig 可能需要 123#export LD_LIBRARY_PATH=/usr/local/protobuf/lib/#export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/protobuf/lib/#export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig 安装mosh 12345678wget https://mosh.org/mosh-1.3.0.tar.gztar xvf mosh-1.3.0.tar.gzcd mosh-1.3.0./autogen.sh./configure --prefix=/usr/local/moshmakemake installexport PATH=$PATH:/usr/local/mosh/bin/ 启动mosh server 1234567# 启动进程/usr/local/bin/mosh-server#要确保mosh-server在用户的PATH中，不然连接不上# 查看进程是否正常ps axu | grep mosh#Auser 1866 0.0 0.0 163172 5900 ? S 02:33 0:00 mosh-server new -c 256 -s -l LANG=zh_CN.UTF-8#Auser 2181 0.0 0.0 162612 5184 ? S 03:25 0:00 mosh-server new -c 256 -s -l LANG=zh_CN.UTF-8 Mosh Clinet安装配置支持全平台，我这是mac系统，所以就只写了mac的，如有需要在官方文档处进行查看，做对应的安装 MacOS客户端 1brew install mobile-shell 使用mosh连接服务器 123#我这里是先将本地的公钥写到要连接服务器的authorized_keys文件中/usr/local/bin/mosh --ssh=\"ssh -p 7777 \" Auser@arvon.top#我这里由于ssh的默认端口更改了，所以需要指定端口7777 参考文档Mosh官方介绍Github项目MoshBINSITE的blog","categories":[],"tags":[{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"},{"name":"网络协议","slug":"网络协议","permalink":"http://arvon.top/tags/网络协议/"},{"name":"网络安全","slug":"网络安全","permalink":"http://arvon.top/tags/网络安全/"}]},{"title":"Nginx添加登录认证","slug":"Nginx添加登录认证","date":"2017-07-16T05:23:00.000Z","updated":"2017-07-18T08:45:31.000Z","comments":true,"path":"2017/07/16/Nginx添加登录认证/","link":"","permalink":"http://arvon.top/2017/07/16/Nginx添加登录认证/","excerpt":"之前在爆Zabbix漏洞的时候，在未进行漏洞升级修复的情况下，可以先对nginx的web入口加一个认证，这样就能很大程度上降低被黑的风险。同理在使用Nginx代理一些服务的时候，也可以用这种方法进行安全上的加固。做起来比较简单，却行之有效。Nginx上一般使用的认证方式有2种，分别是auth_basic（本机认证）及ngx_http_auth_request_module（第三方认证）","text":"之前在爆Zabbix漏洞的时候，在未进行漏洞升级修复的情况下，可以先对nginx的web入口加一个认证，这样就能很大程度上降低被黑的风险。同理在使用Nginx代理一些服务的时候，也可以用这种方法进行安全上的加固。做起来比较简单，却行之有效。Nginx上一般使用的认证方式有2种，分别是auth_basic（本机认证）及ngx_http_auth_request_module（第三方认证） auth_basic(本机认证)配置 如给web服务设置代理nginx配置文件如下 12345678910111213server &#123; listen 7777; server_name 127.0.0.1; auth_basic \"test-web\"; auth_basic_user_file /usr/local/nginx/testweb.db; location / &#123; proxy_pass http://11.11.11.5:80; proxy_set_header Host $host:$server_port; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_redirect off; &#125; #charset koi8-r;&#125; 创建登录密码文件 12#可以使用htpasswd，或者使用openssl printf \"taiyouxi:$(openssl passwd -crypt YourPassword)\\n\" &gt;/usr/local/nginx/testweb.db 设置说明： –开启认证–默认值: auth_basic off;语法: auth_basic string | off;如使用auth_basic “test-web”;会在弹窗上显示test-web的字样–密码文件说明–语法: auth_basic_user_file file;配置段: http, server, location, limit_except ngx_http_auth_request_module（第三方认证）配置由于这个模块并不属于内置模块，默认使用yum安装是不会带这个模块的，可以下载源码重新带上这个模块编译安装。 原yum默认编译参数查看 12[root@ip-10-222-0-85 nginx]# nginx -V#结果太多不贴了，就是没有所需的这个模块，所以就再编一个测试 先源码安装nginx 12345678yum -y install gcc gcc-c++ make libtool zlib zlib-devel openssl openssl-devel pcre pcre-develwget http://nginx.org/download/nginx-1.10.3.tar.gztar xvf nginx-1.10.3.tar.gzgit clone git://github.com/perusio/nginx-auth-request-module.gitcd nginx-1.10.3./configure --prefix=/usr/local/nginx --add-module=../nginx-auth-request-modulemakemake install 测试代码 12345678910111213141516171819server &#123; listen 20010; server_name 127.0.0.1; location / &#123; auth_request /auth; proxy_pass http://127.0.0.1:80; proxy_set_header Host $host:$server_port; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_redirect off; #charset koi8-r; &#125; location = /auth &#123; proxy_pass http://127.0.0.1:80/passwd/HttpBasicAuthenticate.php; proxy_pass_request_body off; proxy_set_header Content-Length \"\"; proxy_set_header X-Original-URI $request_uri; &#125;&#125; php认证代码 12345678910111213141516#vim html/passwd/HttpBasicAuthenticate.php&lt;?phpif(isset($_SERVER['PHP_AUTH_USER'], $_SERVER['PHP_AUTH_PW']))&#123; $username = $_SERVER['PHP_AUTH_USER']; $password = $_SERVER['PHP_AUTH_PW']; if ($username == 'wang' &amp;&amp; $password == '123456')&#123; return true; &#125;&#125;header('WWW-Authenticate: Basic realm=\"Git Server\"');header('HTTP/1.0 401 Unauthorized');?&gt; 说明 用户访问server 弹出框中输入的用户名、密码保存在 $_SERVER 变量中中间 if 段，只做演示用实际中应该是拿用户输入的用户名、密码跟数据库中的数据做比较参考：http://www.cnblogs.com/wangxiaoqiangs/p/6184181.html","categories":[],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://arvon.top/tags/Nginx/"},{"name":"网络安全","slug":"网络安全","permalink":"http://arvon.top/tags/网络安全/"}]},{"title":"AWS-cli操作S3及DynamoDB","slug":"AWS-cli操作S3及DynamoDB","date":"2017-07-08T10:23:45.000Z","updated":"2017-07-08T11:59:32.000Z","comments":true,"path":"2017/07/08/AWS-cli操作S3及DynamoDB/","link":"","permalink":"http://arvon.top/2017/07/08/AWS-cli操作S3及DynamoDB/","excerpt":"AWS的cli还是挺好用的，文档也比较全面，其实看个文档就可以使用，但是我毕竟是我，记下来，减少查询搜索的时间也是挺赚的，不是么。。话说“懒人使世界进步”，其实有个很重要的前提就是你这个懒人做出了可以帮你出色完成原定任务的东西，这时你才有资格懒。很显然，我没有。。。But我一直在实现这个前提的路上前进","text":"AWS的cli还是挺好用的，文档也比较全面，其实看个文档就可以使用，但是我毕竟是我，记下来，减少查询搜索的时间也是挺赚的，不是么。。话说“懒人使世界进步”，其实有个很重要的前提就是你这个懒人做出了可以帮你出色完成原定任务的东西，这时你才有资格懒。很显然，我没有。。。But我一直在实现这个前提的路上前进 S3操作Tips：官方文档看这里，好吧，中文看这里 创建bucket 1aws s3 mb s3://arvon-test-bucket1 上传文件到bucket 1234#上传文件aws s3 cp README.md s3://arvon-test-bucket1/#上传目录aws s3 cp files s3://arvon-test-bucket1/files --recursive 删除文件或目录 1234#删除文件aws s3 rm s3://arvon-test-bucket1/README.md#删除目录aws s3 rm s3://arvon-test-bucket1/files --recursive 下载文件或目录 1234#下载文件aws s3 cp s3://arvon-test-bucket1/README.md ./#下载目录aws --region ap-southeast-1 s3 cp s3://arvon-test-bucket1 ./ --recursive 删除bucket 1234#删除空bucketaws s3 rb s3://arvon-test-bucket1#删除非空bucketaws s3 rb s3://arvon-test-bucket1 --force DynamoDB操作Tips:官方文档看这里,为了方便自己记忆就动手实践一遍，顺便记录一下，当然还有国语版 啰嗦几个点1）创建含排序键的表示，指定key-schema时一定是先指定HASH这个类型再指定RANGE这个，不然一定报错2）如需修改读写容量修改命令最后的provisioned-throughput下面对应值即可3）就是需要提前设置连接服务器的环境变量如key及region，不然需命令行指定 创建dynamoDB表创建表含排序键 1aws dynamodb create-table --table-name Arvon-test1 --attribute-definitions AttributeName=ArvonMeID,AttributeType=S AttributeName=moid,AttributeType=S --key-schema AttributeName=ArvonMeID,KeyType=HASH AttributeName=moid,KeyType=RANGE --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 创建表不含排序建 1aws dynamodb create-table --table-name Arvon-test2 --attribute-definitions AttributeName=Hello,AttributeType=S --key-schema AttributeName=Hello,KeyType=HASH --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 删除dynamo表 1aws dynamodb delete-table --table-name Arvon-test1 参考官方就是好用","categories":[],"tags":[{"name":"云服务","slug":"云服务","permalink":"http://arvon.top/tags/云服务/"}]},{"title":"记ARDB频繁崩溃错误","slug":"记ARDB频繁崩溃错误","date":"2017-07-07T15:23:45.000Z","updated":"2017-07-08T08:23:49.000Z","comments":true,"path":"2017/07/07/记ARDB频繁崩溃错误/","link":"","permalink":"http://arvon.top/2017/07/07/记ARDB频繁崩溃错误/","excerpt":"背景：在业务规模对db没有高需求的情况下使用redis存储其实是有些浪费的，毕竟内存要比磁盘贵很多。这也就是我们要把部分db从redis迁移到ardb的原因，虽然在上线之前已经做了内网测试以及线上部分db替换测试，但是并未触发ardb的这个bug，其中线上测试了30d，现在分析看来，应该是因为线上替换的这个db压力不高，用户基数小。记录一下，关于ardb可以看这里","text":"背景：在业务规模对db没有高需求的情况下使用redis存储其实是有些浪费的，毕竟内存要比磁盘贵很多。这也就是我们要把部分db从redis迁移到ardb的原因，虽然在上线之前已经做了内网测试以及线上部分db替换测试，但是并未触发ardb的这个bug，其中线上测试了30d，现在分析看来，应该是因为线上替换的这个db压力不高，用户基数小。记录一下，关于ardb可以看这里 从崩溃到崩溃 现象：直接dwon掉，ardb的log没有记录 系统日志：在/var/log/message中有如下日志 kernel: [20758288.642227] rocksdb:bg7[25004]: segfault at 7f370d206e7e ip 0000000000578f46 sp 00007f370fbfde90 error 6 in ardb-server[400000+423000] addr2排查： 12[root@ip-10-33-4-xx ardb-0.9.4]# addr2line -e ./src/ardb-server 0000000000578f46/opt/ardb/ardb-0.9.4/src/db/rocksdb/rocksdb_engine.cpp:222 撸源码报错段 12345if (n &gt; 0 &amp;&amp; NULL != buffer)&#123; buffer[n] = 0; LOG_WITH_LEVEL(level, \"[RocksDB]%s\", buffer);&#125; 撸代码修改为（rocksdb_engine.cpp:222修改源代码中这个文件的222行）说明：这个代码经经沟通是一个临界问题，在win下和Linux下表现是不一样的，截取字符串存在问题，下面给出的是修复代码。因我并不懂C++所以理解有限，欢迎更正补充。 123456789101112if (n &gt; 0)&#123; if (n &gt;= buf_len) &#123; buffer[buf_len-1] = 0; LOG_WITH_LEVEL(level, \"[RocksDB]%s\", buffer); &#125; else &#123; buffer[n] = 0; LOG_WITH_LEVEL(level, \"[RocksDB]%s\", buffer); &#125;&#125; 总结这里我个人说三点对于更换DB的注意事项 1.数据备份，并需考虑备份数据对于前后DB的可用性，如Redis的备份数据ARDB是否可用，是否备份还原策略可以准确顺利2.对新DB的调研一定要充分。如系统平台、程序版本、底层依赖、bug反馈修复速率3.新DB是否有大量的成功企业实践，实践企业反馈4.回滚至原有DB方案制定、可行性实践及分析5.新DB备份策略 排查手段这里介绍一下Linux下的core dump机制 1.什么是core dump当程序运行的过程中异常终止或崩溃，操作系统会将程序当时的内存状态记录下来，保存在一个文件中，这种行为就叫做Core Dump（中文有的翻译成“核心转储”)。我们可以认为 core dump 是“内存快照”，但实际上，除了内存信息之外，还有些关键的程序运行状态也会同时 dump 下来，例如寄存器信息（包括程序指针、栈指针等）、内存管理信息、其他处理器和操作系统状态和信息。core dump 对于编程人员诊断和调试程序是非常有帮助的，因为对于有些程序错误是很难重现的，例如指针异常，而 core dump 文件可以再现程序出错时的情景。2.如何产生core dump参考这个吧http://www.cnblogs.com/hazir/p/linxu_core_dump.html这个链接包含了详细信息，不过为了方便我还是在下面啰嗦一下 如何开启core dump 12345#立即开启，重启后失效ulimit -c unlimited#修改配置，重启后生效vim /etc/security/limits.conf #添加如下行* soft core unlimited 如何查看调试core文件 1gdb core_demo core_demo.core.24816 以上，换DB事情蛮大的，还是要慎重再慎重，表示线上频繁崩溃很绝望，记录一下引以为戒。","categories":[],"tags":[{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"},{"name":"踩坑指南","slug":"踩坑指南","permalink":"http://arvon.top/tags/踩坑指南/"}]},{"title":"open-falcon业务监控实践","slug":"open-falcon业务监控实践","date":"2017-07-06T11:50:45.000Z","updated":"2017-07-25T08:11:12.000Z","comments":true,"path":"2017/07/06/open-falcon业务监控实践/","link":"","permalink":"http://arvon.top/2017/07/06/open-falcon业务监控实践/","excerpt":"上一篇已经写了falcon的基础安装配置以及简单的使用说明，现在来记录下一些业务相关的监控如何做，截图居多。心疼流量三秒…其实实现方法都不难，设计一个良好的命名规范以及科学的分组，实际上对监控来说是十分必要的。还有就是要对监控指标的判断做个性化设置，最好就是看图的话一眼就概览全局（要让图清晰直观的反应状况），报警的话做到真实有效人性化。这篇里面只涉及如何对常见服务进行配置。 对于基础监控：cpu、内存、IO、网络适合折线图；而磁盘使用量这些其实设置多维度报警即可（如磁盘使用率&gt;80%且小于20G报警） 对于服务和端口：监控出数字显示服务总数量，服务正常数量、服务异常数量，然后配置报警即可 对于业务监控：对业务指标使用折线图一般都是适用的","text":"上一篇已经写了falcon的基础安装配置以及简单的使用说明，现在来记录下一些业务相关的监控如何做，截图居多。心疼流量三秒…其实实现方法都不难，设计一个良好的命名规范以及科学的分组，实际上对监控来说是十分必要的。还有就是要对监控指标的判断做个性化设置，最好就是看图的话一眼就概览全局（要让图清晰直观的反应状况），报警的话做到真实有效人性化。这篇里面只涉及如何对常见服务进行配置。 对于基础监控：cpu、内存、IO、网络适合折线图；而磁盘使用量这些其实设置多维度报警即可（如磁盘使用率&gt;80%且小于20G报警） 对于服务和端口：监控出数字显示服务总数量，服务正常数量、服务异常数量，然后配置报警即可 对于业务监控：对业务指标使用折线图一般都是适用的 监控自定义服务进程或端口Tips：一般而言，对于某一服务仅监听服务或端口取其一即可，因为一般而言进程挂了端口自然也就down了，这两者是有直接关联的。但也分业务，具体看需求吧 监控端口Tips：端口监控，falcon现在的机制是配置完策略agent才会去采集这些信息。可参考官网说明 设置端口监控模板 模板关联至主机组 然后就可以收到报警信息了(这里为测试方便监控了Mysql的3306端口，有值就报警) 监控服务Tips：官方给出两种方案，一种取/proc/$pid/status这里面的name字段，另一种取/proc/$pid/cmdline里的name字段，这里拿mysql举例，个人推荐cmdline，因为name字段很容易重复 获取两个字段 123456789101112 ps axu | grep mysql #root 5465 0.0 0.0 113176 1600 ? S May18 0:00 /bin/sh /usr/libexec/mysql55/mysqld_safe --datadir=/var/lib/mysql --socket=/var/lib/mysql/mysql.sock --pid-file=/var/run/mysqld/mysqld.pid --basedir=/usr --user=mysql#mysql 5658 0.0 1.2 1336076 101032 ? Sl May18 27:08 /usr/libexec/mysql55/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/lib64/mysql/plugin --user=mysql --log-error=/var/log/mysqld.log --pid-file=/var/run/mysqld/mysqld.pid --socket=/var/lib/mysql/mysql.sock#tai_ops 28522 0.0 0.0 110408 868 pts/0 S+ 03:04 0:00 grep mysql cat /proc/5465/status | grep -i name #Name: mysqld_safe cat /proc/5465/cmdline #/bin/sh/usr/libexec/mysql55/mysqld_safe--datadir=/var/lib/mysql--socket=/var/lib/mysql/mysql.sock--pid-file=/var/run/mysqld/mysqld.pid--basedir=/usr--user=mysql cat /proc/5658/status | grep -i name #Name: mysqld cat /proc/5658/cmdline #/usr/libexec/mysql55/mysqld--basedir=/usr--datadir=/var/lib/mysql--plugin-dir=/usr/lib64/mysql/plugin--user=mysql--log-error=/var/log/mysqld.log--pid-file=/var/run/mysqld/m Dashboard上模板配置 报警如下注意：我测试的host组有3台机器，其中一台是没有mysql的，所以检测不到进程所以就报警了，从这也可以看出，falcon默认是不收集这些数据的，当你写上规则之后它会按这个规则进行抓取不管有没有这个服务，只要你配置了规则，就按这个规则去取数据，取不到或取到报警阈值都会报警 常用服务监控Tips：常用服务监控除了官方提供的一些方案也可以自己往transfer接口post自定义的监控数据，不过个人感觉使用crontab的方式收集数据还是挺僵硬的 Redis监控/Ardb监控官方提供了两种方案，都在github开源,由于第二种方案目前还不支持redis3.2.0，所以我使用第一种。实际上就是连接到redis然后获取一些redis的info信息，然后用固定的数据格式post给falcon，ardb与redis监控基本相同。 获取监控脚本需要特别注意脚本中redis-cli的命令路径，我就折在这里了 12345git clone https://github.com/iambocai/falcon-monit-scripts.gitll /data/falcon_scripts/falcon-monit-scripts/redis/redis-monitor.py#vim redis-monitor.py 修改脚本，主要该host、port、redis-cli的路径#设置crontab -e#* * * * * python /opt/falcon-agent/scripts/falcon-monit-scripts/redis/redis-monitor.py web配置如下 出图如下 Mysql监控由于小米官方给出的方案是需要go环境进行的编译的，所以就直接在falcon的server机器上进行编译，然后拿编译的包对mysql机器进行分发部署 下载并编译监控mysql的脚本源码这里监控连接数据库的用户必须是root不然会access deney 12345678910cd $GOPATH/src/github.com/open-falcongit clone https://github.com/open-falcon/mymon.gitcd mymongo get ./...go build -o mymon#crontab设置 crontab -e 或直接写配置#* * * * * cd $GOPATH/src/github.com/open-falcon/mymon &amp;&amp; ./mymon -c etc/mon.cfg#echo '* * * * * cd $GOPATH/src/github.com/open-falcon/mymon &amp;&amp; ./mymon -c etc/mon.cfg' &gt; /etc/cron.d/mymontar czvf mysql_monitor.tar.gz mymon etc/#然后去部署这个tar包即可 调试正常后log输出具体监控项、tag等信息都可以通过日志获得，安装文档中有队metric的汇总表 …{“level”:”debug”,”msg”:”MetaData Metric:Innodb_mutex_spin_waits Endpoint:ip-10-222-0-63 Value:302 CounterType:COUNTER Tags:port=3306 Timestamp:1499389081 Step:60”,”time”:”2017-07-07T00:58:01Z”}{“level”:”debug”,”msg”:”MetaData Metric:Innodb_mutex_spin_rounds Endpoint:ip-10-222-0-63 Value:1780 CounterType:COUNTER Tags:port=3306 Timestamp:1499389081 Step:60”,”time”:”2017-07-07T00:58:01Z”}{“level”:”debug”,”msg”:”MetaData Metric:Innodb_mutex_os_waits Endpoint:ip-10-222-0-63 Value:42 CounterType:COUNTER Tags:port=3306 Timestamp:1499389081 Step:60”,”time”:”2017-07-07T00:58:01Z”}{“level”:”debug”,”msg”:”MetaData Metric:Is_slave Endpoint:ip-10-222-0-63 Value:0 CounterType:GAUGE Tags:port=3306 Timestamp:1499389081 Step:60”,”time”:”2017-07-07T00:58:01Z”}{“level”:”info”,”msg”:”Send response 127.0.0.1:3306: success”,”time”:”2017-07-07T00:58:01Z”}{“level”:”debug”,”msg”:”Send to http://10.222.0.44:1988/v1/push, size: 1”,”time”:”2017-07-07T00:58:01Z”}{“level”:”debug”,”msg”:”MetaData Metric:mysql_alive_local Endpoint:ip-10-222-0-63 Value:1 CounterType:GAUGE Tags:port=3306 Timestamp:1499389081 Step:60”,”time”:”2017-07-07T00:58:01Z”}{“level”:”info”,”msg”:”Alive data response 127.0.0.1:3306: success”,”time”:”2017-07-07T00:58:01Z”} Web配置举例 出图如下 nginx监控这个目前没什么需求，就直接先参考小米给出的方案吧,其实都大同小异，有需求的话之后可能会自己写一些业务方面的监控脚本。到时候再共享 关于排错 绘图数据流向及验证 agent-&gt;transfer-&gt;graph-&gt;query-&gt;dashboard可以从 graph** 的http接口进行验证","categories":[],"tags":[{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"},{"name":"Falcon","slug":"Falcon","permalink":"http://arvon.top/tags/Falcon/"}]},{"title":"open-falcon安装部署记录","slug":"open-falcon安装部署记录","date":"2017-06-29T11:50:45.000Z","updated":"2017-07-25T08:11:06.000Z","comments":true,"path":"2017/06/29/open-falcon安装部署记录/","link":"","permalink":"http://arvon.top/2017/06/29/open-falcon安装部署记录/","excerpt":"监控对运维来说是相当重要的，现在来说falcon确实相对zabbix有一定优势，不过也存在明显短板，优势是架构设计以及一些设计思路很好，就单说模板继承，这个相对zabbix就很有优势，毕竟是已经有了前人的经验，小米之前也是用的zabbix，所以大众常见的zabbix痛点在falcon上基本得到了改善，但falcon的UI真是很难受啊，不过毕竟火起来还不久，又是一个互联网领头企业开源的项目，还是很有前景的，不妨尝试一下，现在只是基本搭建完成，TAG系统和表达式这些还没开始尝试，最近应该还会了解这个，以下是安装配置及简单调试使用过程。","text":"监控对运维来说是相当重要的，现在来说falcon确实相对zabbix有一定优势，不过也存在明显短板，优势是架构设计以及一些设计思路很好，就单说模板继承，这个相对zabbix就很有优势，毕竟是已经有了前人的经验，小米之前也是用的zabbix，所以大众常见的zabbix痛点在falcon上基本得到了改善，但falcon的UI真是很难受啊，不过毕竟火起来还不久，又是一个互联网领头企业开源的项目，还是很有前景的，不妨尝试一下，现在只是基本搭建完成，TAG系统和表达式这些还没开始尝试，最近应该还会了解这个，以下是安装配置及简单调试使用过程。 架构图(来自小米官方)及组件说明 Agent: 部署在目标机器采集机器监控项Transfer： 数据接收端，转发数据到后端Graph和JudgeGraph： 操作rrd文件，存储监控数据Query：查询各个Graph数据，提供统一http查询接口Dashboard： 查询监控历史趋势图的web端Alarm：主要负责告警Task： 负责一些定时任务，索引全量更新，垃圾索引清理，自身组件监控 安装搭建环境准备我的版本环境： version: Amazon 2015.03Type: t2.largeGit version: 2.7.5 (need &gt;= 1.7.5)GO version: 1.8.3 (need &gt;= 1.6) 安装环境 123yum install -y redisyum install -y mysql-serveryum install -y git 安装go环境 123456789101112131415export HOME=/home/tai_opsexport WORKSPACE=$HOME/open-falcongo_pkg='go1.8.3.linux-amd64.tar.gz'cd $HOMEecho $HOME#wget http://dinp.qiniudn.com/go1.4.1.linux-amd64.tar.gz#wget https://storage.googleapis.com/golang/$&#123;go_pkg&#125;tar zxf $&#123;go_pkg&#125;mkdir -p $&#123;WORKSPACE&#125;/srcecho \"\" &gt;&gt; .bashrcecho 'export GOROOT=$HOME/go' &gt;&gt; .bashrcecho 'export GOPATH=$HOME/$&#123;WORKSPACE&#125;' &gt;&gt; .bashrcecho 'export PATH=$GOROOT/bin:$GOPATH/bin:$PATH' &gt;&gt; .bashrcecho \"\" &gt;&gt; .bashrcsource .bashrc 下载源码 123mkdir -p $GOPATH/src/github.com/open-falconcd $GOPATH/src/github.com/open-falcongit clone --recursive https://github.com/open-falcon/falcon-plus.git 初始化数据库 123456789101112mysql_host='localhost'mysql_user='root'mysql_pass='arvon2014'cd $GOPATH/src/github.com/open-falcon/falcon-plus/scripts/mysql/db_schema/for i in `ls`;do mysql -h $&#123;mysql_host&#125; -u$&#123;mysql_user&#125; -p$&#123;mysql_pass&#125; &lt; $idone#mysql -h $&#123;mysql_host&#125; -u$&#123;mysql_user&#125; -p$&#123;mysql_pass&#125; &lt; db_schema/graph-db-schema.sql#mysql -h $&#123;mysql_host&#125; -u$&#123;mysql_user&#125; -p$&#123;mysql_pass&#125; &lt; db_schema/dashboard-db-schema.sql#mysql -h $&#123;mysql_host&#125; -u$&#123;mysql_user&#125; -p$&#123;mysql_pass&#125; &lt; db_schema/portal-db-schema.sql#mysql -h $&#123;mysql_host&#125; -u$&#123;mysql_user&#125; -p$&#123;mysql_pass&#125; &lt; db_schema/links-db-schema.sql#mysql -h $&#123;mysql_host&#125; -u$&#123;mysql_user&#125; -p$&#123;mysql_pass&#125; &lt; db_schema/uic-db-schema.sql Build安装包可选是build全部组件还是个别组件,我这里选择了全部build 12345678cd $GOPATH/src/github.com/open-falcon/falcon-plus/# make all modulesmake all## make specified module#make agent### pack all modulesmake pack 基础组件安装配置 解压build安装包到需要的安装路径 1234mkdir -pv /data/tai-falconcp $GOPATH/src/github.com/open-falcon/falcon-plus/open-falcon-v0.2.0.tar.gz /datacd /data/tar -xvf open-falcon-v0.2.0.tar.gz -C /data/tai-falcon 调试各个组件主要就是要修改数据库等信息,如下 12345678910# /data/tai-falcon/hbs/config/cfg.json: \"database\": \"root:arvon2014@tcp(127.0.0.1:3306)/falcon_portal?loc=Local&amp;parseTime=true\",# /data/tai-falcon/aggregator/config/cfg.json: \"addr\": \"root:arvon2014@tcp(127.0.0.1:3306)/falcon_portal?loc=Local&amp;parseTime=true\",# /data/tai-falcon/nodata/config/cfg.json: \"dsn\": \"root:arvon2014@tcp(127.0.0.1:3306)/falcon_portal?loc=Local&amp;parseTime=true&amp;wait_timeout=604800\",# /data/tai-falcon/alarm/config/cfg.json: \"addr\": \"root:arvon2014@tcp(127.0.0.1:3306)/alarms?charset=utf8&amp;loc=Asia%2FChongqing\",# /data/tai-falcon/api/config/cfg.json: \"faclon_portal\": \"root:arvon2014@tcp(127.0.0.1:3306)/falcon_portal?charset=utf8&amp;parseTime=True&amp;loc=Local\",# /data/tai-falcon/api/config/cfg.json: \"graph\": \"root:arvon2014@tcp(127.0.0.1:3306)/graph?charset=utf8&amp;parseTime=True&amp;loc=Local\",# /data/tai-falcon/api/config/cfg.json: \"uic\": \"root:arvon2014@tcp(127.0.0.1:3306)/uic?charset=utf8&amp;parseTime=True&amp;loc=Local\",# /data/tai-falcon/api/config/cfg.json: \"dashboard\": \"root:arvon2014@tcp(127.0.0.1:3306)/dashboard?charset=utf8&amp;parseTime=True&amp;loc=Local\",# /data/tai-falcon/api/config/cfg.json: \"alarms\": \"root:arvon2014@tcp(127.0.0.1:3306)/alarms?charset=utf8&amp;parseTime=True&amp;loc=Local\",# /data/tai-falcon/graph/config/cfg.json: \"dsn\": \"root:arvon2014@tcp(127.0.0.1:3306)/graph?loc=Local&amp;parseTime=true\", 推荐启动顺序：judge &gt; graph &gt; transfer &gt; agent &gt; alarm &gt; api &gt; aggregator &gt; nodata &gt; gateway提示：主要调试直接看对应模块下的log即可 dashboard安装配置 安装开发包 12345pip install virtualenvyum install -y python-develyum install -y openldap-develyum install -y mysql-develyum groupinstall \"Development tools\" 下载dashboard的源码 12cd $HOME/open-falcongit clone https://github.com/open-falcon/dashboard 安装环境依赖 123cd $HOME/open-falcon/dashboard/virtualenv ./env./env/bin/pip install -r pip_requirements.txt 配置修改 1234vim ./gunicorn.confvim ./rrd/config.py#主要也是修改数据库，也可以修改服务端口./control start 访问dashboard测试这个没有默认的用户密码，自己注册后即可登录，且注册不会验证邮箱，登录后如下 Agent端配置 生成agent部署包 123cd $GOPATH/src/github.com/open-falcon/falcon-plus/modules/agent./control pack#pack 即可打出agent包，我这里是falcon-agent-5.1.2.tar.gz，然后把这个包部署到需要监控的机器上 配置启动agent 1234567mkdir ~/falcon_aget;cd ~/falcon_agentcp falcon-agent-5.1.2.tar.gz ./tar xvf falcon-agent-5.1.2.tar.gz##然后修改配置cp cfg.example.json cfg.json#vim cfg.jsoon#修改heartbeat、transfer的配置地址端口等改为对应配置即可 可以通过浏览器查看上报的监控数据 报警邮件设置（小米规范使用http接口，不过小米也有相应的封装工具，如果二次开发可以自己定制实现方法） 下载邮件封装程序源码123456789cd $GOPATH/github.com/open-falcon/git clone https://github.com/open-falcon/mail-provider.gitcd $GOPATH/github.com/open-falcon/mail-provider/# 开始build出包go get ./..../control build./control pack#测试邮件curl http://127.0.0.1:4000/sender/mail -d \"tos=youremail@126.com&amp;subject='hello'&amp;content='world'\" 报警配置 简要说明：大概说三种情况 正常：创建主机组–&gt;根据主机组创建监控模板–&gt;主机组关联模板 模板继承：在一种的情况下，很可能存在一个主机组里个别机器可能需要的阈值并不一样，这时可以继承这个主机组的通用模板，在此基础上做特别的设置，除此之外的监控项还按这个主机组的通用模板 TAG模式：这种模式可独立于模板模式进行，比较适合监控同个项目或者个别重要业务的场景 设置Hostgroup（主机组） 创建主机组 为主机组添加主机(主机填写hostname主机名即可) 设置Templates（模板） 创建模板 设置报警策略 主机组关联模板 查看报警 报警 参考文献小米官方手册0.2.0","categories":[],"tags":[{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"},{"name":"Falcon","slug":"Falcon","permalink":"http://arvon.top/tags/Falcon/"}]},{"title":"Linux启动流程","slug":"Linux启动流程","date":"2017-06-24T22:50:45.000Z","updated":"2017-06-26T06:50:03.000Z","comments":true,"path":"2017/06/25/Linux启动流程/","link":"","permalink":"http://arvon.top/2017/06/25/Linux启动流程/","excerpt":"背景故事：为啥想起来写Linux启动流程了呢，这个呢要从一个月以前说起，蜜汁尴尬~~~。我媳妇用的Ubuntu，然后她需要装很多环境，不知道为何内核老是更新，结果就是后面装东西的时候提示/boot分区空间不足无法安装，但我天真可爱的媳妇不会清呀，然后我就来了，一看简单啊，删呗，凡事总有BUT，我觉得这个事情嘛要清就要清彻底，就不rm-rf了，直接卸安装包多清爽，于是乎系统挂了，剁手啊打什么通配符。。。不过总结一下还是对系统层认识不够生动，所以就补一篇日志，万一我最棒的媳妇看到了呢，那就点点点~","text":"背景故事：为啥想起来写Linux启动流程了呢，这个呢要从一个月以前说起，蜜汁尴尬~~~。我媳妇用的Ubuntu，然后她需要装很多环境，不知道为何内核老是更新，结果就是后面装东西的时候提示/boot分区空间不足无法安装，但我天真可爱的媳妇不会清呀，然后我就来了，一看简单啊，删呗，凡事总有BUT，我觉得这个事情嘛要清就要清彻底，就不rm-rf了，直接卸安装包多清爽，于是乎系统挂了，剁手啊打什么通配符。。。不过总结一下还是对系统层认识不够生动，所以就补一篇日志，万一我最棒的媳妇看到了呢，那就点点点~ 计算机如何启动的又看了一遍阮一峰关于这个的文章，觉得自己就先不写了，后续自己再理解着写吧。。时间有限啊，参考计算机是如何启动的 系统如何启动的说明一下这些图都是通过搜索引擎抓过来的，并不是我画的。更详细的说明看参考地址里的文章里面写的很好。来看看上图，这个就是Linux系统启动的流程不过比较简略，但要点都在，下张图会更详尽一点，接着看吧这张图就更详细点了，不过还有更详尽的，Look下面嗯，这样就很尴尬了，感觉看图就够了，我也就不啰嗦了，蛮好~~ 参考地址阮一峰大牛的网络日志来自家住海边喜欢浪的文章","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"},{"name":"系统原理","slug":"系统原理","permalink":"http://arvon.top/tags/系统原理/"}]},{"title":"Linux后台执行执行程序及环境变量优先级","slug":"Linux后台运行程序及环境变量优先级","date":"2017-06-24T17:23:45.000Z","updated":"2018-03-02T08:02:16.000Z","comments":true,"path":"2017/06/25/Linux后台运行程序及环境变量优先级/","link":"","permalink":"http://arvon.top/2017/06/25/Linux后台运行程序及环境变量优先级/","excerpt":"背景故事：为什么写这个呢，因为有次给媳妇弄翻墙的shadows，配完了想让她每次开机就自动运行，然后发现了个事情，就是脚本有输出的时候直接用&amp;是不大好的。还查了下，这可不应该啊，虽然说好久不用这个玩意，不过还是应该掌握的，所以就当记个笔记吧","text":"背景故事：为什么写这个呢，因为有次给媳妇弄翻墙的shadows，配完了想让她每次开机就自动运行，然后发现了个事情，就是脚本有输出的时候直接用&amp;是不大好的。还查了下，这可不应该啊，虽然说好久不用这个玩意，不过还是应该掌握的，所以就当记个笔记吧 关于用户环境变量profile的设置登录后首先读取/etc/profile这个配置，这个配置是对所有用户生效的。然后依次寻找以下三个文件，这三个文件定义了对当前用户的配置 第一个是~/.bash_profile第二个是~/.bash_login第三个是~/.profile高能预警： 以上三个文件只要有一个存在就不会读取后面的文件了，比如有了~/.bash_login就不读取~/.profile了 另外如果修改了profile的配置，直接修改后对于当前终端是不生效的，一般有两种方法使之生效 source ~/.profile 【这里只是举例子，你修改的是哪个配置就source哪个配置】 退出当前终端再重新打开 关于sudu命令当你使用sudo去执行一个程序时，处于安全的考虑，这个程序将在一个新的、最小化的环境中执行，也就是说，诸如PATH这样的环境变量，在sudo命令下已经被重置成默认状态了。所以当一个刚初始化的PATH变量中不包含你所要运行的程序所在的目录，用sudo去执行，你就会得到”command not found”的错误提示。要想改变PATH在sudo会话中的初始值，用文本编辑器打开/etc/sudoers文件，找到”secure_path”一行，当你执行sudo 命令时，”secure_path”中包含的路径将被当做默认PATH变量使用。添加所需要的路径(如 /usr/local/bin）到”secure_path”下,如下1Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin:/usr/local/bin 关于export使用export可以直接声明环境变量，不过这种方式只对当前终端有效。所以一般在脚本里或测试时会用到这种方式，如果是需要每次登陆都有这些变量的话，还是需要写入全局或用户的配置中去。 关于后台运行脚本或命令一般用两种方法，一种是直接command+&amp;，还有种就是nohup command+&amp;。一般来说用第二种的多一点，因为如果脚本有输出的话终端就会被占用，而且关闭终端后台进程会终止；但是第二种会把输出记录在当前目录nohup.out的输出中,而且关闭端口后台进程也不会结束。 看命令吧，下面通过演示怎么在后台执行这个没用的脚本12345while true;do ehco \"Hello world, `date`\" sleep 2done#named hello.sh 使用&amp; 12sh hello.sh &amp;#像上面这种有输出的，不要用这个，反正不推荐 使用nohup 1nohup sh hello.sh &amp; 查看后台任务jobs 1234jobs#bogon:tmp arvon$ jobs#[1]- Running sh xxx.sh &amp;#[2]+ Running sh ppp.sh &amp; 调出后台任务fg 12fg %1fg %2 使用ctrl+z将前台程序放入后台，此时程序会变为stop 使用bg将后台暂停的程序启动12345bogon:tmp arvon$ jobs[1]- Stopped sh xxx.sh[2]+ Stopped sh ppp.shbogon:tmp arvon$ bg %1[1]- sh xxx.sh &amp;","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"},{"name":"系统原理","slug":"系统原理","permalink":"http://arvon.top/tags/系统原理/"}]},{"title":"Linux管理员的瑞士军刀-AWK","slug":"Linux管理员的瑞士军刀-AWK","date":"2017-06-23T16:00:00.000Z","updated":"2017-06-24T10:15:58.000Z","comments":true,"path":"2017/06/24/Linux管理员的瑞士军刀-AWK/","link":"","permalink":"http://arvon.top/2017/06/24/Linux管理员的瑞士军刀-AWK/","excerpt":"作为Linux系统管理员的瑞士小军刀，awk的作用是显而易见的，但是一般情况下使用的都是很简单的分割打印，但有时候需要稍微复杂一些的用法，这里进行一下我的总结。其实之前有写过一篇awk的博客，不过写的比较基础，这次作为补充并尽可能覆盖常用用法，之前的地址戳这里","text":"作为Linux系统管理员的瑞士小军刀，awk的作用是显而易见的，但是一般情况下使用的都是很简单的分割打印，但有时候需要稍微复杂一些的用法，这里进行一下我的总结。其实之前有写过一篇awk的博客，不过写的比较基础，这次作为补充并尽可能覆盖常用用法，之前的地址戳这里 感受一下通过一个简单的实例来进行说明，实例数据如下,其他介绍放在后面，毕竟实例比理论来的直观嘛： NumID Name Math English Chinese M5 Arvon 13 14 15 F3 Mo 92 02 26 F4 Pikachu 52 10 11 M1 Steavn 1 2 3 F2 World 4 5 56 鲜活的小栗子 基础1：获取姓名和英语成绩 1awk -F' ' '&#123;print $2,$4&#125;' xxx.txt 基础2：设置输入和输出分隔符的姓名和成绩 1awk 'BEGIN&#123;FS=\" \";OFS=\"---\"&#125;&#123;print $2,$4&#125;' xxx.txt 基础3： 输出行号列数带描述的姓名和成绩 1awk 'BEGIN&#123;FS=\" \";OFS=\"---\"&#125;&#123;print \"filename:\" FILENAME, \"lineNum:\"NR, \"leishu:\"NF, $2,$4&#125;' xxx.txt 基础4：添加Title和结束符并设置输入输出分隔符的例子 1awk 'BEGIN&#123;print \"T1\",\"T2\",\"T3\",\"T4\"&#125;&#123;FS=\" \";OFS=\"---\"&#125;&#123;print \"filename:\" FILENAME, \"lineNum:\"NR, \"leishu:\"NF, $2,$4&#125;END&#123;print \"Game Over\"&#125;' xxx.txt 基础5： 带匹配的，例如匹配Arvon并输出成绩 123awk 'BEGIN&#123;FS=\" \"&#125;/Arvon/&#123;print $0&#125;' xxx.txtawk 'BEGIN&#123;FS=\" \"&#125;/M[1-9]/&#123;print $0&#125;' xxx.txtawk 'BEGIN&#123;FS=\" \"&#125;/M./&#123;print $0&#125;' xxx.txt 嗨嗨的大栗子说明一下：awk的条件是从C语言借鉴而来，反正C我也不会，但awk应该会，关于AWK变成的资料极多，我这里就写一些常用简单的啦~还有还有–&gt;awk工作流程：先执行BEGIN，然后读取文件，读入有/n换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域,随后开始执行模式所对应的动作action。接着开始读入第二条记录······直到所有的记录都读完，最后执行END操作。 高阶1：行计数累加，获取每个人的成绩总和 1awk 'BEGIN&#123;FS=\" \"&#125;/M[1-9]|F[1-9]/&#123;print $0,$3+$4+$5&#125;' xxx.txt 高阶2：列累加，获取所有人每科成绩的总和 1awk 'BEGIN&#123;FS=\" \"&#125;/F[1-9]|M[1-9]/&#123;sMath=sMath+$3;sEnglish=sEnglish+$4;sChinese=sChinese+$5&#125;END&#123;print sMath,sEnglish,sChinese&#125;' xxx.txt 高阶3：数据筛选计数，获取数学成绩大于10的人数，并列出是谁 1awk 'BEGIN&#123;renshu=0&#125;&#123;FS=\" \"&#125;/F[1-9]|M[1-9]/&#123;if ($3&gt;10) &#123;print $0; renshu+=1&#125;&#125;END&#123;print \"totleNum:\" renshu&#125;' xxx.txt 高阶5：带过滤筛选的求和，求英语成绩大于等于5的人的各科成绩总和 1awk 'BEGIN&#123;sMath=0;sEng=0;sChi=0&#125;/F[1-9]|M[1-9]/&#123;if($4&gt;=5)&#123;print $0;sMath+=$3;sEng+=$4;sChi+=$5&#125;&#125;END&#123;print \"sMath:\" sMath, \"sEng:\" sEng, \"sChi:\" sChi&#125;' xxx.txt 名词解释 内建变量 Record（记录）:awk从数据文件上读取数据的基本单位，默认内建变量RS为换行如：例子中的“M5 Arvon 13 14 15”就是一条记录 Field（字段）：记录中被分隔开的子字符串，默认内建变量FS为空格如：例子中第一条记录的第一个字符串为M5，第二个为Arvon 内建变量 变量名称 描述 \\$n 当前记录的第n个字段，字段间由FS分隔 $0 完整的输入记录 ARGC 命令行参数的数目 ARGIND 命令行中当前文件的位置(从0开始算) ARGV 包含命令行参数的数组 CONVFMT 数字转换格式(默认值为%.6g)ENVIRON环境变量关联数组 ERRNO 最后一个系统错误的描述 FIELDWIDTHS 字段宽度列表(用空格键分隔) FILENAME 当前文件名 FNR 各文件分别计数的行号 FS 字段分隔符(默认是任何空格) IGNORECASE 如果为真，则进行忽略大小写的匹配 NF 输入字段分割符 NR 已经读出的记录数，就是行号，从1开始 OFMT 数字的输出格式(默认值是%.6g) OFS 输出记录分隔符（输出换行符），输出时用指定的符号代替换行符 ORS 输出记录分隔符(默认值是一个换行符) RLENGTH 由match函数所匹配的字符串的长度 RS 记录分隔符(默认是一个换行符) RSTART 由match函数所匹配的字符串的第一个位置 SUBSEP 数组下标分隔符(默认值是/034) 以上，还有很多用法暂时就先这样吧，有了再补充，awk编程也是厉害了","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"},{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"},{"name":"正则","slug":"正则","permalink":"http://arvon.top/tags/正则/"}]},{"title":"GTX直压上云技术实践","slug":"GTX直压上云技术实践","date":"2017-05-31T16:00:00.000Z","updated":"2017-06-02T10:41:40.000Z","comments":true,"path":"2017/06/01/GTX直压上云技术实践/","link":"","permalink":"http://arvon.top/2017/06/01/GTX直压上云技术实践/","excerpt":"update:测试时出现二进制文件大小恰好为256M整数倍时解压不退出的bug,提交至维护人员后已经解决，不得不说反馈速度真的是快，从我提出到解决总共用了不到1天时间。给点个赞，棒棒哒GTX Compressor(直压上云技术)调研,由于现业务存在大规模业务数据上传云存储，云厂商提供接口具有一些局限性，现在尝试gtz这个工具。该工具目前支持阿里云及AWS非常符合业务场景，具有高压缩比、高性能、高速直压云等特点，且有数据校验机制，而且开包即用，不依赖当前系统其他任何库。 介绍：GTX Compressor是Genetalks公司GTX Lab实验室开发的面向大型数据（数GB甚至数TB数据，尤其是生物信息数据）上云，而量身定制的复杂通用数据压缩打包系统，可以对任意基因测序数据以及数据目录进行高压缩率的快速打包，形成单个压缩数据文件，以方便存储档与远程传输、校验。区别于以往的压缩工具，GT Compressor系统着力于 高压缩率，高速率，方便的数据抽取 。——————摘自Github上该项目官方介绍","text":"update:测试时出现二进制文件大小恰好为256M整数倍时解压不退出的bug,提交至维护人员后已经解决，不得不说反馈速度真的是快，从我提出到解决总共用了不到1天时间。给点个赞，棒棒哒GTX Compressor(直压上云技术)调研,由于现业务存在大规模业务数据上传云存储，云厂商提供接口具有一些局限性，现在尝试gtz这个工具。该工具目前支持阿里云及AWS非常符合业务场景，具有高压缩比、高性能、高速直压云等特点，且有数据校验机制，而且开包即用，不依赖当前系统其他任何库。 介绍：GTX Compressor是Genetalks公司GTX Lab实验室开发的面向大型数据（数GB甚至数TB数据，尤其是生物信息数据）上云，而量身定制的复杂通用数据压缩打包系统，可以对任意基因测序数据以及数据目录进行高压缩率的快速打包，形成单个压缩数据文件，以方便存储档与远程传输、校验。区别于以往的压缩工具，GT Compressor系统着力于 高压缩率，高速率，方便的数据抽取 。——————摘自Github上该项目官方介绍 环境要求 64位 Linux 系统（CentOS 6.5以上或Ubuntu 12.04以上，推荐Ububtu 14.04及以上64位操作系统) 4核以上，最小8GB内存的主机系统（若要达到最大并发性，推荐32核 64GB内存，或与AWS C4.8xlarge机器相同配置） 我的测试环境为：Centos7.3_x64 安装及使用 安装非常简单暴力，直接就是开包即用，下载对应系统版本即可 安装 123wget https://github.com/Genetalks/gtz/archive/master.zipunzip xvf master.ziptar xvf gtz_0.2.2b_centos_pre_release.tgz 使用方法 123456USAGE:./gtz [--list] [-e &lt;string&gt;] [-f] [--endpoint &lt;string&gt;] [--timeout &lt;string&gt;] [--secret-access-key &lt;string&gt;] [--access-key-id &lt;string&gt;] [-b &lt;string&gt;] [-s &lt;string&gt;] [-c] [-n &lt;string&gt;] [-l &lt;string&gt;] [-i] [-d] [--delete] [-a] [-g &lt;number&gt;] [-o &lt;string&gt;] [--] [--version] [-h] &lt;file names&gt; ... 压缩解压选项解释 1234567891011121314151617181920### 通用选项-h：输出以上命令行帮助信息--version：输出gt_compress程序的版本号--access-key-id : 指定云平台用户ID--secret-access-key： 指定云平台用户密钥--endpoint ： 指定阿里云OSS平台的访问域名和数据中心### 压缩选项参数-f, --force ： 强制删除容器内的object--timeout ： 指定上传超时阀值-i：压缩时增加索引，主要用于在压缩文件中快速检索fastq文件的某段内容，该选项会降低压缩速度-a：追加模式，本次压缩的内容会追加到压缩文件中-g：分组加速压缩，分组越多，需要的cpu和内存越多，压缩速度越快。不指定该值时，程序会根据cpu和内存自动选择最优值-o：指定压缩文件名，不指定时，默认为out.gtzfile_name：需要压缩的文件或目录, 若不指定，则从标准输入中读入数据### 解压选项参数-d,--decode : 解压模式 --list : 列出压缩包中所有的压缩文件名，与-d参数一起使用 -e, --extract : 解压压缩包中指定的压缩文件，文件名之间用冒号:分割，与-d参数一起使用--timeout ： 指定下载超时阀值-c,--stdout : 解压数据输出至标准输出, 只能与 -d 参数一起使用-o：指定输出文件名，使用-n或-l时需要指定该选项，否则不需要该选项file_name：需要压缩的文件, 若不指定，则从标准输入中读入数据 使用范例 注意：使用时可以使用命令行参数指定key变量和endpoint变量，也可以使用export声明，如export access_key_id=xxxxxxexport secret_access_key=xxxxxxexport endpoint=xxxxxx （该环境变量只有上传至OSS时才需设置） 本地压缩解压 12345678####----压缩----#直接压缩，1G的数据压到68k了，不过源数据不具备参考性，dd出来的数据./gtz -o test.gtz up_data_test/gtz-test3.log#通过zcat和管道压缩，支持二进制文件zcat up_data_test/gtz-test3.log |./gtz -o test2.gtz####----解压----#解压单个文件，解压路径为压缩时的路径./gtz -d ./test.gtz 压缩上传AWS例子上传文件在S3上显示为目录gtz-test4.name，该目录里面下面为原文件同名目录及一个gtz.meta文件，如图 1234567export AWS_ACCESS_KEY_ID='your_key'export AWS_SECRET_ACCESS_KEY='Your_key'export AWS_DEFAULT_REGION='your_region'#上传单个文件，上传1G的测试数据需要50s左右./gtz -o s3://arvon-gtz-test/gtz-test4.name up_data_test/gtz-test4.db#追加文件到压缩包，如果不加-a参数会覆盖原来的文件./gtz -o s3://arvon-gtz-test/ up_data_test/gtz-test3.log 解压缩AWS上S3到本地 1./gtz -d s3://arvon-gtz-test/test3 官方文档Github项目地址","categories":[],"tags":[{"name":"云服务","slug":"云服务","permalink":"http://arvon.top/tags/云服务/"},{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"}]},{"title":"常见压缩格式对比","slug":"常见压缩格式对比","date":"2017-05-29T16:00:00.000Z","updated":"2017-05-30T06:48:22.000Z","comments":true,"path":"2017/05/30/常见压缩格式对比/","link":"","permalink":"http://arvon.top/2017/05/30/常见压缩格式对比/","excerpt":"数据压缩可以分为无损压缩和有损压缩，有损，指的是压缩之后就无法完整还原原始信息，但是压缩率可以很高，主要应用于视频、话音等数据的压缩，因为损失了一点信息，人是很难察觉的，或者说，也没必要那么清晰照样可以看可以听；无损压缩则用于文件等等必须完整还原信息的场合。目前只归纳zip、rar、tar.gz、tar.bz常见的这几种，其他的以后接触了再进行补充。","text":"数据压缩可以分为无损压缩和有损压缩，有损，指的是压缩之后就无法完整还原原始信息，但是压缩率可以很高，主要应用于视频、话音等数据的压缩，因为损失了一点信息，人是很难察觉的，或者说，也没必要那么清晰照样可以看可以听；无损压缩则用于文件等等必须完整还原信息的场合。目前只归纳zip、rar、tar.gz、tar.bz常见的这几种，其他的以后接触了再进行补充。注意：压缩率大小直接受源文件影响，所以不同类型源文件压缩效率差别会很大，以下数值仅可作参考，压缩率越低压缩效率越高，例如：100G压缩到10G压缩率为10% 普通文本文件压缩率对比 压缩格式 压缩率 占用cpu 耗时 7-zip 10% 50% 600s win-zip 30% 65% 200s WinRAR 25% 80% 240s tar.gz 11% 55% 500s tar.bz2 15% 65% 550s 影音文件压缩率对比 压缩格式 压缩率 占用cpu 耗时 7-zip 40% 80% 500s win-zip 80% 80% 150s WinRAR 45% 70% 450s tar.gz 45% 65% 400s tar.bz2 43% 70% 450s 参考资料zip压缩原理分析zip压缩原理及实现tar压缩原理","categories":[],"tags":[{"name":"系统原理","slug":"系统原理","permalink":"http://arvon.top/tags/系统原理/"}]},{"title":"Nginx重定向及域名CNAME问题","slug":"Nginx重定向及域名CNAME记录","date":"2017-05-29T09:12:05.000Z","updated":"2017-05-29T09:12:20.000Z","comments":true,"path":"2017/05/29/Nginx重定向及域名CNAME记录/","link":"","permalink":"http://arvon.top/2017/05/29/Nginx重定向及域名CNAME记录/","excerpt":"关于域名解析的几个问题整理，最近换了博客评论系统，原因是多说马上就停止支持了，所以现在转用网易云跟帖，个人感觉还可以。期间有个域名解析的问题我觉得可以记录下。主要就是DNS的CNAME记录以及Nginx的301、302重定向。 关于DNS的CNAME解析 其实这个也蛮好理解，不涉及数据报内容的改变，就是将要解析的域名指向另一个域名解析。对比A记录就更清晰了，A记录是域名到IP的解析，而CNAME记录是域名解析到域名。所以通过域名的CNAME记录对web访问时没有任何影响的，它只是改变了域名解析的过程而已。","text":"关于域名解析的几个问题整理，最近换了博客评论系统，原因是多说马上就停止支持了，所以现在转用网易云跟帖，个人感觉还可以。期间有个域名解析的问题我觉得可以记录下。主要就是DNS的CNAME记录以及Nginx的301、302重定向。 关于DNS的CNAME解析 其实这个也蛮好理解，不涉及数据报内容的改变，就是将要解析的域名指向另一个域名解析。对比A记录就更清晰了，A记录是域名到IP的解析，而CNAME记录是域名解析到域名。所以通过域名的CNAME记录对web访问时没有任何影响的，它只是改变了域名解析的过程而已。 关于URL重定向 所谓URL重定向其实就是URL跳转，这个可以通过Nginx的HttpRewriteModule设置重写http请求头。URL重定向氛围302临时重定向和301永久重定向，301永久重定向会使搜索引擎抓取新的内容时使用重定向后的新地址，而302临时重定向会使搜索引擎抓取新内容时保留重定向前的旧地址。nginx的rewrite相当于apache的rewriterule(大多数情况下可以把原有apache的rewrite规则加上引号就可以直接使用)，它可以用在server,location 和IF条件判断块中,命令格式如下：rewrite 正则表达式 替换目标 flag标记flag标记可以用以下几种格式：last – 基本上都用这个Flag。break – 中止Rewirte，不在继续匹配redirect – 返回临时重定向的HTTP状态302permanent – 返回永久重定向的HTTP状态301 301永久重定向Nginx代码 1234567891011server &#123; listen 80; server_name arvon.top; if ($host = 'www.arvon.top' ) &#123; rewrite ^/(.*)$ http://arvon.top/$1 permanent; &#125; location / &#123; root html/public; index index.html; &#125;&#125; 302临时重定向Nginx代码 1234567891011server &#123; listen 80; server_name arvon.top; if ($host = 'blog.arvon.top' ) &#123; rewrite ^/(.*)$ http://arvon.top/$1 redirect; &#125; location / &#123; root html/public; index index.html; &#125;&#125; Nginx地址重定向 使用alias或root进行网站路径定义，也可进行资源路径重定向。 使用root定义语法：root path;默认：root html;配置块：http、server、location、if 123location /download/ &#123; root /opt/web/html/; &#125; 使用alias定义语法：alias path;配置块： location; 1234567location /conf &#123; alias /usr/local/nginx/conf/; &#125;#如果用root，则如location /conf &#123; root /usr/local/nginx/; &#125; 使用alias时，在URI向实际文件路径的映射过程中，已经把location后配置的/conf这部分字符串丢弃掉，因此，/conf/nginx.conf请求将根据alias path映射为path/nginx.conf。root则不然，它会根据完整的URI请求来映射，因此，/conf/nginx.conf请求会根据root path映射为path/conf/nginx.conf。这也是root可以放置到http、server、location或if块中，而alias只能放置到location块中的原因。 更详细的说明1. 大风的博客：nginx配置url重定向-反向代理2. 网站路径定义","categories":[],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://arvon.top/tags/Nginx/"}]},{"title":"Jumpserver实践记录","slug":"Jumpserver搭建记录","date":"2017-05-24T04:32:05.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2017/05/24/Jumpserver搭建记录/","link":"","permalink":"http://arvon.top/2017/05/24/Jumpserver搭建记录/","excerpt":"Jumpserver是一个由一群有理想有抱负的大牛们开发的开源跳板机及资产管理系统，最近在整理搭建，发现还是蛮好用的，过程记录如下，感谢作者开源！！","text":"Jumpserver是一个由一群有理想有抱负的大牛们开发的开源跳板机及资产管理系统，最近在整理搭建，发现还是蛮好用的，过程记录如下，感谢作者开源！！ Environment aws ec2 centos7.3 Python: 3.5 Django: 1.11 Mysql Install install rely env(使用Redhat) 12345678910111213wget http://dev.mysql.com/get/mysql57-community-release-el7-7.noarch.rpmrpm -ivh mysql57-community-release-el7-7.noarch.rpmyum install -y mysql-community-serversystemctl start mysqld.servicemysqladmin -uroot -p password \"your_password\"pip uninstall pycryptorm -rf /usr/lib64/python2.6/site-packages/Crypto/rm -rf /usr/lib64/python2.6/site-packages/pycrypto-2.6.1-py2.6-linux-x86_64.eggpip install pycrypto==2.4.1wget https://www.python.org/ftp/python/3.6.0/Python-3.6.0.tar.xztar xvf Python-3.6.0.tar.xz;cd Python-3.6.0./configure --prefix=/usr/local/python3make &amp;&amp; make install modify config 12#My path: vim /data/jumpserver_pkg/jumpserver-master/install/install.pypython install.py #直接执行即可 Solve the problem mysql error:报错为django相关 使用utf8创建databaseCREATE DATABASE IF NOT EXISTS jumpserver DEFAULT CHARACTER SET utf8; Aws ec2 批量添加主机脚本 获取aws ec2运行主机列表 1234567891011121314151617---#name:get_ec2_host_list.yml- hosts: localhost connection: local gather_facts: True tasks: - name: ec2 instance facts ec2_remote_facts: region: cn-north-1 filters: instance-state-name: running register: ec2 - set_fact: ec2_out=&#123;&#123; ec2 &#125;&#125; - shell: rm -rf running_ec2_list.txt - shell: echo \"&#123;&#123; item.0.private_ip_address &#125;&#125;\" \"&#123;&#123;item.0.tags.Name&#125;&#125;\" &gt;&gt; running_ec2_list.txt with_together: - \"&#123;&#123; ec2_out.instances &#125;&#125;\" 生成Excel文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#!/usr/bin/python# -*- coding:utf-8 -*-################################################################################Author: arvon#Email: yafeng2011@126.com#Blog: http://blog.arvon.top/#Date: 2017-05-23#Filename: write_jumpserver_host.py#Revision: 1.0#License: GPL#Description: use ansible get host list then use xlrd module write excel#Notes:###############################################################################import osimport openpyxl#varsport='22'host_group='group_name'aws_access_id='your_id'aws_secret_id='your_id'server_file_name='./running_ec2_list.txt'dest_filename = 'asset_cn_dev.xlsx'##functionsdef create_server_file(): os.environ['AWS_ACCESS_KEY_ID'] = aws_access_id os.environ['AWS_SECRET_ACCESS_KEY'] = aws_secret_id os.system('ansible-playbook get_ec2_host_list.yml')def write_jumpserver_excel(): wb = openpyxl.Workbook() ws1 = wb.active ws1.title = 'Assets' ws1.append(['IP地址','端口号','主机名','管理账号','用户名','密码','主机组']) with open (server_file_name, 'r') as f1: server_num = len(open(server_file_name,'rU').readlines()) row = int(2) for eachline in f1: server_info=eachline.split() private_ip=server_info[0] tag_name=server_info[1] ws1.cell(column=1,row=row,value=private_ip) ws1.cell(column=2,row=row,value=port) ws1.cell(column=3,row=row,value=tag_name) ws1.cell(column=4,row=row,value='默认') ws1.cell(column=5,row=row,value='') ws1.cell(column=6, row=row, value='') ws1.cell(column=7, row=row, value=host_group) wb.save(filename=dest_filename) row=row+1if __name__=='__main__': write_jumpserver_excel() 设置教程 使用jumpserver账户密码登录，如果未设置就是admin及默认密码（如果没有改端口默认为8000） 进入设置页面，注意这里填写的用户只能一次，新添加一个会覆盖原来的设置 建立用户组，然后建立用户时关联用户组，这里的用户和组都是相对jumpserver其实在服务器上并不需要存在这些 添加资产，可以使用上面的python脚本进行批量添加 创建sudo权限控制组，方便对权限进行精细控制 创建用户，这时并没有真正创建，需要保存后进行推送，推送需要选择资产或资产组，选择完成点击推送后会在对应资产上创建该用户 确定jumpserver用户以哪个系统用户访问对应资产 设置完成，此时jumpserver新建用户可以通过接收到的邮件信息进行访问了 上传下载这个比较好理解直接操作一遍就清楚了 另外有审计功能也是很不错，方便问题定位，也是点着看看就清楚了 再次感谢jumpserver的开发团队！！！以上 Reference jumpserver_doc install doc","categories":[],"tags":[{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"}]},{"title":"MacOS下pip安装报permitted错误解决","slug":"MacOS下pip安装报permitted错误解决","date":"2017-05-23T17:23:45.000Z","updated":"2017-06-27T03:20:35.000Z","comments":true,"path":"2017/05/24/MacOS下pip安装报permitted错误解决/","link":"","permalink":"http://arvon.top/2017/05/24/MacOS下pip安装报permitted错误解决/","excerpt":"安装个python的etcd模块报错，那红红的一串很是好看。。。","text":"安装个python的etcd模块报错，那红红的一串很是好看。。。 故障描述：就是这种权限报错OSError: [Errno 1] Operation not permitted ，原因如下 Mac现在系统有个称为SIP的机制(System Integrity Protection)，默认下系统启用SIP系统完整性保护机制，无论是对于硬盘还是运行时的进程限制对系统目录的写操作，macosx 10.11 EI Capitan使用了Rootlees，可以理解为一个更高等级的内核保护，系统会默认锁定/system , /sbin , /usr这三个目录，所以安装的时候会报这个错误 解决方法： 很魔性 12pip install etcd --user U#基于用户的权限来安装模块包，这样问题就解决了 很粗暴(没尝试，也不推荐) 12csrutil status#需要进入macos的recovery模式终端进行修改csrutil enable","categories":[],"tags":[{"name":"其他OS","slug":"其他OS","permalink":"http://arvon.top/tags/其他OS/"}]},{"title":"计算机度量单位总结","slug":"计算机度量单位总结","date":"2017-04-24T13:23:45.000Z","updated":"2017-06-27T06:07:01.000Z","comments":true,"path":"2017/04/24/计算机度量单位总结/","link":"","permalink":"http://arvon.top/2017/04/24/计算机度量单位总结/","excerpt":"第一次可以分不清大B小b，但二次就不应该。。。","text":"第一次可以分不清大B小b，但二次就不应该。。。 来看一看： name 简写 次方 name 简写 次方 kilobyte KB 10^3 kibibyte KiB 2^10 megabyte MB 10^6 mebibyte MiB 2^20 gigabyte GB 10^9 gibibyte GiB 2^30 terabyte TB 10^12 tebibyte TiB 2^40 petabyte PB 10^15 pebibyte PiB 2^50 exabyte EB 10^18 exbibyte EiB 2^60 zettabyte ZB 10^21 zebibyte ZiB 2^70 yottabyte YB 10^24 yobibyte YiB 2^80 看个详细的图标 单位 描述 bit 计算机最小的数据单位，1比特等于1或者0，是计算机处理、存储、传输数据时使用的二进制格式。 byte 用于描述数据文件大小、磁盘或者其他存储介质空间的容量或者通过网络传输的数据量的单位，1字节相当于8比特。 kb(kilobit) 大约1000比特。 kB(kilobyte) 大约1000字节（精确值为1024字节） Mb(megabit) 大约1000万比特 MB(megabyte) 大约1000万字节(精确值为1048576字节)。megabyte有时记作“meg.”。大多数个人电脑使用MB来计算内存大小，大文件有时也用MB来度量大小。 GB(gigabyte) 大约10亿字节。Gigbyte有时记作“gig”。大多数的个人电脑使用GB来度量硬盘大小。 TB(terabyte) 大约1万亿字节。一些高端计算机使用TB来度量硬盘大小。 kbit/s(kilobits per second) 1000比特每秒。它是计算通过网络连接传输的数据量的标准单位。 kB/s(kilobytes per second) 1000字节每秒。它是计算通过网络连接传输的数据量的标准单位。 Mbit/s(megabits per second) 100万比特每秒。它是计算机通过网络连接传输的数据量的标准单位。基础以太网工作带宽为10Mbit/s。 MB/s(megabyte per second) 100万字节每秒。它是计算通过网络连接传输的数据量的标准单位。 Gbit/s(gigabits per second) 10亿比特每秒。它是计算机通过网络连接传输的数据量的标准单位。10G或者10吉比特以太网工作带宽为10Gbit/s。 Tbit/s(terabits per second) 1万亿比特每秒。它是计算机通过网络连接传输的数据量的标准单位。 HZ(hertz) 频率单位。声波、交流电及其他循环波形的状态转换速度或周期。1Hz表示一秒钟一个循环。 MHz(megahertz) 兆赫。它是描述计算机微处理器等处理芯片速度的一个常用单位。许多无线电话工作在这个频段。 GHz(gigahertz) 吉(1 000 000 000)赫。它是描述计算机微处理器等处理芯片速度的一个常用单位。许多无线电话和无线LAN工作在这个频段。 另外： MB等单位以10为底数的指数，MiB是以2为底数的指数，如：1KB=10^3=1000, 1MB=10^6=1000000=1000KB,1GB=10^9=1000000000=1000MB,而 1KiB=2^10=1024,1MiB=2^20=1048576=1024KiB。与我们密切相关的是我们在买硬盘的时候，操作系统报的数量要比产品标出或商家号称的小一些，主要原因是标出的是以MB、GB为单位的，1GB就是1,000,000,000 Byte，而操作系统是以2进制为处理单位的，因此检查硬盘容量时是以MiB、GiB为单位，1GB=2^30=1,073,741,824，相比较而言，1GiB要比1GB多出1,073,741,824-1,000,000,000=73,741,824，所以检测实际结果要比标出的少一些。 内容来自以下链接整理，鸣谢一片叶子的博客starshine的博客","categories":[],"tags":[{"name":"系统原理","slug":"系统原理","permalink":"http://arvon.top/tags/系统原理/"},{"name":"其他OS","slug":"其他OS","permalink":"http://arvon.top/tags/其他OS/"}]},{"title":"小米Air系统重装Win7","slug":"小米Air系统重装Win7","date":"2017-03-01T13:00:00.000Z","updated":"2017-08-01T02:43:36.000Z","comments":true,"path":"2017/03/01/小米Air系统重装Win7/","link":"","permalink":"http://arvon.top/2017/03/01/小米Air系统重装Win7/","excerpt":"Tips:为毛装个系统还要记一下 网络的解释说第六代酷睿使用了更新的主板芯片，win7缺少驱动。 Skylake一代会支持DDR4，100系列芯片组升级也很诱人，但新平台有个大坑需要注意——Skylake平台上不能通过USB接口装Windows 7系统了。 Skylake一代Intel会移除EHCI主控，改为支持XHCI主控，但Windows 7系统原生不支持XHCI主控。再进一步说就是，如果你打算在Skylake平台上继续使用Windows 7系统，那么安装系统时就会遇到一个难题——不能通过USB硬盘安装，也不能通过USB光驱安装。 Skylake的这个改动只影响Windows 7系统，Windows 8及之后的系统都会原生支持XHCI主控，装系统时USB接口没问题。","text":"Tips:为毛装个系统还要记一下 网络的解释说第六代酷睿使用了更新的主板芯片，win7缺少驱动。 Skylake一代会支持DDR4，100系列芯片组升级也很诱人，但新平台有个大坑需要注意——Skylake平台上不能通过USB接口装Windows 7系统了。 Skylake一代Intel会移除EHCI主控，改为支持XHCI主控，但Windows 7系统原生不支持XHCI主控。再进一步说就是，如果你打算在Skylake平台上继续使用Windows 7系统，那么安装系统时就会遇到一个难题——不能通过USB硬盘安装，也不能通过USB光驱安装。 Skylake的这个改动只影响Windows 7系统，Windows 8及之后的系统都会原生支持XHCI主控，装系统时USB接口没问题。 设置BIOG 1、开机，按F2进BIOS。选择Security，再选择Set Supervisor Password，这是设置管理员和密码，只有设置了这个，我们下一步才能操作。设置好后，按F10，点Yes。2、重启后，继续按F2，就会出现要你输入管理员密码的提示。输入密码进BIOS，选择Security，往下找，选择Secure Boot，选择Disabled，关闭微软安全认证，不然安装win7会蓝屏的。3、再往上移，找到TPM Availability，改成Hiden。然后往下选择boot，选择硬盘启动模式UEFI改为Legacy。USB Boot改为Enabled，然后选择Boot Devices Order，找到你的U盘PE名称调到第一位，确认后按F10，点Yes重启。4、BIOS设置好后，用注入了NVME驱动的PE进去用DiskGenius把GPT分区转换成MBR，然后和普通笔记本装系统一样了，但是系统也是有要求的，要注入NVME驱动的系统，重启后才不会蓝屏。 下载列表含NVME驱动的PE工具：http://xz.gqgtpc.com/pe/Gqgtupe3.0.1_Setup.exe含NVME驱动的Win7系统：https://page61.ctfile.com/fs/1773061-211197383 经验贴http://bbs.xiaomi.cn/t-13410315http://www.miui.com/thread-8800142-1-1.html","categories":[],"tags":[{"name":"其他OS","slug":"其他OS","permalink":"http://arvon.top/tags/其他OS/"}]},{"title":"关于Ansible条件判断的一个思考","slug":"关于Ansible条件判断的一个思考","date":"2017-02-01T13:00:00.000Z","updated":"2017-08-03T11:30:16.000Z","comments":true,"path":"2017/02/01/关于Ansible条件判断的一个思考/","link":"","permalink":"http://arvon.top/2017/02/01/关于Ansible条件判断的一个思考/","excerpt":"背景：生产环境中偶然遇到一个问题，在使用Playbook调用supervisor进行stop进程时，虽然supervisor指令成功执行，但进程并没有结束。然后准备使用ansible原生的条件判断，但是目前并没有找到合适的。所以呢就先用笨方法自己手动实现类似于while的判断循环结构。另外引发的思考：1.程序在成功启动的情况下什么操作会导致程序的瞬间崩溃。2.运行状态下移除程序源文件对程序有何影响。3.我需要一本书《程序是如何跑起来的》哈哈。。下面写个playbook模拟下使用Ansible如何等待后台进程执行完毕","text":"背景：生产环境中偶然遇到一个问题，在使用Playbook调用supervisor进行stop进程时，虽然supervisor指令成功执行，但进程并没有结束。然后准备使用ansible原生的条件判断，但是目前并没有找到合适的。所以呢就先用笨方法自己手动实现类似于while的判断循环结构。另外引发的思考：1.程序在成功启动的情况下什么操作会导致程序的瞬间崩溃。2.运行状态下移除程序源文件对程序有何影响。3.我需要一本书《程序是如何跑起来的》哈哈。。下面写个playbook模拟下使用Ansible如何等待后台进程执行完毕 概览 目录结构 123456789├── ansible.cfg├── hosts├── roles│ └── test_wait│ ├── tasks│ │ └── main.yml│ └── templates│ └── check_running_file.sh.j2└── test_wait.yml 摘要：手动在各个机器上建/tmp/helloworld这个文件，然后执行playbook，然后手动删除这个文件，看到只有这个文件消失时才成功执行下面带when判断的指令，目标达成 检测脚本 123456789101112[root@hosts test_ansible]# cat roles/test_wait/templates/check_running_file.sh.j2time_out=0dest_file='/tmp/helloworld'while [ -f \"$&#123;dest_file&#125;\" ];do sleep 1 let time_out=$&#123;time_out&#125;+1 if [ $&#123;time_out&#125; -ge 30 ];then echo \"######Stop gamex Error Time out is $&#123;time_out&#125; Please Check it!!!#####\" exit 0 fidoneexit 0 Playbook 12345678910111213141516171819[root@hosts test_ansible]# cat roles/test_wait/templates/check_running_file.sh.j2time_out=0dest_file='/tmp/helloworld'while [ -f \"$&#123;dest_file&#125;\" ];do sleep 1 let time_out=$&#123;time_out&#125;+1 if [ $&#123;time_out&#125; -ge 30 ];then echo \"######Stop gamex Error Time out is $&#123;time_out&#125; Please Check it!!!#####\" exit 0 fidoneexit 0[root@hosts test_ansible]# cat test_wait.yml- hosts: test remote_user: ec2-user gather_facts: True become: yes roles: - test_wait","categories":[],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"http://arvon.top/tags/Ansible/"}]},{"title":"Nginx状态码总结","slug":"Nginx状态码总结","date":"2016-12-04T03:23:45.000Z","updated":"2017-06-26T12:33:09.000Z","comments":true,"path":"2016/12/04/Nginx状态码总结/","link":"","permalink":"http://arvon.top/2016/12/04/Nginx状态码总结/","excerpt":"工作中常见的状态码其实并不多，去小米面试的时候问到关于这个问题，回答的不太好，这里就总结记录一下。万一面试再遇到呢。是吧","text":"工作中常见的状态码其实并不多，去小米面试的时候问到关于这个问题，回答的不太好，这里就总结记录一下。万一面试再遇到呢。是吧 状态码 描述 100 Continue 初始的请求已经接受，客户应当继续发送请求的其余部分 200 正常的返回 201 已创建，请求成功，服务器创建了新的资源 202 已接受，服务器已经接受了请求，但未开始处理 203 非授权信息，服务器已成功处理请求，但一些应答头可能不正确，因为使用的是文档的拷贝 204 无内容，服务器成功处理了请求，但没有返回任何内容 205 （重置内容） 服务器成功处理了请求，但没有返回任何内容。 206 （部分内容） 服务器成功处理了部分 GET 请求。 300 （多种选择） 针对请求，服务器可执行多种操作。 服务器可根据请求者 (user agent) 选择一项操作，或提供操作列表供请求者选择。 301 （永久移动） 请求的网页已永久移动到新位置。 服务器返回此响应（对 GET 或 HEAD 请求的响应）时，会自动将请求者转到新位置。 302 （临时移动） 服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。 303 （查看其他位置） 请求者应当对不同的位置使用单独的 GET 请求来检索响应时，服务器返回此代码。 304 （未修改） 自从上次请求后，请求的网页未修改过。 服务器返回此响应时，不会返回网页内容。 305 （使用代理） 请求者只能使用代理访问请求的网页。 如果服务器返回此响应，还表示请求者应使用代理。 307 （临时重定向） 服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。 400 （错误请求） 服务器不理解请求的语法。 401 （未授权） 请求要求身份验证。 对于需要登录的网页，服务器可能返回此响应。 403 （禁止） 服务器拒绝请求。 404 （未找到） 服务器找不到请求的网页。 405 （方法禁用） 禁用请求中指定的方法。 406 （不接受） 无法使用请求的内容特性响应请求的网页。 407 （需要代理授权） 此状态代码与 401（未授权）类似，但指定请求者应当授权使用代理。 408 （请求超时） 服务器等候请求时发生超时。 409 （冲突） 服务器在完成请求时发生冲突。 服务器必须在响应中包含有关冲突的信息。 410 （已删除） 如果请求的资源已永久删除，服务器就会返回此响应。 411 （需要有效长度） 服务器不接受不含有效内容长度标头字段的请求。 412 （未满足前提条件） 服务器未满足请求者在请求中设置的其中一个前提条件。 413 （请求实体过大） 服务器无法处理请求，因为请求实体过大，超出服务器的处理能力。 414 （请求的 URI 过长） 请求的 URI（通常为网址）过长，服务器无法处理。 415 （不支持的媒体类型） 请求的格式不受请求页面的支持。 416 （请求范围不符合要求） 如果页面无法提供请求的范围，则服务器会返回此状态代码。 417 （未满足期望值） 服务器未满足”期望”请求标头字段的要求。 500 （服务器内部错误） 服务器遇到错误，无法完成请求。 501 （尚未实施） 服务器不具备完成请求的功能。 例如，服务器无法识别请求方法时可能会返回此代码。 502 （错误网关） 服务器作为网关或代理，从上游服务器收到无效响应。 503 （服务不可用） 服务器目前无法使用（由于超载或停机维护）。 通常，这只是暂时状态。 504 （网关超时） 服务器作为网关或代理，但是没有及时从上游服务器收到请求。 505 （HTTP 版本不受支持） 服务器不支持请求中所用的 HTTP 协议版本。","categories":[],"tags":[{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"},{"name":"Nginx","slug":"Nginx","permalink":"http://arvon.top/tags/Nginx/"}]},{"title":"Iptables整理笔记","slug":"Iptables整理笔记","date":"2016-11-17T03:23:17.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/11/17/Iptables整理笔记/","link":"","permalink":"http://arvon.top/2016/11/17/Iptables整理笔记/","excerpt":"iptabels还是很实用的，分享下我的整理笔记","text":"iptabels还是很实用的，分享下我的整理笔记 场景规则 123456789101112131415161718192021222324252627282930313233343536373839404142#对本机lo回环地址放开iptables -I INPUT -i lo -j ACCEPT#对本机访问外部网络放开(ESTABLISHED表示tcp的一种状态，RELETED为ftp的一种状态)iptabels -I INPUT -m state --state ESTABLISHED,RELETED -j ACCEPT#对所有地址开放本机tcp（80，22，10-21）端口iptabels -I INPUT -p tcp --dport 22 -j ACCEPTiptabels -I INPUT -p tcp --dports 10:21 -j ACCEPT -m comment --comment \" 2015-09-24 by arvon\"#允许所有的地址开放本机的基于ICMP协议的数据包访问iptables -I INPUT -p icmp -j ACCEPT#ftp主动模式下规则iptables -I INPUT -m state --state ESTABLISHED,RELATED -j ACCEPTiptables -I INPUT -p tcp --dport 21 -j ACCEPT#ftp被动模式下规则iptables -I INPUT -p tcp --dport 21 -j ACCEPT ##vim /etc/vsftpd/vsftpd.conf #pasv_min_port=50000 #pasv_max_port=60000iptables -I INPUT -p tcp --dports 50000:60000 -j ACCEPT#ftp被动模式下规则二iptables —I INPUT -m state --state ESTABLISHED,RELATED -j ACCEPTiptables -I INPUT -p tcp --dport 21 -j ACCEPT #modprobe nf_conntrack_ftp #临时开启内核连接追踪模块ftp #vim /etc/sysconfig/iptables-config #开机自动加载 ##IPTABLES_MODULES=\"nf_conntrack_ftp\"#防CC攻击#对源地址为12并且并发大于10的数据包进行拒绝并返回错误信息iptables -I INPUT -p tcp --dport 80 -s 192.168.1.12 -m connlimit --connlimit-above 10 -j REJECT#当icmp不超过10个时放行，当超过10个每分钟放行1个iptables -A INPUT -p icmp -m limit --limit 1/m --limit-burst 10 -j ACCEPTiptables -A INPUT -p icmp DROP#防SYN攻击iptables -N syn-floodiptables -A INPUT -p tcp --syn -j syn-floodiptables -I syn-flood -p tcp -m limit --limit 3/s --limit-burst 6 -j RETURNiptables -A syn-flood -j REJECT#转发iptables -A FORWARD -p tcp -s 10.10.0.0/24 -m multiport --dports 80,110,21 -j ACCEPT#工作日工作时间禁止访问tencent的域名iptables -I FORWARD -p udp --dport 53 -m string --string \"tencent\" -m time --timestart 8:00 --timestop 12:00 --days Mon,Tue,Wed,Thu,Fri,Sat -j DROP#内核参数调整sysctl -w net.ipv4.ip_forward=1 #开启内核数据包转发功能sysctl -w net.ipv4.tcp_syncookies=1 #开启cookies验证，一定程度防止syn攻击 Iptables命令 123456789101112131415#列出所有规则iptables -nVL#添加规则iptables -I INPUT -s 192.168.1.0/24 -p tcp -m multiport --dport 22,80 -j ACCEPT#删除一条规则iptables -D INPUT -p icmp -j ACCEPTiptables -D INPUT 2#清除所有规则iptables -F#SNATiptables -t nat -A POSTROUTING -s 10.10.10.177.0/24 -j SNAT --to 10.10.188.111#DNATiptables -t nat -A PREROUTING -d 10.10.188.111 -p tcp --dport 80 -j DNAT --to 10.10.177.222:80#在最后添加默认拒绝的规则iptables -P INPUT DROP iptables规则组成 Usage： 四张表 + 五条链(hook point) + 规则四张表 filter表 访问控制，规则匹配 nat表 请求转发 mangle表 修改数据包，改变包头中内容（TTL，TOS，MARK），需要对应交换机的支持 raw表五条链 INPUT OUTPUT FORWARD PREROUTING POSTROUTING 数据包访问控制 ACCEPT 接收数据包 DROP 直接丢弃数据包，不给客户端返回信息 REJECT 丢弃数据包并给客户端返回信息数据包改写 SNAT 对源地址进行改写 DNAT 对目标地址进行改写信息记录 LOG 将访问信息记录到log","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"},{"name":"网络安全","slug":"网络安全","permalink":"http://arvon.top/tags/网络安全/"}]},{"title":"yum安装ffmpeg记录","slug":"yum安装ffmpeg记录","date":"2016-10-25T08:34:00.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/10/25/yum安装ffmpeg记录/","link":"","permalink":"http://arvon.top/2016/10/25/yum安装ffmpeg记录/","excerpt":"Tips：最近安装一个名为ffmpeg的包，但是epel和ali的源都没有这个包。所以","text":"Tips：最近安装一个名为ffmpeg的包，但是epel和ali的源都没有这个包。所以就google了一下，但是网上提供的方案在我实际环境中并不好使，贴在后面了 个人好使 code12 常规不好使 yum源代码添加如下 yum install -y automake autoconf libtool gcc gcc-c++ 12345[dag]name=Dag RPM Repository for Red Hat Enterprise Linuxbaseurl=http://apt.sw.be/redhat/el$releasever/en/$basearch/daggpgcheck=0enabled=1 yum install -y ffmpeg ffmpeg-devel 好使的 123456rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7rpm --import http://li.nux.ro/download/nux/RPM-GPG-KEY-nux.rorpm -Uvh http://li.nux.ro/download/nux/dextop/el7/x86_64/nux-dextop-release-0-1.el7.nux.noarch.rpmyum repolistyum install -y ffmpegffmpeg -version","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"}]},{"title":"Notes：第一本Docker书","slug":"Notes：第一本Docker书","date":"2016-10-11T07:49:18.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/10/11/Notes：第一本Docker书/","link":"","permalink":"http://arvon.top/2016/10/11/Notes：第一本Docker书/","excerpt":"图解http已经阅读完毕，现在开始转向docker的学习，先读《第一本Docker书》，就这样，简要记录读书笔记。","text":"图解http已经阅读完毕，现在开始转向docker的学习，先读《第一本Docker书》，就这样，简要记录读书笔记。 简介 Docker组件 Docker客户端和服务器 Docker镜像 Registry: 用来保存用户构建的镜像，分为公共和私有两种 Docker容器 常用命令123456789101112131415161718192021222324252627282930313233343536373839404142434445yum install dockersystemctl enable dockersystemctl start dockerdocker info#查看docker信息docker run -i -t ubuntu /bin/bash#以ubuntu为基础镜像创建容易完成后并运行/bin/bash启动一个shell，-i保证STDIN开启-t创建一个可以与之交互的伪终端这两个是创建可交互容器的最基本参数docker run --name arvon_web -i -t ubuntu /bin/bash#创建arvon_web为容器名的容器，可以使用容器名代替容器ID操作容器docker ps#查看正在运行的容器docker ps -a#查看所有容器，包括运行的和停止的docker start arvon_web#启动一个容器，可以使用容器名也可以使用容器ID，启动时会沿用docker run启动时指定的参数，因此会运行交互式会话的shell，使用docker attach可以附着到启动的容器docker attach arvon_web#附着到arvon_web这个容器，如果退出容器的shell，容器也会随之停止运行docker run --name demo_arvon -d ubuntu /bin/sh -c \"while true;do echo hello world; sleep 1; done\"#创建一个守护式进程，并一直输出helloworld，运行后不进入容器，容器在后台运行，可以通过docker ps查看到docker logs demo_arvon#可以查看demo_arvon这个容器的日志，是即时输出docker logs -f demo_arvon#相当于tailf，是实时输出的日志docker logs -ft demo_arvon#带时间戳的实时输出docker top demo_arvon#查看容器内的进程docker exec -d demo_arvon touch /etc/new_config_file#在demo_arvon这个容器中创建一个new_config_file的空文件docker stop demo_arvon#停止容器运行docker ps -n x#显示最后的x的容器，不管这些容器是否在运行都会列出来docker inspect demo_arvon#检查容器然后返回容器的配置信息，包括名称、网络配置等docker inspect --format='&#123;&#123; .State.Running &#125;&#125;' demo_arvon#查看容器的运行状态，true为运行，false为停止docker inspect --format '&#123;&#123; .NetworkSettings.IPAddress &#125;&#125;' demo_arvon#查看容器的IP地址,对运行中的容器有效，为运行的容器返回为空docker inspect --format='&#123;&#123; .State.Running &#125;&#125;' arvon_web demo_arvon#可以一次指定多个容器，会显示每个容器的输出结果docker rm arvon_web#删除容器docker rm `docker ps -a -q`#删除所有容器 镜像使用及构建 使用镜像123456docker images#列出镜像列表docker pull fedora#将fedora镜像拉取到本地docker search puppet#查找Docker Hub上公共可用的镜像 使用dockercommit构建镜像 Command1234567891011121314#使用docker commit创建镜像，不推荐docker loginUsername: arvon2014Password:Email: mail@126.comLogin Succeeded#docker run --name t_img -i -t ubuntu /bin/bashdocker commit -m=\"A new custom image\" --author=\"arvon\" t_img arvon2014/web_test：webserver#其中t_img为容器名可以为容器ID， arvon2014为镜像的用户名web_rest为仓库名；webserver为标签-m为描述-author为作者信息，这些都可以省略docker inspect arvon2014/web_test:webserver#查看新创建镜像的信息docker run -t -i arvon2014/web_test:webserver /bin/bash#使用新创建的镜像运行一个容器","categories":[],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://arvon.top/tags/学习笔记/"},{"name":"Docker","slug":"Docker","permalink":"http://arvon.top/tags/Docker/"}]},{"title":"yum 报错：rpmdb open failed","slug":"yum-报错：rpmdb-open-failed","date":"2016-09-30T03:42:03.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/09/30/yum-报错：rpmdb-open-failed/","link":"","permalink":"http://arvon.top/2016/09/30/yum-报错：rpmdb-open-failed/","excerpt":"背景：刚刚直接kill掉yum进程后，yum报了rpmdb的报错，记一下解决方法","text":"背景：刚刚直接kill掉yum进程后，yum报了rpmdb的报错，记一下解决方法 报错如下 123456rpmdb: Thread/process 17502/140092449953536 failed: Thread died in Berkeley DB libraryerror: db3 error(-30974) from dbenv-&gt;failchk: DB_RUNRECOVERY: Fatal error, run database recoveryerror: cannot open Packages index using db3 - (-30974)error: cannot open Packages database in /var/lib/rpmCRITICAL:yum.main:Error: rpmdb open failed 解决方法 123cd /var/lib/rpm/rm -rf __db.*rpm --rebuilddb","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"}]},{"title":"Python复习笔记","slug":"Python复习笔记","date":"2016-09-19T08:54:29.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/09/19/Python复习笔记/","link":"","permalink":"http://arvon.top/2016/09/19/Python复习笔记/","excerpt":"python复习笔记记录一下观看milo在网易云课堂上《疯狂的python》的复习笔记，源视频地址戳这里","text":"python复习笔记记录一下观看milo在网易云课堂上《疯狂的python》的复习笔记，源视频地址戳这里 第一章：导论课时1-2：简介 编译python文件,生成pyc文件 123#编译1.py文件生成1.pyc的二进制文件import py_compilepy_compile.compile('1.py') 编译python文件，生成pyo文件 12#pyo为优化的二进制文件python -O -m py_compile 1.py 课时3：变量 变量赋值 123a = 1b_c = 'abc'_bc3 = 'a2b3' python变量特性 12345678910111213141516#python的变量和C等语言不一样，python是以数据申请内存空间，即同个变量不用数据使用不同的内存空间，使用id函数可以查看数据对应的内存空间。&gt;&gt;&gt; a=1&gt;&gt;&gt; a1&gt;&gt;&gt; id(a)22053688&gt;&gt;&gt; a=2&gt;&gt;&gt; a2&gt;&gt;&gt; id(a)22053664&gt;&gt;&gt; b=2&gt;&gt;&gt; b2&gt;&gt;&gt; id(b)22053664 课时4：运算符和表达式 赋值运算符 1234567891011&gt;&gt;&gt; a=1&gt;&gt;&gt; a+=2&gt;&gt;&gt; a3&gt;&gt;&gt; a+=3&gt;&gt;&gt; a6&gt;&gt;&gt; a-=4&gt;&gt;&gt; a2&gt;&gt;&gt; 算术运算符 1234+ - * 基础加减乘/ 要小数点1.0/2 整除3.0//2% 取余数** 取幂运算 关系运算符 1234561 &lt; 22 &gt; 11 == 11 &lt;= 12 &gt;= 21 != 2 逻辑运算符 1231 &lt; 2 and 2 &lt; 31 &lt; 2 or 2 &gt; 3not 1 &gt; 2 小练习 1234a=int(raw_input('number1 = '))b=int(raw_input('number2 = '))c = a+bprint str(a)+' + '+str(b)+' = '+str(c) 第二章：数据类型主要数据类型五种包括数字、字符串、元组、列表和字典 课时5：数字和字符串 数字类型包括整型、长整型、浮点型、复数型 123456789101112131415&gt;&gt;&gt; num1 = 123&gt;&gt;&gt; type(num1)&lt;type 'int'&gt;&gt;&gt;&gt; num2 = 123L&gt;&gt;&gt; type(num2)&lt;type 'long'&gt;&gt;&gt;&gt; num3 = 99999999999999999999&gt;&gt;&gt; type(num3)&lt;type 'long'&gt;&gt;&gt;&gt; num4 = 1.23&gt;&gt;&gt; type(num4)&lt;type 'float'&gt;&gt;&gt;&gt; num5 = 1.23j&gt;&gt;&gt; type(num5)&lt;type 'complex'&gt; 字符串类型可以使用单引号、双引号、三单引定义 123456789&gt;&gt;&gt; a = 123&gt;&gt;&gt; stra = '123'&gt;&gt;&gt; type(a)&lt;type 'int'&gt;&gt;&gt;&gt; type(stra)&lt;type 'str'&gt;&gt;&gt;&gt; strb = \"let's say \\\"hello\\\"\"&gt;&gt;&gt; print strblet's say \"hello\" 字符串切片 1234567891011121314151617&gt;&gt;&gt; a = 'abcde'&gt;&gt;&gt; a[1:4]'bcd'&gt;&gt;&gt; a[:]'abcde'&gt;&gt;&gt; a[:4]'abcd'&gt;&gt;&gt; a[4:]'e'&gt;&gt;&gt; a[::1]'abcde'&gt;&gt;&gt; a[::2]'ace'&gt;&gt;&gt; a[-1]'e'&gt;&gt;&gt; a[-4:-1]'bcd' 课时6：元组列表、元组和字符串都是序列，序列的主要特点是索引操作符合切片操作符 序列的基本操作 12345678910111213141516171819202122len() #求序列长度+ #连接两个序列* #重复序列元素in #判断元素是否在序列中max() #返回最大的值min() #返回最小的值cmp(tuple1, tuple2) #比较两个序列值是否相同#EX---&gt;&gt;&gt; str1 = 'abcde'&gt;&gt;&gt; str2 = '12345'&gt;&gt;&gt; str1 + str2'abcde12345'&gt;&gt;&gt; str1 * 5'abcdeabcdeabcdeabcdeabcde'&gt;&gt;&gt; 'a' in str1True&gt;&gt;&gt; max(str1)'e'&gt;&gt;&gt; min(str2)'1'&gt;&gt;&gt; cmp(str1, str2)1 元组通过小括号定义()，元组和列表类似，不过元组被使用元组的值也不改变。 123456&gt;&gt;&gt; userinfo1=(\"zou\", 31, \"female\")&gt;&gt;&gt; userinfo1[1]31&gt;&gt;&gt; userinfo1[0]'zou'#单一元素的元组定义时要加逗号 课时7：列表 列表操作方法通过中括号定义[]，列表是可变类型的数据 123456789101112131415listmilo=['arvon', 24, 'male']#取值age=listmilo[1]#添加listmilo.append('Linux')#删除listmilo.remove(listmilo[2])del(listmilo[2])#修改listmilo[1]=18#查看&gt;&gt;&gt; listmilo ['arvon', 18, 'Linux']#查找&gt;&gt;&gt; \"Linux\" in listmiloTrue 对象和类快速入门对象=属性+方法 课时8：字典使用花括号定义，字典是python中唯一的映射类型（哈希表），字典的对象是可变的，但字典的键必须使用不可变对象，并且一个字典中可以使用不同类型的键keys()或者values()返回键列表或者值列表，items()返回包含键值对的元组。 例子可以直接使用key访问，key不存在会报错，可以使用had_key()方法或者in/not in来判断，另had_key()方法即将弃用 12345&gt;&gt;&gt; a=123&gt;&gt;&gt; b=456&gt;&gt;&gt; dic4=&#123;a:'aaa',b:'bbb','c':'ccc'&#125;&gt;&gt;&gt; dic4&#123;456: 'bbb', 'c': 'ccc', 123: 'aaa'&#125; 使用dict方法生成字典和使用fromkeys生成字典 12fdict=dict(['x',1],['y',2])ddict=&#123;&#125;.fromkeys(('x','y'),-1) 字典的添加和删除字典是无序的所以可以任意添加元素，列表就不行 1234567891011121314151617181920#添加和修改元素元素&gt;&gt;&gt; dic1=&#123;'name':'arvon','age':24,'work':'BJ'&#125;&gt;&gt;&gt; dic1['tel']=123456&gt;&gt;&gt; dic1&#123;'age': 24, 'work': 'BJ', 'tel': 123456, 'name': 'arvon'&#125;#删除元素dict1.clear() #删除dict1字典的所有元素del dict1 #删除dict1这个字典#取值&gt;&gt;&gt; dic1=&#123;'a':1, 'b':2, 'c':3&#125;&gt;&gt;&gt; dic1['a']1&gt;&gt;&gt; dic.get('c')3#返回字典可key列表和values列表&gt;&gt;&gt; dic1=&#123;'a':1, 'b':2, 'c':3&#125;&gt;&gt;&gt; dict.keys(dic1)['a', 'c', 'b']&gt;&gt;&gt; dict.values(dic1)[1, 3, 2] 第三章：流程控制主要包括判断和循环 课时9：分支结构逻辑值（bool）用来表示诸如：对和错、真和假、空与非空等概念。逻辑值True表示非空的量如（string，tuple，list，set，dictonary等），逻辑值False表示0，None，空的量等。 if else判断 1234if 1&lt;2: print 'Yes'else: print 'No' elif 多条件判断 12345a=raw_input(\"Input a num: \")if 1 &lt; a &lt; 3: print str(a)+\" is 1-3\"elif a &gt;= 3: print str(a)+\" is 3-*\" 课时10：逻辑运算符逻辑运算符包括”and”,”or”,”not” 无聊的例子 12345a = 5if a &gt; 1 and a !=2: if a==4 or 1&lt;2: if not a !=5: print \"Oh\" 有用的not 12345def fun(): return 0if not fun(): print \"ok\" 课时11：for循环使用for可以循环字符串、元组和列表 使用for循环字符串 12for i in 'abcde': print i 使用range方法快速生成序列 123#第一参数为开始值不设置默认为0，第二个为结束值，第三个为步长不写默认为步值为1for i in range(0,100,2): print i 小题：计算1到100累加的值 1234num=0for x in range(1,101): num=num+xprint num 课时12：遍历遍历对象可以是字符串、元组、列表 使用索引遍历 12for x in range(len(\"hello\")): print \"hello\"[x] 字典的遍历 1234d = &#123;1:111, 2:222, 4:444, 3:333&#125;for x in d: print x print d[x] 字典元组拆分法 123456d = &#123;1:111, 2:222, 4:444, 3:333&#125;f = d.items()print ffor k,v in f: print k print v 课时13：循环控制主要使用for和while进行循环控制 Python特殊的for循环，在python中for循环是可以有else的 12345#在循环遍历结束时最后会打印一次ending，如果程序未正常遍历结束则不会触发for i in range(3): print ielse: print \"ending\" 使用break跳出循环 1234567for i in range(5): print \"hello\"+str(i) if i == 3: print i breakelse: print \"ending\" 使用continue跳出本次循环 12345678910for i in range(5): print \"hello\"+str(i) if i == 2: print \"22222222\" continue if i == 3: print i breakelse: print \"ending\" 使用pass进行占位操作（代码桩） 123for i in range(5): passprint \"Go\" 使用exit退出程序 123for i in range(5): print i exit 课时14：while循环主要做条件循环，直到表达式为假跳出循环,在设计while循环时一定要让有条件退出 最简单的死循环 123#while 1:while True: print \"hello\" 使用条件退出while 12345while True: print \"Haha\" x = raw_input(\"Input q for quit: \") if x == 'q': break 使用while表达式进行条件判断退出 123456x = ''while x != 'q': print \"hello\" x = raw_input(\"please input a str, q for quit: \") if not x: break while中的else 12345678x = ''while x != 'q': print \"hello\" x = raw_input(\"please input a str, q for quit: \") if not x: breakelse: print \"ending\" 第四章：函数函数就是完成特定功能的语句组，可以通过函数名在程序不同位置多次执行（函数调用）。 课时15：函数定义和调用 使用def定义函数，括号里面是参数列表() 12345def add(a,b): c = a + b print cadd(1,2)add(3,4) 基本函数和返回值初探 123456789a = 100def fun(): if False: print \"Hello\" print a return 0#fun()if not fun(): print \"ok\" 课时16：函数形参、实参、默认参数在定义函数时函数名后面括号中的变量叫做形参，在调用函数时函数名后面的括号中的变量叫做实参 简单例子 12345def fun(x): print \"ok\" print xs = raw_input(\"Input something: \")fun(s) 默认参数例子 123456def fun(x,y='lalala'): print x,ys = raw_input(\"Input something: \")fun(4)fun(y='cacaca')fun(2,'goog') 课时17：变量作用域在python中任何变量都有其特定的作用域，一般在函数中定义的变量只能在函数内部使用，也叫局部变量。在一个文件顶部定义的变量可以提供给该文件中的任何函数调用，也叫全局变量。 一个例子 12345678a = 150def fun(): a = 100 print 'in',a #这里打印出的是100fun()print 'out',a#out打印出是150 使用global将局部变量声明为全局变量 123456789a = 150def fun(): a = 100 global b b = 12345 print 'in',afun()print 'out',aprint b 课时18：函数返回值函数被调用后会返回一个指定的值即返回值，不指定默认返回None，可以使用return直接指定，返回值可以是任意类型，return执行后函数终止 一个例子1234567#coding:utf8def f(x,y): t=x+y return tz = f(2,3)print z#此时z为None 课时19：冗余参数处理 正常多类型传值 1234567def f(x): print xf(1)f('abc')f(['arvon','mo'])f(&#123;'arvon':123,'blog':'arvon.top'&#125;)f(range(10)) 传递元组到多个参数,号和**的使用传递元组使用，传递字典使用，推荐使用，原因看例子 123456789def f(name='name',age='0'): print 'name: %s' % name print 'age: %s' % agef()f('test',12)t=('arvon',24)tt=&#123;'age':23, 'name':'mo'&#125;f(*t)f(**tt) 冗余例子,args的使用关于args的意思是接收多余的参数，把这些参数当做一个元组，这个元组名称为args,使用**args冗余字典方式的参数 12345678def go(x,*args,**dargs): print x print args print dargsgo(1,2)go(1,2,3)go(1,2,3,4)go(1,2,3,4,'m'=5) 课时20：匿名函数lambdalambda表达式函数是一种快速定义单行的最小函数，从Lisp借用而来，可使用在任何需要函数的地方。 最简实例 12345678910&gt;&gt;&gt; def f(x,y):... return x*y...&gt;&gt;&gt; f(2,3)6&gt;&gt;&gt; lambda x,y:x*y&lt;function &lt;lambda&gt; at 0x7fa5c3e7ea28&gt;&gt;&gt;&gt; g = lambda x,y:x*y&gt;&gt;&gt; g(2,3)6 reduce函数的使用括号中第一项为函数，第二项为列表，一次作用两个值，配合lambda很好用。 1234567l = range(1,6)def f(x,y): return x*yone=reduce(f,l)g = lambda x,y:x+ytwo=reduce(g,l)print one,two 课时21：实现分支结构switch语句用于编写多分支结构的程序类似if else，但python并未提供switch语句。在python当中使用字典来实现相同的功能。 精简的例子 1&#123;1:case1,2:case2&#125;.get(x,lambda *arg, **key:)() 通过字典调用函数 123456789101112131415161718#coding:utf-8from __future__ import divisiondef jia(x,y): return x+ydef jian(x,y): return x-ydef cheng(x,y): return x*ydef chu(x,y): return x/yoperator = &#123;'+':jia,'-':jian,'*':cheng,'/':chu&#125;#print operator['+'](3,2)#print operator['/'](3,2)#print jia(3,2)def fff(x,o,y): print operator.get(o)(x,y)fff(3,'+',2)fff(3,'/',2) 上个例子的复杂方式，有多余的判断方便和上面例子对比 123456789101112131415161718192021222324#coding:utf-8from __future__ import divisiondef jia(x,y): return x+ydef jian(x,y): return x-ydef cheng(x,y): return x*ydef chu(x,y): return x/ydef operator(x,o,y): if o == '+': print jia(x,y) elif o == '-': print jian(x,y) elif o == '*': print cheng(x,y) elif o == '/': print chu(x,y) else: passoperator(4, '+', 2)operator(4, '-', 2)operator(4, '*', 2) 课时22：常用内置函数 使用callable检测是否可以直接调用函数 12345678&gt;&gt;&gt; callable(min)True&gt;&gt;&gt; callable(f)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;NameError: name 'f' is not defined&gt;&gt;&gt; callable(divmod)True 使用abs取绝对值 12345678#def a(x):# if x &lt; 0:# return -x# else:# return x#print a(10)#print a(-9)print abs(-9) 使用max取最大值,使用min取最小值 123l = range(12)print max(l)print min(l) 取列表长度 123456#coding:utf8l=[1, 2, 4, 5, 6]#取列表元素个数print len(l)#取商和摩print divmod(5,2) 测试数据类型是否相同 1234#if type(l) == type([]):isinstance(l,list)isinstance(l,int)isinstance(l,str) 使用cmp判断字符串是否一样 123# if l == 'strxxx':cmp(l,'strxxx')#相同返回0，0在判断时为假，需使用not 类型转换函数 12345678910long() #长整形int()float()str()list() #列表tuple() #元组hex() #16进制转换oct() #8进制转换chr() ord() 课时23：与类相关的内置函数 与string相关的几个函数 123456789101112131415#首字母大写&gt;&gt;&gt; s = 'hello world'&gt;&gt;&gt; s.capitalize()'Hello world'#字符串替换,查看帮助使用help(str.capitalize)&gt;&gt;&gt; s.replace('hello','good')'good world'#字符串切割&gt;&gt;&gt; ip='192.168.1.123'&gt;&gt;&gt; ip.split('.')['192', '168', '1', '123']&gt;&gt;&gt; ip.split('.',1)['192', '168.1.123']&gt;&gt;&gt; ip.split('.',2)['192', '168', '1.123'] 直接使用内置函数跟使用import导入的小区别 12345s = 'hello world's.replace('hello','good')##---使用importimport stringstring.replace(s,'hello','good') 使用filter函数过滤 123456#filter（function，list），会把函数判断为Ture时list的元素取出来l=range(10)def f(x): if x &gt; 5: return Trueprint filter(f,l) 课时24：序列处理函数 使用zip或map进行并行遍历使用zip只能对元素数量一样的，map可以将数量不同的地方用None代替 1234567&gt;&gt;&gt; name=['arvon', 'mo', 'lover']&gt;&gt;&gt; age=['23', '25', '26']&gt;&gt;&gt; tel=['123456', '324566', '54334123']&gt;&gt;&gt; zip(name,age,tel)[('arvon', '23', '123456'), ('mo', '25', '324566'), ('lover', '26', '54334123')]&gt;&gt;&gt; map(None,name,age,tel)[('arvon', '23', '123456'), ('mo', '25', '324566'), ('lover', '26', '54334123')] map函数高阶用法，可以对遍历后的数据进行函数操作 123456789&gt;&gt;&gt; a=[1,3,5]&gt;&gt;&gt; b=[2,4,6]&gt;&gt;&gt; def mf(x,y):... return x*y...&gt;&gt;&gt; map(None,a,b)[(1, 2), (3, 4), (5, 6)]&gt;&gt;&gt; map(mf,a,b)[2, 12, 30] reduce阶乘例子 12345678910&gt;&gt;&gt; l=range(1,100)&gt;&gt;&gt; def xf(x,y):... return x +y...&gt;&gt;&gt; reduce(xf,l)4950&gt;&gt;&gt; reduce(lambda x,y:x+y,l)4950&gt;&gt;&gt; filter(lambda x:x%2 == 0,l)[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98] 课时25：模块和包模块是python组织代码的基本方式，python脚本都是以py为扩展名的文件保存，一个脚本可以单独运行也可以导入另一个脚本运行，当导入运行时，被导入的脚本就称作模块（module）。模块名与脚本名字相同，如test.py的模块名就是test可以通过import test导入。 python找寻模块路径优先级 1234#当前目录&gt;lib下&gt;其他#查找导入模块的路径import a_moduleprint a_module.__file__ 实用的name,python内置当直接运行脚本返回为main，当被调用执行返回为脚本名。 12345678910111213141516171819202122232425from __future__ import divisiondef jia(x,y): return x+ydef jian(x,y): return x-ydef cheng(x,y): return x*ydef chu(x,y): return x/ydef operator(x,o,y): if o == '+': print jia(x,y) elif o == '-': print jian(x,y) elif o == '*': print cheng(x,y) elif o == '/': print chu(x,y) else: passif __name__ == '__main__': operator(4, '+', 2) operator(4, '-', 2) operator(4, '*', 2) operator(4, '/', 2) 简单例子 123import calprint cal.jia(1,2)#实用cal调用模块，实用.符号调用模块内的函数也叫方法 Python模块可以按目录组织成包 12345###创建一个包的步骤#- 建立一个名字为包名字的目录#- 在该目录下创建一个__init__.py文件#- 根据需要在该目录下存放脚本文件、已编译扩展及子包#- import pack.m1, pack.m2, pack.m3 第五章：正则表达式课时26：简介正则表达式（RE）是一种小型的、高度专业化的编程语言，它内嵌在python中，并通过re模块实现。 小例子1234567&gt;&gt;&gt; import re&gt;&gt;&gt; s = 'abc'&gt;&gt;&gt; s = r'abc'&gt;&gt;&gt; re.findall(s,'aaaaaaaa')[]&gt;&gt;&gt; re.findall(s,'aaaaaabcaa')['abc'] 课时27：元字符 普通字符包括大多数字母和字符以及数字等都匹配自身 1234&gt;&gt;&gt; st = 'top tip tqp twp tep'&gt;&gt;&gt; res=r'top'&gt;&gt;&gt; re.findall(res,st)['top'] 元字符包括.^$*+?{}[]|() 12345678910111213&gt;&gt;&gt; str='I can say \"tip top t4p world ^go\" and I say hello world'&gt;&gt;&gt; rs1 = r'I'&gt;&gt;&gt; re.findall(rs1,str)['I', 'I']&gt;&gt;&gt; rs2 = r'^I'&gt;&gt;&gt; re.findall(rs2,str)['I']&gt;&gt;&gt; rs3 = r't[io]p'&gt;&gt;&gt; re.findall(rs3,str)['tip', 'top']&gt;&gt;&gt; rs4 = r'\\^go'&gt;&gt;&gt; re.findall(rs4.str)[^go] 关于转义的列表 123456\\d #匹配任何十进制数相当于[0-9]\\D #匹配任何非数字字符，相当于[^0-9]\\s #匹配任何空白字符，相当于[\\t\\n\\r\\f\\v]\\S #匹配任何非空白字符，相当于[^\\t\\n\\r\\f\\v]\\w #匹配任何字母数字字符，相当于[a-zA-Z0-9_]\\W #匹配任何非字符数字字符，相当于[^a-zA-Z0-9_] 关于重复的正则其中*表示0次或多次、+表示一次或多次，？表示0次或一次，”.”表示匹配一次 123456789101112131415161718&gt;&gt;&gt; tel = '010-123456'&gt;&gt;&gt; rs = r'010-\\d&#123;6&#125;'&gt;&gt;&gt; re.findall(rs,tel)['010-123456']&gt;&gt;&gt; re.findall(rs,tel)['010-123456']&gt;&gt;&gt; rs = r'010-\\d*'&gt;&gt;&gt; re.findall(rs,tel)['010-123456']&gt;&gt;&gt; rs = r'010-\\d+'&gt;&gt;&gt; re.findall(rs,tel)['010-123456']&gt;&gt;&gt; rs = r'010-\\d&#123;5&#125;?'&gt;&gt;&gt; re.findall(rs,tel)['010-12345']&gt;&gt;&gt; rs = r'010-\\d?'&gt;&gt;&gt; re.findall(rs,tel)['010-1'] 关于花括号灵活用法 12#可控制匹配次数，如1到5此rs = r'a&#123;1,5&#125;' 课时28：正则表达式常用函数 正则表达式编译执行，经常用的话建议采用这种方法 1234567&gt;&gt;&gt; import re&gt;&gt;&gt; r1 = \"\\d&#123;3,4&#125;-?\\d&#123;6&#125;\"&gt;&gt;&gt; p_tel = re.compile(r1)&gt;&gt;&gt; p_tel&lt;_sre.SRE_Pattern object at 0x7f44193a2f10&gt;&gt;&gt;&gt; re.findall(p_tel,'010-123456')['010-123456'] match和search方法 1234567891011121314#group()返回被re匹配的字符串#start()返回匹配开始的位置#end()返回匹配结束的位置#span()返回一个元组包含匹配的位置&gt;&gt;&gt; import re&gt;&gt;&gt; csvt_re = re.compile(r'csvt',re.I)&gt;&gt;&gt; csvt_re.findall('csVt')['csVt']&gt;&gt;&gt; csvt_re.findall('csVt csvt CsVt')['csVt', 'csvt', 'CsVt']&gt;&gt;&gt; csvt_re.match('csvt hello')&lt;_sre.SRE_Match object at 0x7f44192f1648&gt;&gt;&gt;&gt; csvt_re.search('csvt hello')&lt;_sre.SRE_Match object at 0x7f44192f16b0&gt; sub函数替换字符串 123456789&gt;&gt;&gt; s'hello csvt'&gt;&gt;&gt; s.replace('csvt','good')'hello good'&gt;&gt;&gt; s'hello csvt'&gt;&gt;&gt; rs = r'c..t'&gt;&gt;&gt; re.sub(rs,'python','csvt caat cvvt cccc')'python python python cccc' 使用re.split进行带正则的分割 123456&gt;&gt;&gt; ip = '1.2.3.4'&gt;&gt;&gt; ip.split('.')['1', '2', '3', '4']&gt;&gt;&gt; s = \"123*456-789+000\"&gt;&gt;&gt; re.split(r'[\\+\\-\\*]',s)['123', '456', '789', '000'] 课时29：正则表达式内置属性及分组 正则编译标志 12345DOTALL,S #使.匹配包括换行在内的所有字符IGNORECASE,I #使匹配对大小写不敏感LOCALE,L #做本地化识别，匹配法语等。。。MULTILINE,M #多行匹配，影响^和$VERBOSE,X #能够使用REs的verbose状态，使之被组织更清晰易懂 关于S的例子 12345678910&gt;&gt;&gt; r1 = r\"csvt.net\"&gt;&gt;&gt; re.findall(r1,'vsvt.net')[]&gt;&gt;&gt; r1 = r\"csvt.net\"&gt;&gt;&gt; re.findall(r1,'csvt.net')['csvt.net']&gt;&gt;&gt; re.findall(r1,'csvtonet')['csvtonet']&gt;&gt;&gt; re.findall(r1,'csvtonet',re.S)['csvtonet'] 关于M的例子 1234567891011&gt;&gt;&gt; s =\"\"\"... hello csvt... csvt hello... hello csvt hello... csvt hehe... \"\"\"&gt;&gt;&gt; r = r\"^csvt\"&gt;&gt;&gt; re.findall(r,s)[]&gt;&gt;&gt; re.findall(r,s,re.M)['csvt', 'csvt'] 关于X的例子，当正则是多行时使用 123456789&gt;&gt;&gt; tel = r\"\"\"... \\d&#123;3,4&#125;... -?... \\d&#123;8&#125;... \"\"\"&gt;&gt;&gt; re.findall(tel,'010-12345678')[]&gt;&gt;&gt; re.findall(tel,'010-12345678',re.X)['010-12345678'] 分组匹配 123456789&gt;&gt;&gt; email = r\"\\w&#123;3&#125;@\\w+(\\.com|\\.cn)\"&gt;&gt;&gt; re.match(email,'zzz@csvt.cn')&lt;_sre.SRE_Match object at 0x7f44192fca08&gt;&gt;&gt;&gt; re.match(email,'zzz@csvt.com')&lt;_sre.SRE_Match object at 0x7f44192fc990&gt;&gt;&gt;&gt; re.match(email,'zzz@csvt.org')&gt;&gt;&gt; re.findall(email,'zzz@csvt.com')#使用findall会优先返回分组匹配的数据，所以一般用match做判断即可['.com'] 利用分组特性的例子 123456789101112&gt;&gt;&gt; s =\"\"\"... hhsdj dskj hello src=arvon yes jdasdfa... adsfasd src=mo yes dasfasdf... src=lover... hello src=python asdfas... \"\"\"&gt;&gt;&gt; r1 = r\"hello src=.+ \"&gt;&gt;&gt; re.findall(r1,s)['hello src=arvon yes ', 'hello src=python ']&gt;&gt;&gt; r1 = r\"hello src=(.+) +yes\"&gt;&gt;&gt; re.findall(r1,s)['arvon'] 课时30：一个小爬虫 下载贴吧或空间中所有图片123456789101112131415161718192021#!/usr/bin/python#coding:utf-8import reimport urllibdef getHtml(url): page = urllib.urlopen(url) html = page.read() return htmldef getImg(html): reg = r'src=\"(.*?\\.jpg)\" size' imgre = re.compile(reg) imglist = re.findall(imgre,html) #print imglist imgnum = 0 for imgurl in imglist: urllib.urlretrieve(imgurl,'%s.jpg' % imgnum) imgnum +=1#wantUrl = raw_input('Input URL: ')wantUrl = 'http://tieba.baidu.com/p/4637471656'html = getHtml(wantUrl)getImg(html) 课时31：数据结构之深拷贝和浅拷贝python对内存的使用，浅拷贝就是对引用的拷贝，而深拷贝是对对象资源的拷贝 实例特点123456789101112131415161718192021222324252627282930313233343536373839404142&gt;&gt;&gt; import copy&gt;&gt;&gt; a = [1, 2, 3, ['a', 'b', 'c']]&gt;&gt;&gt; b=a&gt;&gt;&gt; c = copy.copy(a)&gt;&gt;&gt; a[1, 2, 3, ['a', 'b', 'c']]&gt;&gt;&gt; b[1, 2, 3, ['a', 'b', 'c']]&gt;&gt;&gt; c[1, 2, 3, ['a', 'b', 'c']]&gt;&gt;&gt; id(a)140296085124304&gt;&gt;&gt; id(b)140296085124304&gt;&gt;&gt; id(c)140296085139680&gt;&gt;&gt; a.append('d')&gt;&gt;&gt; a[1, 2, 3, ['a', 'b', 'c'], 'd']&gt;&gt;&gt; b[1, 2, 3, ['a', 'b', 'c'], 'd']&gt;&gt;&gt; c[1, 2, 3, ['a', 'b', 'c']]&gt;&gt;&gt; id(a[0])30618424&gt;&gt;&gt; id(c[0])30618424&gt;&gt;&gt; a[3].append('d')&gt;&gt;&gt; a[1, 2, 3, ['a', 'b', 'c', 'd'], 'd']&gt;&gt;&gt; c[1, 2, 3, ['a', 'b', 'c', 'd']]&gt;&gt;&gt; d = copy.deepcopy(a)&gt;&gt;&gt; a[1, 2, 3, ['a', 'b', 'c', 'd'], 'd']&gt;&gt;&gt; d[1, 2, 3, ['a', 'b', 'c', 'd'], 'd']&gt;&gt;&gt; a[3].append('e')&gt;&gt;&gt; a[1, 2, 3, ['a', 'b', 'c', 'd', 'e'], 'd']&gt;&gt;&gt; d[1, 2, 3, ['a', 'b', 'c', 'd'], 'd'] 高级功能课时32：文件读写 文件的读写，使用open或file函数实现 12345678910#usage: file_handler = open(filename, mode)#moder #只读，默认r+ #读写w #写入，先删除原文件，再重新写入，如果没有就创建文件w+ #读写，先删除源文件，如果文件没有就创建（可以写入输出）a #写入，在文件末尾追加新的内容，文件不存在就创建a+ #读写，在文件末尾追加新的内容，文件不存在就创建b #打开二进制文件，可以与r，w，a，+结合使用U #支持所有的换行符号。如\\r,\\n,\\r\\n 使用open和file打开、读取、关闭文件 12345678910111213141516&gt;&gt;&gt; fo = open('/data/python/tmp/file/test.txt')&gt;&gt;&gt; fo.read()'hello world\\n'&gt;&gt;&gt; fo.close()&gt;&gt;&gt; fo.read()Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;ValueError: I/O operation on closed file&gt;&gt;&gt; fo1 = file('/data/python/tmp/file/test.txt')&gt;&gt;&gt; fo1.read()'hello world\\n'&gt;&gt;&gt; fo1.close()&gt;&gt;&gt; fo1.read()Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;ValueError: I/O operation on closed file 使用write对文件写入 123456&gt;&gt;&gt; fnew = open('new.txt', 'w')&gt;&gt;&gt; fnew.write(\"hello world\\nMy name is arvon\\n\")&gt;&gt;&gt; fnew.close()&gt;&gt;&gt; rnew = open('new.txt')&gt;&gt;&gt; rnew.read()'hello world\\nMy name is arvon\\n' 课时33：文件对象方法 文件对象方法 123456789FileObject.close() #关闭文件String = FileObject.readline([size])List = FileObject.readlines([size])String = FileObject.read([size])FileObject.next()FileObject.write(string)FileObject.writelines(List)FileObject.seek(偏移量，选项)FileObject.flush() 使用for遍历文件行 12345678910&gt;&gt;&gt; for i in open('new.txt'):... print i...hello worldMy name is arvongo go go&gt;&gt;&gt; 使用readline读取行,使用readlines返回列表 123456789101112&gt;&gt;&gt; f1 = open('new.txt')&gt;&gt;&gt; f1.readline()'hello world\\n'&gt;&gt;&gt; f1.readline()'My name is arvon\\n'&gt;&gt;&gt; f1.readline()'go go go\\n'&gt;&gt;&gt; f1.readline()''&gt;&gt;&gt; f1 = open('new.txt')&gt;&gt;&gt; f1.readlines()['hello world\\n', 'My name is arvon\\n', 'go go go\\n'] 使用next，返回当前行，并将指针指到下一行 12345678910111213#readline会读取结束会读取空字符串，而next不会&gt;&gt;&gt; f1 = open('new.txt')&gt;&gt;&gt; f1.next()'hello world\\n'&gt;&gt;&gt; f1.next()'My name is arvon\\n'&gt;&gt;&gt; f1.next()'go go go\\n'&gt;&gt;&gt; f1.next()Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;StopIteration&gt;&gt;&gt; 使用writelines实现多行写入可以多行写，效率比write高，速度快 12345678&gt;&gt;&gt; f1 = open('new.txt', 'a')&gt;&gt;&gt; l = ['one\\n', 'two\\n', 'three\\n']&gt;&gt;&gt; f1 = open('new.txt', 'a')&gt;&gt;&gt; f1.writelines(l)&gt;&gt;&gt; f1.close()&gt;&gt;&gt; f2 = open('new.txt')&gt;&gt;&gt; f2.read()'hello world\\nMy name is arvon\\ngo go go\\none\\ntwo\\nthree\\n' 关于指针seek简单操作 12345678910111213#说明FileObject.seek(偏移量，选项)#选项=0，表示将文件指针指向文件头部到偏移量字节处#选项=1，表示将文件指针指向文件的当前位置，向后移动偏移量字节#选项=2，表示将文件指针指向从文件的尾部，向前移动偏移量字节&gt;&gt;&gt; f2 = open('new.txt')&gt;&gt;&gt; f2.read()'hello world\\nMy name is arvon\\ngo go go\\none\\ntwo\\nthree\\n'&gt;&gt;&gt; f2.read()''&gt;&gt;&gt; f2.seek(0,0)&gt;&gt;&gt; f2.seek(0,0)&gt;&gt;&gt; f2.read()'hello world\\nMy name is arvon\\ngo go go\\none\\ntwo\\nthree\\n' 使用flush提交更新，可以在不使用close的情况下查看文件的写入情况 123&gt;&gt;&gt; f1=open('new.txt','w')&gt;&gt;&gt; f1.writelines(l)&gt;&gt;&gt; f1.flush() 查找hello的个数 123456789101112131415161718import ref1 = open('a.t')print len(re.findall('hello',f1.read()))f1.close#----two&gt;&gt;&gt; f1 = open('a.t')&gt;&gt;&gt; f1.read()'hello world\\nhello hello world\\n'&gt;&gt;&gt; re1 = r'(hello) '&gt;&gt;&gt; import re&gt;&gt;&gt; f1.seek(0,0)&gt;&gt;&gt; re.findall(re1,f1.read())['hello', 'hello', 'hello']&gt;&gt;&gt; re.findall(re1,f1.read())[]&gt;&gt;&gt; f1.seek(0,0)&gt;&gt;&gt; len(re.findall(re1,f1.read()))3 文件内容替换 123456789101112f1 = open('a.t')f2 = open('a2.t')for i in f1: f2.write(i.replace('hello', 'csvt')f1.close()f2.close()#---2fp1 = file('a.t', 'w+')s = f1.read()f1.seek(0,0)f1.write(s.replace(\"hello\", \"csvt\"))fp1.close() 课时34：OS模块 os模块常用函数 12345678mkdir(path[,mode=0777])makedirs(name,mode=511)rmdir(path)removedirs(path)listdir(path)getcwd()chdir(path)walk(top,topdown=True, onerror=None) 实例参照 12345678910111213141516#codeing:utf8import os#创建单个目录= mkdiros.mkdir('./mydir')#创建多级目录= mkdir -pos.mkdirs('./a/b/c')#删除空目录os.rmdir('./mydir')#删除多级空目录os.rmdirs('./a/b/c')#列出当前目录下文件,不包含子目录= lsos.listdir('.')#获取当前路径= pwdos.getcwd('.')#切换目录= cdos.chdir('./a') 课时35：目录遍历 列出多级目录中的文件路径(自写) 1234567891011import osdef dirList(path): filelist = os.listdir(path) fpath = os.getcwd() for filename in filelist: filepath = os.path.join(fpath,path,filename) if os.path.isdir(filepath): dirList(filepath) else: print filepathdirList('testdir') 利用walk模块递归 123456import osallDate = os.walk('testdir')for dirpath,zidir,filenames in allDate: for eachfile in filenames: eachfilepath = os.path.join(dirpath,eachfile) print eachfilepath 课时36：异常处理 常见python异常 12345678910111213AssertionError #assert语句失败AttributerError #试图访问一个对象没有的属性IOError #输入输出异常，基本是无法打开文件ImportError #无法引入模块或包，基本是路径问题IndentationError #语法错误，代码没有正确对齐IndexError #下标索引超出序列边界KeyError #试图访问字典中不存在的键KeyboardInterrupt #Ctrl-c终止NameError #使用一个还未赋予对象的变量SyntaxError #python代码逻辑语法错误TypeError #传入的对象类型与要求不符UnboundLocalError #试图访问一个还未设置的全局变量，基本上由于另有一个同名全局变量ValueError #传入一个不被期望的值，即使类型正确 异常及异常抛出使用try时报错会终止执行错误语句以下的语句 123456789#coding:utf8filename = raw_input(\"要操作的文件：\")try: open(filename) print filenameexcept IOError,msg: print \"该文件不存在\"except NameError,msg: pass finally子句，不关心捕获什么异常，代码必须执行，如文件关闭、释放锁、把数据库连接返还给连接池等。 12345678try: f = open(filename) print helloexcept IOError,msg: passfinally: f.close() print \"ok\" 使用raise抛出异常,抛出的异常类型必须是python中已定义的类型，不能随意起名 123filename = raw_input(\"something: \")if filename == \"hello\": raise TypeError(\"nothing!!!!\") 课时37：mysql数据库模块 安装MySQL-python模块 1yum install MySQL-python 使用MySQLdb模块,交互模式下 1234567891011121314151617181920212223242526272829303132&gt;&gt;&gt; import MySQLdb&gt;&gt;&gt; conn = MySQLdb.connect(user='root',passwd='admin',host='127.0.0.1')&gt;&gt;&gt; cur = conn.cursor()&gt;&gt;&gt; conn.select_db('test')&gt;&gt;&gt; cur.execute(\"insert into mytable(id,username) value(2,'mo');\")1L&gt;&gt;&gt; sqli = \"insert into mytable(id,username) value(%s, %s);\"&gt;&gt;&gt; cur.execute(sqli,(3,'lover'))&gt;&gt;&gt; sqlim = \"insert into mytable(id,username) values(%s,%s);\"&gt;&gt;&gt; cur.executemany(sqli,[(4,'haha'),(5,'papa'),(6,'dada')])3L&gt;&gt;&gt; cur.execute('delete from mytable where id=4')1L&gt;&gt;&gt; cur.execute(\"update mytable set username='gogo' where id=5\")1L&gt;&gt;&gt; cur.execute(\"select * from mytable\")6L&gt;&gt;&gt; cur.fetchone()(1L, 'arvon')&gt;&gt;&gt; cur.execute(\"select * from mytable\")6L&gt;&gt;&gt; cur.fetchone()(1L, 'arvon')&gt;&gt;&gt; cur.fetchone()(1L, 'arvon')&gt;&gt;&gt; cur.scroll(0,'absolute')&gt;&gt;&gt; cur.fetchone()(1L, 'arvon')&gt;&gt;&gt; cur.fetchmany(cur.execute(\"select* from mytable\"))((1L, 'arvon'), (2L, 'mo'), (3L, 'lover'), (5L, 'gogo'), (6L, 'dada'), (7L, 'dudu'))&gt;&gt;&gt; cur.close()&gt;&gt;&gt; conn.close() 在脚本中使用的例子 123456789101112131415161718192021222324252627282930import MySQLdb#coding:utf-8#mysql&gt;create table mytable (id int , username char(20));conn = MySQLdb.connect(user='root',passwd='admin',host='127.0.0.1')#连接到数据库服务器cur = conn.cursor()#连接到数据库后游标的定义conn.select_db('test')#连接到test数据库cur.execute(\"insert into mytable(id,username) value(2,'mo');\")#插入一条数据sqlim = \"insert into mytable(id,username) values(%s,%s);\"cur.executemany(sqli,[(4,'haha'),(5,'papa'),(6,'dada')])#使用格式化字符串，一次添加多条数据，同理可应用于修改和删除cur.execute('delete from mytable where id=4')#删除一条数据cur.execute(\"update mytable set username='gogo' where id=5\")#修改一条数据cur.execute(\"select * from mytable\")cur.fetchone()cur.scroll(0,'absolute')cur.fetchmany()#查询一条数据，先select出数据条目数量，再通过fetchone依次取值,取值完成后可以通&gt;过scroll重新定义游标位置，如上为让游标在到开头，使用getchmany可以以元组形式取出所有值cur.fetchmany(cur.execute(\"select* from mytable\"))#使用这种方法可以直接取出所有值cur.close()#关闭游标conn.close()#关闭数据库连接 课时38：面向对象编程之类和对象在python中将所有类型都当做对象 类和对象 面向过程和面向对象的编程面向过程的编程：函数式编程、c程序等面向对象的编程：C++，Java，Python等 类和对象是面向对象中的两个重要概念类：是对事物的抽象，如汽车模型对象：是类的一个实例，如轿车、客车 范例说明汽车模型可以对汽车的特征和行为进行抽象，然后可以实例化为一台真实的汽车实体出来。 Python类定义 Python类的定义使用class关键字定义一个类，并且类名的首字母要大写；当程序员需要创建的类型不能用简单类型表示时就需要创建类；类把需要的变量和函数结合在一起，这种包含也称为封装。 Python类的结构 class 类名:… 成员变量… 成员函数(至少有一个形参self)… 简单的例子 1234567891011&gt;&gt;&gt; class Test:... first = 123... second = 456... def f(self):... return 'test'...&gt;&gt;&gt; dog = Test()&gt;&gt;&gt; dog.f()'test'&gt;&gt;&gt; dog.first123 对象的创建创建对象的过程称之为实例化；当一个对象被创建后，包括三方面的特征：对象的句柄、属性和方法。句柄用于区分不同的对象对象的属性和方法与类中的成员变量和成员函数对应 小例 if name == “main“… myClass1 = MyClass() ok, python复习告一段落,下一阶段docker进阶","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://arvon.top/tags/Python/"},{"name":"学习笔记","slug":"学习笔记","permalink":"http://arvon.top/tags/学习笔记/"}]},{"title":"Centos下搭建svn服务器","slug":"Centos下搭建svn服务器","date":"2016-09-09T08:57:56.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/09/09/Centos下搭建svn服务器/","link":"","permalink":"http://arvon.top/2016/09/09/Centos下搭建svn服务器/","excerpt":"前言：因需求需搭建一个svn服务器，这里顺带记录一下svn的搭建及使用方法。","text":"前言：因需求需搭建一个svn服务器，这里顺带记录一下svn的搭建及使用方法。 环境准备 centos6.7 x_64 yum源 windows客户端测试 搭建记录软件安装 安装svn软件 1yum install subversion -y 创建版本库目录 1mkdir -pv /data/svn/svnrepos 创建版本库 1svnadmin create /data/svn/svnrepos 配置修改配置文件路径均在版本库目录下，以我的为例即/data/svn/svnrepos/conf下 修改svnserve.conf文件，打开以下代码注释 12345anon-access = read #匿名用户可读auth-access = write #授权用户可写password-db = /data/svn/svnrepos/conf/passwd #使用哪个文件作为账号文件authz-db = /data/svn/svnrepos/conf/authz #使用哪个文件作为权限文件realm = Arvon Test Repository #提示信息 修改passwd文件如下在[users]下面添加用户名和密码,此文件就是管理账户密码的文件 12[users]arvon = arovn.top 修改authz文件如下在[groups]下添加目录或子目录做到更细致的权限控制，在目录下控制用户权限 12345[groups][/]#/ 表示对根目录（即/svn/project目录）下的所有子目录范围设置权限；#[/abc] 表示对资料库中abc项目设置权限；arvon = rw 启动svn服务 直接使用命令 1svnserver -d -r /data/svn/svnrepos/ 使用守护进程启动（需修改启动配置） 1234#vi /etc/init.d/svnservedaemon --pidfile=$&#123;pidfile&#125; $exec $args -r /data/svn/svnrepos#然后使用命令启动/etc/init.d/svnserve start 使用测试客户端下载 windows mac/linxu 客户端测试使用由于目前好用的贴图网不好找就参考被人吧,感谢感谢~~ TortoiseSVN使用简介 中文教程pdf版下载 TortoiseSVN使用方法","categories":[],"tags":[{"name":"版本控制","slug":"版本控制","permalink":"http://arvon.top/tags/版本控制/"}]},{"title":"CentOS6.8搭建个人dokuwiki","slug":"CentOS6-8搭建个人dokuwiki","date":"2016-08-20T03:42:10.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/08/20/CentOS6-8搭建个人dokuwiki/","link":"","permalink":"http://arvon.top/2016/08/20/CentOS6-8搭建个人dokuwiki/","excerpt":"摘要：dokuwiki是一个开源的wiki引擎程序，非常适合知识库管理，因为有的东西知识不完善不好用blog写，有时也许即时记录一下想法什么的，所以这个还是很有必要的。之前也对比过其他的wiki程序，不过这个比较符合我的要求，所以就用这个程序，这里主要对安装配置做一下记录。","text":"摘要：dokuwiki是一个开源的wiki引擎程序，非常适合知识库管理，因为有的东西知识不完善不好用blog写，有时也许即时记录一下想法什么的，所以这个还是很有必要的。之前也对比过其他的wiki程序，不过这个比较符合我的要求，所以就用这个程序，这里主要对安装配置做一下记录。 环境准备 这里使用的是CentOS6.8系统安装。 使用nginx+php发布,另外dokuwiki不需要数据库，这点我很喜欢。 nginx+php环境准备 参照我之前的nginx+php环境搭建 注意安装完成php-fpm完成后记得查看php进程是否正常，php-fpm进程默认是9000端口 安装dokuwiki 下载dokuwiki下载地址https://download.dokuwiki.org/注意:下载的时候会让选择语言，这里我没直接贴链接的原因也是这个，选一个中文即可，英文默认必须安装。 安装dokuwiki12345678910#解压tar xvf dokuwiki-a1b9b25f129b085a00920bd821719ccd.tgz#移动到nginx网页存放目录mv dokuwiki /usr/share/nginx/html/#修改权限chmod -R 777 /usr/share/nginx/html/dokuwiki/data/chmod -R 777 /usr/share/nginx/html/dokuwiki/conf/#重启nginx和php-fpm/etc/init.d/nginx restart/etc/init.d/php-fpm restart 网页安装配置 登陆http://yourIP/dokuwiki/install.php 然后可以选择右上角的语言，之后就可以注册超级用户使用了。安装配置之后记得删除网站目录下的install.php文件 前面如果不注意会出现很多问题，比如，提示/data/pages无法写入的就是权限修改问题这个用上面的chmod命令可以解决 还会就是网页配置的时候有个bug，如果直接http://yourIP/install.php不安装反倒下载这个文件的时候，修改nginx配置文件将这个url修改为我上面写的那个url即可，其实就是换个路径就好了，`另外`需先确认nginx+php是正常的。 ok，以上，附我的wiki地址:http://wiki.arvon.top","categories":[],"tags":[{"name":"知识管理","slug":"知识管理","permalink":"http://arvon.top/tags/知识管理/"}]},{"title":"Ubuntu14.04命令行连接无线网络","slug":"Ubuntu14-04命令行连接无线网络","date":"2016-08-17T04:34:25.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/08/17/Ubuntu14-04命令行连接无线网络/","link":"","permalink":"http://arvon.top/2016/08/17/Ubuntu14-04命令行连接无线网络/","excerpt":"摘要：自己有个性能一般的ThinkPad本子，装了Ubuntu14.04，不过最近感觉触摸板很不好用，就打算使用命令行直接使用不用图形界面了，也更省资源。命令行启动后遇见个问题，我不会用命令行直接连接无线网络，真是尴尬，这里记录一下解决过程。","text":"摘要：自己有个性能一般的ThinkPad本子，装了Ubuntu14.04，不过最近感觉触摸板很不好用，就打算使用命令行直接使用不用图形界面了，也更省资源。命令行启动后遇见个问题，我不会用命令行直接连接无线网络，真是尴尬，这里记录一下解决过程。 设置开机启动命令行 我这里使用的是方法一 方法一 修改grup文件 1234#sudo vim /etc/default/grub#将以下注释行末尾添加text#GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\"GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash text\" 刷新grub 1sudo update-grup 方法二 编写配置文件 1234567891011121314#sudo chmod +w /boot/grub/grub.cfg#sudo vim /boot/grub/grub.cfg#复制以下内容 menuentry 'Ubuntu，Linux 3.2.0-24-generic-pae' --class ubuntu --class gnu-linux --class gnu --class os &#123; recordfail gfxmode $linux_gfx_mode insmod gzio insmod part_msdos insmod ext2 set root='(hd0,msdos8)' search --no-floppy --fs-uuid --set=root 689a61a1-06fd-4ffe-95d7-8671e97bbe81 linux /boot/vmlinuz-3.2.0-24-generic-pae root=UUID=689a61a1-06fd-4ffe-95d7-8671e97bbe81 ro quiet splash $vt_handoff initrd /boot/initrd.img-3.2.0-24-generic-pae&#125; 粘贴复制的内容并修改 123456789101112#参照如下menuentry 'Ubuntu，Linux 3.2.0-24-generic-pae(command mode)' --class ubuntu --class gnu-linux --class gnu --class os &#123; recordfail gfxmode $linux_gfx_mode insmod gzio insmod part_msdos insmod ext2 set root='(hd0,msdos8)' search --no-floppy --fs-uuid --set=root 689a61a1-06fd-4ffe-95d7-8671e97bbe81 linux /boot/vmlinuz-3.2.0-24-generic-pae root=UUID=689a61a1-06fd-4ffe-95d7-8671e97bbe81 ro text initrd /boot/initrd.img-3.2.0-24-generic-pae&#125; 关闭grub隐藏菜单 123#vim /etc/default/grubGRUB_HIDDEN_TIMEOUT=7#修改以上的数值原值为0修改为非0即可 刷新Grub 1sudo update-grub 命令行连接无线网络可用的方法 查看哪个网卡支持无线网络 12sudo iwconfig#我的网卡名称是wlan0，可以看IEEE802.11bgn作为参照 确认网卡启动 12sudo ip link set wlan0 up#sudo ifconfig wlan0 up 扫描无线网络 12sudo iw dev wlan0 scan | less#sudo iwlist wlan0 scan | more 连接无线网络这里就分多种情况了（因为加密方式有3种，WEP、WPA、WPA2） 对于常用的WPA方式加密 12345678910#先生成要连接的无线网络的账号密码配置文件，借助wpa_passphrase命令sudo wpa_passphrase ESSID PWD &gt; ssidname.conf#ssidname.conf这个文件名字可以自己随便起，位置也可以随便放，ESSID为无线名称，PWD为ESSID这个无线的连接密码sudo wpa_supplicant -B -i wlan0 -Dwext -c ./ssidname.conf#连接该无线网络，此时还不能上网，需要获取IPsudo iwconfig wlan0#查看此时wlan0网卡的状态sudo dhclient wlan0#sudo dhcpcd wlan0#获取IP,现在就可以看是不是能上网了 对于WEB加密的无线 1sudo iw dev wlan0 connect [网络 SSID] key 0:[WEP 密钥] 对于没有密码的 1sudo iw dev wlan0 connect [网络 SSID] 遇见的问题 关于”opertion not possible due to RF-kill”问题描述：遇见这个问题是我已经用命令行连接上了无线，然后重启打算再熟悉一遍的时候，启动网卡就报这个错误。解决：12sudo rfkill block all#软件上关闭所有的(网卡、蓝牙、红外) 参考文档 点滴的blog Linux技术网 Linux开源中文社区 红黑联盟 Linux吧 以上，属于归纳整理","categories":[],"tags":[{"name":"其他OS","slug":"其他OS","permalink":"http://arvon.top/tags/其他OS/"}]},{"title":"DNS端口使用解释","slug":"DNS端口使用解释","date":"2016-08-16T04:59:31.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/08/16/DNS端口使用解释/","link":"","permalink":"http://arvon.top/2016/08/16/DNS端口使用解释/","excerpt":"前言：我们公司有个面试题很有意思，DNS的默认查询端口是什么？可能大家都很熟悉dns使用53端口，那究竟是使用udp协议还是tcp协议呢，恩，我也不清楚，所以就看了个科普贴！","text":"前言：我们公司有个面试题很有意思，DNS的默认查询端口是什么？可能大家都很熟悉dns使用53端口，那究竟是使用udp协议还是tcp协议呢，恩，我也不清楚，所以就看了个科普贴！ DNS端口有TCP53和UDP53两个端口DNS在进行区域传输的时候使用TCP协议，其他时候则使用UDP协议 使用TCP53端口的情况 区域传送时使用TCP，主要有一下两点考虑： 辅域名服务器会定时（一般时3小时）向主域名服务器进行查询以便了解数据是否有变动。如有变动，则会执行一次区域传送，进行数据同步。区域传送将使用TCP而不是UDP，因为数据同步传送的数据量比一 个请求和应答的数据量要多得多 TCP是一种可靠的连接，保证了数据的准确性。 使用UDP53端口的情况 域名解析时使用UDP协议 客户端向DNS服务器查询域名，一般返回的内容都不超过512字节，用UDP传输即可。不用经过TCP三次握手，这样DNS服务器负载更低，响应更快。虽然从理论上说，客户端也可以指定向DNS服务器查询的时候使用TCP，但事实上，很多DNS服务器进行配置的时候，仅支持UDP查询包。 使用TCP的情况非常罕见，神秘兮兮。其实当解析器发出一个request后，返回的response中的tc删节标志比特位被置1时，说明反馈报文因为超长而有删节。这是因为UDP的报文最大长度为512字节。解析器发现后，将使用TCP重发request，TCP允许报文长度超过512字节。既然TCP能将data stream分成多个segment，它就能用更多的segment来传送任意长度的数据。 为什么既使用UDP协议又使用TCP协议 TCP与UDP传送字节的长度限制：UDP报文的最大长度为512字节，而TCP则允许报文长度超过512字节。当DNS查询超过512字节时，协议的TC标志出现删除标志，这时则使用TCP发送。通常传统的UDP报文一般不会大于512字节。 TCP和UDP连接的简单区别说明 TCP是一种面向连接的协议，提供可靠的数据传输，一般服务质量要求比较高的情况，使用这个协议。UDP—用户数据报协议，是一种无连接的传输层协议，提供面向事务的简单不可靠信息传送服务。 TCP与UDP的区别 UDP和TCP协议的主要区别是两者在如何实现信息的可靠传递方面不同。TCP协议中包含了专门的传递保证机制，当数据接收方收到发送方传来的信息时，会自动向发送方发出确认消息；发送方只有在接收到该确认消息之后才继续传送其它信息，否则将一直等待直到收到确认信息为止。 与TCP不同，UDP协议并不提供数据传送的保证机制。如果在从发送方到接收方的传递过程中出现数据报的丢失，协议本身并不能做出任何检测或提示。因此，通常人们把UDP协议称为不可靠的传输协议。相对于TCP协议，UDP协议的另外一个不同之处在于如何接收突发性的多个数据报。不同于TCP，UDP并不能确保数据的发送和接收顺序。事实上，UDP协议的这种乱序性基本上很少出现，通常只会在网络非常拥挤的情况下才有可能发生。 既然UDP是一种不可靠的网络协议，那么还有什么使用价值或必要呢？其实不然，在有些情况下UDP协议可能会变得非常有用。因为UDP具有TCP所望尘莫及的速度优势。虽然TCP协议中植入了各种安全保障功能，但是在实际执行的过程中会占用大量的系统开销，无疑使速度受到严重的影响。反观UDP由于排除了信息可靠传递机制，将安全和排序等功能移交给上层应用来完成，极大降低了执行时间，使速度得到了保证。 DNS的规范规定了2种类型的DNS服务器，一个叫主DNS服务器，一个叫辅助DNS服务器。在一个区中主DNS服务器从自己本机的数据文件中读取该区的DNS数据信息，而辅助DNS服务器则从区的主DNS服务器中读取该区的DNS数据信息。当一个辅助DNS服务器启动时，它需要与主DNS服务器通信，并加载数据信息，这就叫做区传送（zone transfer）。 以上，致谢bbs版主小侠唐在飞,以上内容均由bbs内容整理而来","categories":[],"tags":[{"name":"网络协议","slug":"网络协议","permalink":"http://arvon.top/tags/网络协议/"}]},{"title":"hexo之Next主题设置","slug":"hexo之Next主题头像设置","date":"2016-08-15T08:21:22.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/08/15/hexo之Next主题头像设置/","link":"","permalink":"http://arvon.top/2016/08/15/hexo之Next主题头像设置/","excerpt":"摘要：使用Next主题搭建了博客，虽然说头像什么的都是浮云，但是没有还是略显不爽，但恕我愚昧只按照Next官方文档没弄出来，官方写的太简略了，记录一下较为详细的步骤。更新：打赏更新：站内搜索","text":"摘要：使用Next主题搭建了博客，虽然说头像什么的都是浮云，但是没有还是略显不爽，但恕我愚昧只按照Next官方文档没弄出来，官方写的太简略了，记录一下较为详细的步骤。更新：打赏更新：站内搜索 设置头像这个头像就是站内侧边栏上那个头像 第一步找个jpg图片 将图片放到站点配置目录images目录下（并没有规定非放在这） 12#我图片放的路径themes/next/source/images/suolong.jpg 修改主题配置文件 123#vim themes/next/_config.yml#在任意位置添加如下内容avatar: images/suolong.jpg 完成（hexo s测试） 设置网站logo也就是浏览器标签上的显示图标,这个图片是有要求的，可以使用在线工具生成。 图片位置 1themes/next/source/images/favicon.ico 修改站点配置文件 12#vim themes/next/_config.ymlfavicon: images/favicon.ico 完成（测试即可） 打赏功能如果url的话很简单，直接参照官网就可以,对于不使用外链，图片放本地的话需要注意下路径 图片位置 12themes/next/source/images/uploads/zhipay.jpgthemes/next/source/images/uploads/weipay.jpg 修改主题配置文件 123456#vim themes/next/_config.yml#使用外链的话很简单，如下，直接写外链地址就可以了#wechatpay: http://i2.piimg.com/517630/8ef419c517319dd1.jpg#alipay: http://i2.piimg.com/517630/cac5f15767848576.jpgalipay: /images/uploads/zhipay.jpgwechatpay: /images/uploads/weipay.jpg 好了，配置完成 站内搜索 安装sitemap 1npm install hexo-generator-search --save 修改站点配置文件 123search: path: sitemap.xml field: post 修改localsearch文件 1234567891011121314151617#vim themes/next/layout/_partials/search/localsearch.swig#修改如下：&lt;div class=\"popup\"&gt;&lt;span class=\"search-icon fa fa-search\"&gt;&lt;/span&gt;&lt;input type=\"text\" id=\"local-search-input\" placeholder=\"search my blog...\"&gt;&lt;div id=\"local-search-result\"&gt;&lt;/div&gt;&lt;span class=\"popup-btn-close\"&gt;close&lt;/span&gt;&lt;/div&gt;#原文件如下：&lt;script type=\"text/javascript\"&gt; var search_path = \"&lt;%= config.search.path %&gt;\"; if (search_path.length == 0) &#123; search_path = \"search.xml\"; &#125; var path = \"&lt;%= config.root %&gt;\" + search_path; searchFunc(path, 'local-search-input', 'local-search-result');&lt;/script&gt; 站内搜索完成 以上","categories":[],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://arvon.top/tags/Hexo/"}]},{"title":"8月读书计划","slug":"8月读书计划","date":"2016-08-12T16:38:24.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/08/13/8月读书计划/","link":"","permalink":"http://arvon.top/2016/08/13/8月读书计划/","excerpt":"前天入了三本图灵的图解系列丛书，要计划读一下，这里做个记录。 书单 图解网络硬件 图解tcp/ip 图解http","text":"前天入了三本图灵的图解系列丛书，要计划读一下，这里做个记录。 书单 图解网络硬件 图解tcp/ip 图解http 为什么读 抓包分析能力是我之前就想学习的技能，但是苦于对协议什么的理解的不够系统，没有完善的知识体系所以心有余而力不足，刚好看到这个系列介绍协议的书就想买了看看，看过之后就可以研读我另外两本关于网络分析的书了。 网络分析读书清单 wireshark网络分析的艺术 实用网络流量分析技术 关于blog读书和写blog这两个是应该坚持的，应该养成记录技术问题的习惯，以后尽量加大blog更新的频率，该做的实验还是要积极去实践和记录的 以上","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://arvon.top/tags/读书/"}]},{"title":"Git钩子在hexo中的应用","slug":"Git钩子在hexo中的应用","date":"2016-08-12T07:12:44.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/08/12/Git钩子在hexo中的应用/","link":"","permalink":"http://arvon.top/2016/08/12/Git钩子在hexo中的应用/","excerpt":"摘要：早该记录了，最开始我的blog是使用hexo+github搭建的使用jacman主题，也是因为对git不够了解，当时绑定域名是使用直接在github上写CNAME文件。感觉太麻烦了，就买了空间换了WordPress，wp也是用了要有快一年，然后感觉markdown支持的真是不好，而且迁移很不爽，然后就换了现在的。关于现在这套blog架构，依然使用hexo作为blog框架，我自己的电脑用作服务器，然后VPS作为web服务器提供发布，使用git hook进行更新blog。 git钩子简介 –摘自git-scmGit 能在特定的重要动作发生时触发自定义脚本。 有两组这样的钩子：客户端的和服务器端的。 客户端钩子由诸如提交和合并这样的操作所调用，而服务器端钩子作用于诸如接收被推送的提交这样的联网操作。 你可以随心所欲地运用这些钩子。","text":"摘要：早该记录了，最开始我的blog是使用hexo+github搭建的使用jacman主题，也是因为对git不够了解，当时绑定域名是使用直接在github上写CNAME文件。感觉太麻烦了，就买了空间换了WordPress，wp也是用了要有快一年，然后感觉markdown支持的真是不好，而且迁移很不爽，然后就换了现在的。关于现在这套blog架构，依然使用hexo作为blog框架，我自己的电脑用作服务器，然后VPS作为web服务器提供发布，使用git hook进行更新blog。 git钩子简介 –摘自git-scmGit 能在特定的重要动作发生时触发自定义脚本。 有两组这样的钩子：客户端的和服务器端的。 客户端钩子由诸如提交和合并这样的操作所调用，而服务器端钩子作用于诸如接收被推送的提交这样的联网操作。 你可以随心所欲地运用这些钩子。 git hook概述钩子都被存储在 Git 目录下的 hooks 子目录中。 也即绝大部分项目中的 .git/hooks 。 当你用 git init 初始化一个新版本库时，Git 默认会在这个目录中放置一些示例脚本。这些脚本除了本身可以被调用外，它们还透露了被触发时所传入的参数。 所有的示例都是 shell 脚本，其中一些还混杂了 Perl 代码，不过，任何正确命名的可执行脚本都可以正常使用 —— 你可以用 Ruby 或 Python，或其它语言编写它们。 这些示例的名字都是以 .sample 结尾，如果你想启用它们，得先移除这个后缀。把一个正确命名且可执行的文件放入 Git 目录下的 hooks 子目录中，即可激活该钩子脚本。 这样一来，它就能被 Git 调用。Git Hooks 就是一些触发特定事件的脚本。比如 commit、push、merge 等等，也区分本地 Hooks 和服务端 Hooks。 使用post-reveive当用户在本地仓库执行git-push命令时，服务器上远程仓库就会对应执行git-receive-pack命令，而git-receive-pack命令会调用pre-receive钩子。使用git部署时的流程：本地git-push –&gt; VPS上Git服务器更新并Hook –&gt;VPS上执行pre-receive脚本，同步到服务器本地仓库目录 git部署blog过程记录VPS安装git 安装初始化12345yum install git-core -yadduser git-blogcd /home/git-bloggit init --bare blog.git#使用--bare参数是创建一个裸仓库，没有工作区，这里不再裸仓库进行操作，只为共享而存在 创建信任 创建从我本机到VPS上git-blog用户的信任12ssh-copy-id git-blog@VPShost#就是把~/.ssh/id_rsa.pub中的内容添加到VPS主机上~/.ssh/authorized_keys文件中 本地配置git发布 Hexo主配置文件_config.yml如下1234567deploy: type: git message: Arvon repo: ssh://git-blog@blog.arvon.top:12345/home/arvon/blog.git #repo: git-blog@blog.arvon.top:/home/arvon/blog.git branch: master#注意的地方，搬来repo那是直接写主机地址就可以了但是，我这主机ssh端口不是默认端口所以就需要写成ssh的形式。 服务器配置git hook 编辑blog.git/hooks下新建post-receive文件，内容如下1234#!/bin/shgit --work-tree=/usr/nginx/html/myblog --git-dir=/home/git-blog/blog.git checkout -f#这个意思是发布的内容推送到/usr/nginx/html/myblog目录下#别忘了 chmod +x post-receive 发布 本地操作 12git new \"test\"git g -d 参考文档swiftyper的blogimys的blog 之前关于hexo的文章linux下hexo配置hexo简明使用Hexo和Github搭建Blog ok,以上，以后还会补充git的东西","categories":[],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://arvon.top/tags/Hexo/"},{"name":"版本控制","slug":"版本控制","permalink":"http://arvon.top/tags/版本控制/"}]},{"title":"Tmux源码安装","slug":"Tmux源码安装","date":"2016-08-12T04:26:51.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/08/12/Tmux源码安装/","link":"","permalink":"http://arvon.top/2016/08/12/Tmux源码安装/","excerpt":"简述：之前其实有写过一篇tmux的简单使用绍,但是不够详尽，而且当时也并没有出现几个问题，当yum不好使的时候，就要上源码了。","text":"简述：之前其实有写过一篇tmux的简单使用绍,但是不够详尽，而且当时也并没有出现几个问题，当yum不好使的时候，就要上源码了。 安装tmux依赖于ncurses-devel和libevent。所以要先安装ncurses-devel，然后libevent，然后tmux 关于libevent库 先装开发库 1yum install gcc kernel-devel make ncurses-devel 安装libevent库 12345678wget https://github.com/downloads/libevent/libevent/libevent-2.0.21-stable.tar.gztar xvf libevent-2.0.21-stable.tar.gzcd libevent-2.0.21-stable./configuremake &amp;&amp; make install#注意了，有个坑，这时libevent并咩有安装在默认路径，所以会报错的，因此需要做个软链或者直接cp一份到默认路径，我的环境是CentOS6.7—x64的，所以就放到/usr/lib64下面cp libevent-2.0.21-stable/.libs/libevent-2.0.so.5 /usr/lib64/#顺便贴一句报错--&gt;tmux: error while loading shared libraries: libevent-2.0.so.5: cannot open shared object file: No such file or directory 关于tmux源码现在装tmux就比较顺滑了 下载tmux源码，方式有多种（tmux、curl、git均可）123456789#这里我使用了2.0版本wget https://github.com/tmux/tmux/releases/download/2.0/tmux-2.0.tar.gz#curl -OL http://downloads.sourceforge.net/tmux/tmux-1.9a.tar.gz#git clone git://git.code.sf.net/p/tmux/tmux-code#根据下载源码具体情况安装tar xvf tmux-2.0.tar.gzcd tmux-2.0./configuremake &amp;&amp; make install 参考文档ok, 开始愉快的使用吧,简单使用命令参照我之前的那一篇就可以Tmux官方很好的普及文章本篇参考","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"}]},{"title":"sort小插曲","slug":"sort小插曲","date":"2016-08-11T08:10:05.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/08/11/sort小插曲/","link":"","permalink":"http://arvon.top/2016/08/11/sort小插曲/","excerpt":"Tips:遇见一个有趣的sort命令问题，很有意思，这里记录一下，顺便也再复习一下sort命令。","text":"Tips:遇见一个有趣的sort命令问题，很有意思，这里记录一下，顺便也再复习一下sort命令。 有意思的例子 要求：将以下文本_按字母排序，_和-之间的按数字升序，-之后的按数字降序排列,不能破坏原本每行的数据，只对上下顺序排列. 文本sort.txt如下123456789def_99-55def_99-11def_123-100abc_456-100abc_123-100def_123-10abc_456-10abc_123-1xzy_789-0 解答 脚本如下12345678910111213cat sort.txt |sed s/_/-/ |sort -t\"-\" -k1,1 -k2,2n -k3,3nr |sed s/-/_/#说明：先将分隔符替换为统一，然后以-为分割符对第一字段按基础排序，第二字段按数字排序，第三字段按数字降序排序#-k1,1这种表达方式是只对本域进行排序是最准确的表达,类似还有-k1.2,1.2表示仅对第一列第二个字符排序，1,1这种表达表示的是一个完整域，如果直接写-k1那就表示从1到最后一个域，这样表述的是不准确的#输出如下abc_123-100abc_123-1abc_456-100abc_456-10def_99-55def_99-11def_123-100def_123-10xzy_789-0 sort命令回顾 基础选项 常规选项(参考:http://man.linuxde.net/sort)12345678910111213#usage: sort [选项] [参数]-b：忽略每行前面开始出的空格字符；-c：检查文件是否已经按照顺序排序；-d：排序时，处理英文字母、数字及空格字符外，忽略其他的字符；-f：排序时，将小写字母视为大写字母；-i：排序时，除了040至176之间的ASCII字符外，忽略其他的字符；-m：将几个排序号的文件进行合并；-M：将前面3个字母依照月份的缩写进行排序；-n：依照数值的大小排序；-o&lt;输出文件&gt;：将排序后的结果存入制定的文件；-r：以相反的顺序来排序；-t&lt;分隔字符&gt;：指定排序时所用的栏位分隔字符；+&lt;起始栏位&gt;-&lt;结束栏位&gt;：以指定的栏位来排序，范围由起始栏位到结束栏位的前一栏位 关于古老的+-选定域的说明sort官方有如下说明(摘自孙愚的博客) On older systems, sort’ supports an obsolete origin-zero syntax+POS1 [-POS2]‘ for specifying sort keys. POSIX 1003.1-2001 (*note Standards conformance::) does not allow this; use `-k’ instead.原来，这种古老的表示方式已经被淘汰了，以后可以理直气壮的鄙视使用这种表示方法的脚本喽！（为了防止古老脚本的存在，在这再说一下这种表示方法，加号表示Start部分，减号表示End部分。最最重要的一点是，这种方式方法是从0开始计数的， 以前所说的第一个域，在此被表示为第0个域。以前的第2个字符，在此表示为第1个字符","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"},{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"}]},{"title":"ssh无交互批量认证","slug":"ssh无交互批量认证","date":"2016-08-08T08:38:38.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/08/08/ssh无交互批量认证/","link":"","permalink":"http://arvon.top/2016/08/08/ssh无交互批量认证/","excerpt":"Tips:在使用ansible部署服务器的时候，部署机需要先做好和其他服务器的免密码登陆，如果一台一台手动执行那就太low了，所以就需要脚本来批量完成这些繁琐的工作，下面是我写的脚本，主要使用了expect这个工具。","text":"Tips:在使用ansible部署服务器的时候，部署机需要先做好和其他服务器的免密码登陆，如果一台一台手动执行那就太low了，所以就需要脚本来批量完成这些繁琐的工作，下面是我写的脚本，主要使用了expect这个工具。 expect介绍 expect是用于提供自动交互的工具，它可以控制处理输入和输出流，然后提供自动填写数据等功能（主要就是替代原本需要人机交互需要做的事情） expect采用tcl(Tool Command Language)的脚本语言 脚本实例（自用脚本可随意参考）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#!/usr/bin/sh################################################################################Author: arvon#Email: yafeng2011@126.com#Blog: http://arvon.top/#Date: 2016/08/08#Filename: write_authorized_keys.sh#Revision: 1.0#License: GPL#Description: auto write authorized to other server#Notes:################################################################################varsusername='root'server_passwd='arvon2014'IP_list=\"172.17.18.61172.17.18.62172.17.18.63\"#functionsfunction Main()&#123;#install_expect_package#create_ras_pubwrite_authorized_file&#125;#install expect packagefunction install_expect_package()&#123;yum install -y expect expect-devel&#125;function create_rsa_pub()&#123;expect -c \"spawn ssh-keygen -t rsa expect &#123; \\\"*y/n*\\\" &#123;send \\\"y\\r\\\"; exp_continue&#125; \\\"*key*\\\" &#123;send \\\"\\r\\\"; exp_continue&#125; \\\"*passphrase*\\\" &#123;send \\\"\\r\\\"; exp_continue&#125; \\\"*again*\\\" &#123;send \\\"\\r\\\";&#125; &#125;\"&#125;function write_authorized_file()&#123;for each_ip in $&#123;IP_list&#125;;do expect -c \" spawn ssh-copy-id $&#123;username&#125;@$&#123;each_ip&#125; expect &#123; \\\"*yes/no*\\\" &#123;send \\\"yes\\r\\\"; exp_continue&#125; \\\"*password*\\\" &#123;send \\\"$&#123;server_passwd&#125;\\r\\\"; exp_continue&#125; \\\"*Password*\\\" &#123;send \\\"$&#123;server_passwd&#125;\\r\\\";&#125; &#125; \" done&#125;#MainMain 参考地址1. Expect官方文档2. python俱乐部3. Beckham008的blog","categories":[],"tags":[{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"},{"name":"Shell","slug":"Shell","permalink":"http://arvon.top/tags/Shell/"}]},{"title":"Cassandra简单使用记录","slug":"Cassandra简单使用记录","date":"2016-08-05T16:14:09.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/08/06/Cassandra简单使用记录/","link":"","permalink":"http://arvon.top/2016/08/06/Cassandra简单使用记录/","excerpt":"简述：在现在公司接触这个比较多，但是之前并不熟悉，所以需要简单记录下基础增删改查，以及简单介绍。我目前的理解就是这个nosql非常适合做集群，在吞吐量上应该远高于传统数据库。需要再以后工作学习中再深入了解，先简单记录一下吧。 简介Apache Cassandra最初由Facebook创建，集Google BigTable的数据模型与Amazon Dynamo的完全分布式的架构于一身,是一个大规模可伸缩的开源NoSQL数据库。Cassandra完美地支持管理大量的结构化，半结构化和非结构化数据，且能够跨多个数据中心和云。Cassandra提供了持续可用性，线性扩展，操作简单，跨多个服务器而没有单点故障。且提供了一个有力的动态数据模型，支持最大的灵活性和快速响应。","text":"简述：在现在公司接触这个比较多，但是之前并不熟悉，所以需要简单记录下基础增删改查，以及简单介绍。我目前的理解就是这个nosql非常适合做集群，在吞吐量上应该远高于传统数据库。需要再以后工作学习中再深入了解，先简单记录一下吧。 简介Apache Cassandra最初由Facebook创建，集Google BigTable的数据模型与Amazon Dynamo的完全分布式的架构于一身,是一个大规模可伸缩的开源NoSQL数据库。Cassandra完美地支持管理大量的结构化，半结构化和非结构化数据，且能够跨多个数据中心和云。Cassandra提供了持续可用性，线性扩展，操作简单，跨多个服务器而没有单点故障。且提供了一个有力的动态数据模型，支持最大的灵活性和快速响应。 cql交互命令（类似sql） 运行cqlsh 直接键入cqlsh命令即可，登陆后可使用help查看帮助1/data/apps/opt/cassandra/bin/cqlsh ssy-db1 创建keyspace（秘钥空间） 主要包括复制策略和durable_writes 123cqlsh&gt; CREATE KEYSPACE \"Test\"cqlsh&gt; WITH replication = &#123;'class':'SimpleStrategy', 'replication_factor' : 3&#125;;#创建名为Test的keyspace(默认创建时不区分大小写的，加上双引号就区分大小写了),采用simple存储策略，factor为存储策略的选项 复制策略介绍 123456org.apache.cassandra.locator.SimpleStrategy#SimpleStrategy针对是一个data center中的多个存储节点(node)的存储，strategy_options表示数据存储所有存储节点(node)的复本数量，选择node的规则是在data center中按照顺时针的方向进行选择；org.apache.cassandra.locator.NetworkTopologyStrategy#NetworkTopologyStrategy是针对多个data center的情况进行处理，这个是以防同一个data center中的所以节点同时出现问题，如掉电；org.apache.cassandra.locator.OldNetworkTopologyStrategy#OldNetworkT opologyStrategy，这个可能会很少用上了，对data center的个数及复本的数量支持的有限，有了NetworkTopologyStrategy就不需要OldNetworkTopologyStrategy了。 删除keyspace 1cqlsh&gt; drop keyspace \"Test\" 修改keyspace参数信息 12cqlsh&gt; ALTER KEYSPACE \"Test\" WITH REPLICATION = &#123; 'class' : 'SimpleStrategy', 'replication_factor' : 1 &#125;;#修改factor为1 进入keyspace 1cqlsh&gt; use \"keyspace_name\"; 查看 查看keyspace 12cqlsh&gt; SELECT * from system.schema_keyspaces;cqlsh&gt; describe keyspaces; 查看已有表 1sqlsh&gt; describe tables; 查看表结构 12sqlsh&gt; use \"keyspace_name\"sqlsh&gt; describe table \"table_name\";","categories":[],"tags":[{"name":"Nosql","slug":"Nosql","permalink":"http://arvon.top/tags/Nosql/"}]},{"title":"Zabbix添加自定义监控项","slug":"Zabbix添加自定义监控项","date":"2016-08-05T07:14:59.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/08/05/Zabbix添加自定义监控项/","link":"","permalink":"http://arvon.top/2016/08/05/Zabbix添加自定义监控项/","excerpt":"Intorduction: 最近一直有使用zabbix，根据需求需要自己手动监控一些服务，这时就需要自己动手自己写一些小脚本了，我这里写了一个比较简单的小脚本，主要用来监控monit监控的服务状态，听起来有些拗口，不过就是这样的。 这里尽可以详细的记录一下，哪里有不对欢迎指正","text":"Intorduction: 最近一直有使用zabbix，根据需求需要自己手动监控一些服务，这时就需要自己动手自己写一些小脚本了，我这里写了一个比较简单的小脚本，主要用来监控monit监控的服务状态，听起来有些拗口，不过就是这样的。 这里尽可以详细的记录一下，哪里有不对欢迎指正 部署思路 zabbix的server端我是直接写脚本安装的。先前blog有记录过。 zabbix的client端和监控脚本采用ansible直接推送和安装。（ansible的配置随后再记录） 配置调试自定义脚本 脚本如下 12345678910111213141516171819202122#!/usr/bin/python##########################################################################Author: arvon#Email: guoyf@easemob.com#Blog: arvon.top#Date: 16/07/06#Version: 1.0#Todo: collect the monit summary process status send to zabbix server##########################################################################import sysimport commandsmy_parameter = sys.argv[1]total_msg = commands.getoutput('monit summary')msg_list = total_msg.split('\\n')a = 0for each_line in msg_list: if my_parameter in each_line and 'Process' in each_line: if 'Running' in each_line: a = 1 else: a = 0print a 开启client端自定义脚本选项 12Include=/etc/zabbix/zabbix_agentd.d/ #在这个路径下可以放自己的配置文件UnsafeUserParameters=1 #就是这个参数1为打开，没有脚本的话打开这个会报错的 配置文件说明自定义在/etc/zabbix/zabbix_agentd.d/下的配置文件里面主要有两个关键因素 12UserParameter=process.msg.web #这个就是定义key名称的地方，但最好定义的规整一点，还有就是别和zabbix自带的key重复sudo python /etc/zabbix/scripts/get_monit_status.py \"'nginx'\" #这个是Value，可以是脚本也可以是个命令什么，这里返回的值就是zabbixServer得到的值 zabbix-get测试说明zabbix-get是zabbix的排错工具，在server上安装执行最好，-s是指定主机，-k是指定要获取哪个key对应的值，这里返回的是1，是我想用1和0在web上方便使用布尔值对服务状态做判断","categories":[],"tags":[{"name":"Zabbix","slug":"Zabbix","permalink":"http://arvon.top/tags/Zabbix/"}]},{"title":"Ansible根据excel自动生成inventory文件","slug":"Ansible根据excel自动生成inventory文件","date":"2016-08-04T16:00:00.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/08/05/Ansible根据excel自动生成inventory文件/","link":"","permalink":"http://arvon.top/2016/08/05/Ansible根据excel自动生成inventory文件/","excerpt":"Tip: 根据excel表格手动写入inventory文件是个费时费力还容易出错的事情，这种事情本就应该使用脚本完成，刚好最近需要，然后就查阅文档，本来是想用shell的，毕竟相比python我还是对shell比较熟悉，奈何python的xlrd模块实在太合适所以就用python写了脚本（需要先安装xlrd模块）。Update:现在不推荐使用xlrd模块了，原因是现在python3.0趋向于使用新模块openpyx，而且xlrd模块只是读取excel还需xlwd模块进行写入操作，不过现在这个模块已经支持不是很好了，我用pip没装上，所以现在使用openpyx模块，在我博客Jumpserver这篇文章里记录有这个模块的用法","text":"Tip: 根据excel表格手动写入inventory文件是个费时费力还容易出错的事情，这种事情本就应该使用脚本完成，刚好最近需要，然后就查阅文档，本来是想用shell的，毕竟相比python我还是对shell比较熟悉，奈何python的xlrd模块实在太合适所以就用python写了脚本（需要先安装xlrd模块）。Update:现在不推荐使用xlrd模块了，原因是现在python3.0趋向于使用新模块openpyx，而且xlrd模块只是读取excel还需xlwd模块进行写入操作，不过现在这个模块已经支持不是很好了，我用pip没装上，所以现在使用openpyx模块，在我博客Jumpserver这篇文章里记录有这个模块的用法 安装xlrd模块12pip install xlrd#想源码安装参照https://pypi.python.org/pypi/xlrd/ Python脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#!/usr/bin/python# -*- coding:utf-8 -*-################################################################################Author: arvon#Email: yafeng2011@126.com#Blog: http://arvon.top/#Date: 2016/08/04#Filename: excel_write_inventory.py#Revision: 1.1#License: GPL#Description: use xlrd module auto create vars and file for ansible#Notes:###############################################################################from xlrd import open_workbook#input varsexcel_file = 'test.xlsx'#input the excel file pathinventory_path = './'#input the inventory file dir pathops_dir_path = 'you/ansible/path'#input the ops-repo dir pathinventory_file = 'inventory.yml'#input the ansible's inventory filenameinventory_name = 'ssy'#define varsdata = open_workbook(excel_file)table = data.sheets()[0]#open the excel first sheetsnrows = table.nrows#nrows is the tables line numbers#functions#write title to inventory filedef write_title(): f = open(inventory_path + inventory_file, 'w') input_msg = \\'''\\hosts_ops_path: /data/ops-repo/inventory: name: ssy hosts:''' f.write('host_ops_path: ' + ops_dir_path + '\\n') f.write('inventory:' + '\\n') f.write(' name: ' + inventory_name + '\\n') f.write(' hosts:' + '\\n') f.close#write all hostsdef write_hosts(row_num,host_name,group_name,host_num = 1): for each_line_num in range(nrows): each_line = table.row_values(each_line_num) if each_line[row_num] == 1: f = open(inventory_path + inventory_file, 'a') f.write(' - name: '+ host_name + str(host_num) + '\\n') host_num = host_num + 1 f.write(' ip: ' + str(each_line[1]) + '\\n') f.write(' group: '+ group_name + '\\n') f.write(\"\\n\") f.close#use functionswrite_title()write_hosts(4,'db','dbserver')write_hosts(5,'redis','redis')write_hosts(6,'zk','zookeeper')write_hosts(7,'kafka','kafka')write_hosts(8,'mysql','mysql')write_hosts(9,'rest','rest')write_hosts(10,'thrift','thrift')write_hosts(11,'push','push')write_hosts(12,'db-ejabberd','ejabberd-db')write_hosts(13,'conn-ejabberd','ejabberd-conn')write_hosts(14,'nginx','nginx')write_hosts(15,'web','web')write_hosts(16,'turn','turn')write_hosts(17,'media','media')write_hosts(18,'coference','coference')","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://arvon.top/tags/Python/"}]},{"title":"zabbix3.0配置文档","slug":"zabbix3.0配置文档","date":"2016-07-29T16:00:00.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/07/30/zabbix3.0配置文档/","link":"","permalink":"http://arvon.top/2016/07/30/zabbix3.0配置文档/","excerpt":"zabbix安装配置记录","text":"zabbix安装配置记录 Command record Yum Repo Install 123#The system Version is CentOS7.2wget http://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.0-1.el7.noarch.rpmrpm -ivh zabbix-release-3.0-1.el6.noarch.rpm LAMP Environment Warning12345678910111213141516#iptablessystemctl stop iptables#selinuxsetenforce 0#install lamp packageyum install mysql mysql mysql-server php php-mysql httpd#Installed:# mysql-community-client.x86_64 0:5.6.30-2.el6 mysql-community-libs.x86_64 0:5.6.30-2.el6 mysql-community-server.x86_64 0:5.6.30-2.el6 nginx.x86_64 1:1.6.3-8.el7 php.x86_64 0:5.4.16-36.1.el7_2.1 php-mysql.x86_64 0:5.4.16-36.1.el7_2.1 /etc/init.d/mysqld startmysqladmin passwordmysql -uroot -padmin -e \"create database zabbix character set utf8;\"mysql -uroot -padmin -e \"grant all privileges on zabbix.* to zabbix@localhost identified by 'zabbix';\"mysql -uroot -padmin -e \"flush privileges;\"#mysql -h$zabbixIP -uzabbix -pzabbixsystemctl start httpdsystemctl status httpd Zabbix Server Zabbix Server Install 12345yum -y install zabbix-server-mysql zabbix-web-mysql zabbix-get#Installed:# zabbix-get.x86_64 0:3.0.3-1.el7 zabbix-server-mysql.x86_64 0:3.0.3-1.el7 zabbix-web-mysql.noarch 0:3.0.3-1.el7 cd /usr/share/doc/zabbix-server-mysql-3.0.3/zcat create.sql.gz | mysql -uroot -padmin zabbix Zabbix Modify config zabbix_server.conf 12345678910111213#cat /etc/zabbix/zabbix_server.conf | egrep -v \"^#|^$\"LogFile=/var/log/zabbix/zabbix_server.logLogFileSize=0PidFile=/var/run/zabbix/zabbix_server.pidDBHost=localhostDBName=zabbixDBUser=zabbixDBPassword=zabbixSNMPTrapperFile=/var/log/snmptrap/snmptrap.logTimeout=4AlertScriptsPath=/usr/lib/zabbix/alertscriptsExternalScripts=/usr/lib/zabbix/externalscriptsLogSlowQueries=3000 zabbix_config 12345678910#vim /etc/httpd/conf.d/zabbix.conf &lt;IfModule mod_php5.c&gt; php_value max_execution_time 300 php_value memory_limit 128M php_value post_max_size 16M php_value upload_max_filesize 2M php_value max_input_time 300 php_value always_populate_raw_post_data -1 php_value date.timezone Asia/Chongqing &lt;/IfModule&gt; Start Zabbix 12systemctl enable zabbix-serversystemctl start zabbix-server Zabbix Web Install 123#Use Web Access: http://$IPADDRESS/zabbix/setup.php#Mdodify Parameter Base： https://www.zabbix.com/documentation/3.0/manual/installation/install_from_packages#The web Default Name/Password is Admin/zabbix Zabbix Agent Install 1yum install zabbix zabbix-agent Modify config 1234567PidFile=/var/run/zabbix/zabbix_agentd.pidLogFile=/var/log/zabbix/zabbix_agentd.logLogFileSize=0Server=127.0.0.1,172.17.18.64 #Add the agent IPServerActive=172.17.18.64 #ModifyHostname=Zabbix serverInclude=/etc/zabbix/zabbix_agentd.d/","categories":[],"tags":[{"name":"Zabbix","slug":"Zabbix","permalink":"http://arvon.top/tags/Zabbix/"}]},{"title":"grep命令","slug":"grep命令","date":"2016-07-14T16:00:00.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/07/15/grep命令/","link":"","permalink":"http://arvon.top/2016/07/15/grep命令/","excerpt":"Tips：很有用的几个选项 常用 定向匹配&amp;忽略大小写12345678#-Bgrep -B 5 part filename #显示filename中匹配part的前5行#-Agrep -A 5 part filename #显示filename中匹配part的后5行#-Cgrep -C 5 part filename #显示filename中匹配part的前后5行#-igrep -i part filename #匹配filename中的part不区分大小写","text":"Tips：很有用的几个选项 常用 定向匹配&amp;忽略大小写12345678#-Bgrep -B 5 part filename #显示filename中匹配part的前5行#-Agrep -A 5 part filename #显示filename中匹配part的后5行#-Cgrep -C 5 part filename #显示filename中匹配part的前后5行#-igrep -i part filename #匹配filename中的part不区分大小写 反选匹配 12#-vegrep -v \"^#|^$\" filename #不显示filename中的空行和开头#的注释行，显示其余内容 计数匹配 1grep -c part filename #显示匹配到part的行数 显示文件名匹配 1234#-Hgrep part -H somefile #显示一群文件中含有part的文件名并显示匹配项相关#-lgrep part -l somefile #只显示含有匹配项的文件名 其他选项 123-s或--no-messages 不显示错误信息。-w或--word-regexp 只显示全字符合的列。-x或--line-regexp 只显示全列符合的列。 帮助 其他遗漏选项参考帮助文档 –help 参考鸟哥文档","categories":[],"tags":[{"name":"正则","slug":"正则","permalink":"http://arvon.top/tags/正则/"}]},{"title":"CentOS搭建rsync服务","slug":"CentOS搭建rsync服务","date":"2016-04-11T05:08:07.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/04/11/CentOS搭建rsync服务/","link":"","permalink":"http://arvon.top/2016/04/11/CentOS搭建rsync服务/","excerpt":"更新：本篇选择独立启动模式 选择rsync服务器启动方式 rsync服务器负载比较高，则使用独立启动模式 rsync服务器负责较低，使用xinetd运行方式 Tips：由于需要将线上大量数据迁移至本地，直接使用ftp或scp会很不方便，使用rsync可以有断点续传功能，而且搭建起来十分快捷方便。 环境 本地CentOS release 6.7 线上CentOS release 6.5 这里是将线上数据拉至本地，本地主机在局域网中 ，并没有单独的公网IP，故线上服务器做rsync的服务端，本地PC做rsync的客户端","text":"更新：本篇选择独立启动模式 选择rsync服务器启动方式 rsync服务器负载比较高，则使用独立启动模式 rsync服务器负责较低，使用xinetd运行方式 Tips：由于需要将线上大量数据迁移至本地，直接使用ftp或scp会很不方便，使用rsync可以有断点续传功能，而且搭建起来十分快捷方便。 环境 本地CentOS release 6.7 线上CentOS release 6.5 这里是将线上数据拉至本地，本地主机在局域网中 ，并没有单独的公网IP，故线上服务器做rsync的服务端，本地PC做rsync的客户端 服务端配置 Install packge 1yum install rsync -y Command 123456mkdir /etc/rsyncdtouch /etc/rsyncd/rsyncd.conf #主配置文件touch /etc/rsyncd/rsyncd.secrets #客户端连接时使用的用户和密码touch /etc/rsyncd/rsyncd.motd #服务端提示信息文件chmod 600 /etc/rsyncd/rsyncd.secretschown root:root /etc/rsyncd/rsyncd.secrets Main_config 1234567891011121314151617181920212223242526272829uid = rootgid = rootuse chroot = noread only = yes #只读，不让客户端上传文件到服务器address = 203.66.131.14 #服务端的IP，写错将无法启动进程port = 873#hosts allow = 192.168.1.0/24 172.16.0.0/255.255.0.0hosts allow = 0.0.0.0/0.0.0.0hosts deny = *max connections = 5pid file = /var/run/rsyncd.pidmotd file = /etc/rsyncd/rsyncd.motdsecrets file = /etc/rsyncd/rsyncd.secretslog file = /var/log/rsync.logtransfer logging = yeslog format = %t %a %m %f %bsyslog facility = local3timeout = 300[mytmp]path = /data/nfsd/backup/arvon_backuplist = yesignore errorsauth users = rsync #只有rsyncd.secrets文件中的zhang3用户可以同步此目录[townhome]path = /home/townlist = no #服务器上同步数据的目录在服务器模块上列出来ignore errors #忽略IO错误auth users = rsync #只有rsyncd.secrets文件中的town用户可以同步此目录comment = hitown #客户端连接时的提示 Password_config 12[hello@host]$ sudo cat /etc/rsyncd/rsyncd.secretsrsync:123456 Look_config 12345# vi /etc/rsyncd/rsyncd.motd################################# Go !!!#欢迎信息，并无卵用################################ Start_service 12/usr/bin/rsync --daemon --config=/etc/rsyncd/rsyncd.conf #启动服务echo '/usr/bin/rsync --daemon --config=/etc/rsyncd/rsyncd.conf' &gt;&gt; /etc/rc.local #设置开机启动 客户端配置 Install 1yum install rsync -y Config 123echo 123456 &gt; /etc/rsync.password #将rsync的密码写入本地#这个rsync.password文件可随意放，不过建议放在etc下chmod 600 /etc/rsync.password #重要，rsync会检查权限，如果不是600会报出错误 测试&amp;使用 列出文件 12rsync --list-only --password-file=/etc/rsync.password rsync@rsync_server_ip::mytmp#这个mytmp是体现在主配置 文件的，也就是/etc/rsyncd/rsyncd.conf 同步文件到本地的目录 123#rsync -auvzP --delete --password-file=/etc/rsync.password rsync@rsync_server_ip::mytmp /your/want/path#由于这里只是需要将线上数据拉到本地，并不需要完全一致，所以可以不用--delete参数，能不用则不用，删除总归是危险操作rsync -auvzP --password-file=/etc/rsync.password rsync@rsync_server_ip::mytmp /your/want/path 参数说明 1234567891011121314-a 相当于-rlptgoD-r是递归-l是链接文件，意思是拷贝链接文件；-p表示保持文件原有权限；-t保持文件原有时间；-g保持文件原有用户组；-o 保持文件原有属主；-D 相当于块设备文件；-u, --update 仅仅进行更新，也就是跳过所有已经存在于DST，并且文件时间晚于要备份的文件。(不覆盖更新的文件)-z 传输时压缩；-P 传输进度；-v 传输时的进度等信息，和-P有点关系，自己试试。--delete 表示客户端的数据要与服务器端完全一致，如果客户端目录里有服务器上不存在的文件，则删除。#执行rsync同步时，慎用-z参数！一旦启用了-z参数，则数据在传输前会先经过压缩，如果文件比较大的话，压缩会很慢！进而出现，明明网速很快，却同步缓慢的情况。 排错 连接不上直接都不出欢迎 界面，查看防火墙，发现问题解决: 12sudo iptables -nvL --line-numbersudo iptables -I INPUT 17 -p tcp -m tcp --dport 873 -j ACCEPT -m comment --comment \"added by arvonfor rsync\" 连接上，有报错权限问题，反正我的是，来了个狠的，已ok，报错如下@ERROR: auth failed on module xxxrsync error: error starting client-server protocol (code 5) at main.c(1530) [sender=3.0.6]解决: 1chmod 600 -R /etc/rsyncd/ 以上","categories":[],"tags":[{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"}]},{"title":"4月读书计划","slug":"4月读书计划","date":"2016-04-03T02:05:16.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/04/03/4月读书计划/","link":"","permalink":"http://arvon.top/2016/04/03/4月读书计划/","excerpt":"PS：学习其实是一件蛮幸福的事情，学的越多感觉不会的越多，不过心里很踏实，最近学习ansibel和git。 关于最近的学习方向 ansibel 最近购买了4本图书，买了就不能白买不是，而且也是机遇不错，现阶段也在使用ansibel，配置管理和版本控制对于运维确实是挺重要的，【奔跑吧ansible】预计花费4周左右时间来学习。 由于最近需要使用AWS，所需需要研究下ec2模块 需要进行学习变量和遍历循环","text":"PS：学习其实是一件蛮幸福的事情，学的越多感觉不会的越多，不过心里很踏实，最近学习ansibel和git。 关于最近的学习方向 ansibel 最近购买了4本图书，买了就不能白买不是，而且也是机遇不错，现阶段也在使用ansibel，配置管理和版本控制对于运维确实是挺重要的，【奔跑吧ansible】预计花费4周左右时间来学习。 由于最近需要使用AWS，所需需要研究下ec2模块 需要进行学习变量和遍历循环 git 这个是版本控制，以前只是用hexo和git做过一段时间bolg，充其量只能算是了解，需要系统的学习一下，购买了蝙蝠书，这个估计会有些晦涩，进度可能会有问题，预计也是4周，根据实际情况再调整。 需要了解gitlab搭建及配置 wireshark 这个网络分析工具，现阶段只能说是知道有这么个东西，做运维的网络分析也是必备技能之一，所以买了本比较肤浅的书籍，【wireshark分析的艺术】，预计学习时间也是一个月。 tcp协议学习 wireshark表达式学习，及tcpdump学习使用 关于技术博客 持之以恒老是想着自己买个云主机，其实现阶段踏踏实实把技术学了，把博客写了就可以了，买主机最起码现在还是不必要的，该把精力主要投入到学习中去，保持博客的更新频率 想法和思路逐渐开始学习思路整理分析，不仅仅记录技术点和写脚本命令，逐步提升博客的质量","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://arvon.top/tags/读书/"}]},{"title":"CentOS搭建支持ftp的cdn服务","slug":"CentOS搭建支持ftp的cdn服务","date":"2016-03-25T04:38:41.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/03/25/CentOS搭建支持ftp的cdn服务/","link":"","permalink":"http://arvon.top/2016/03/25/CentOS搭建支持ftp的cdn服务/","excerpt":"摘要：由于测试需要，需用nginx搭建cdn服务器，并要求可以使用ftp上传。送个之前nginx搭建cdn的–&gt;飞机票,下面是主要vsftp的配置，nginx配置cdn参照飞机票那篇即可。","text":"摘要：由于测试需要，需用nginx搭建cdn服务器，并要求可以使用ftp上传。送个之前nginx搭建cdn的–&gt;飞机票,下面是主要vsftp的配置，nginx配置cdn参照飞机票那篇即可。 搭建nginx cdn服务器 具体参照使用nginx作cdn源站 nginx默认不开启文件目录显示，需要自行手动修改开启,在nginx.conf文件(/usr/local/nginx/conf/)里的http{}里面加入如下代码： 如显示不正常需删除多余index页面12345678autoindex on;#PS:另外两个参数最好也加上去autoindex_exact_size off;#默认为on，显示出文件的确切大小，单位是bytes。#改为off后，显示出文件的大概大小，单位是kB或者MB或者GBautoindex_localtime on;#默认为off，显示的文件时间为GMT时间。#改为on后，显示的文件时间为文件的服务器时间 搭建ftp服务器（来源：参照Johnny整理的步骤整合） 安装软件 1yum install vsftpd -y 修改配置文件 12345678910111213141516171819#[root@stag-gw-cnd conf.d]# cat /etc/vsftpd/vsftpd.conf | egrep -v \"^$|^#\"anonymous_enable=NOlocal_enable=YESwrite_enable=YESlocal_umask=022dirmessage_enable=YESxferlog_enable=YESconnect_from_port_20=YESxferlog_std_format=YESlisten=YESchroot_local_user=YESlocal_root=/usr/share/nginx/cdn/Testpasv_promiscuous=YESpasv_enable=YESpasv_max_port=10100pasv_min_port=10090pam_service_name=vsftpduserlist_enable=YEStcp_wrappers=YES 防火墙配置 1234vim /etc/sysconfig/iptablesiptables -I INPUT -p tcp -m state --state NEW -m tcp --dport 21 -j ACCEPTiptables -I INPUT -p tcp --destination-port 10090:10100 -j ACCEPT/etc/init.d/iptables save 服务配置 12service vsftpd restartiptables -nvL 用户配置 12345adduser ftptestpasswd ftptestusermod -s /sbin/nologin ftptestusermod -d /usr/share/nginx/cdn/Test ftptestchown dcxjftp:ftp -R /usr/share/nginx/cdn/Test 安全配置 123vim /etc/pam.d/vsftpd#注释下面这行#auth required pam_shells.so 以上","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"}]},{"title":"Nginx搭建CDN源站记录","slug":"Nginx搭建CDN源站","date":"2016-03-03T04:32:05.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/03/03/Nginx搭建CDN源站/","link":"","permalink":"http://arvon.top/2016/03/03/Nginx搭建CDN源站/","excerpt":"摘要：nginx有很多很强大的功能，可以做web，可以做代理、可以做SLB、还可以做缓存CDN等等，这里记录以下做缓存时的配置。","text":"摘要：nginx有很多很强大的功能，可以做web，可以做代理、可以做SLB、还可以做缓存CDN等等，这里记录以下做缓存时的配置。 Install package yum.repo 1wget http://mirrors.opencas.cn/epel/6/i386/epel-release-6-8.noarch.rpm Install 1yum install nginx-1.0.15-12.el6.x86_64 Service config 配置：example one创建cdn源站其端口为12345在/etc/nginx/conf.d这个目录下添加一个cdn.conf的文件 12345678910111213141516171819202122232425262728293031323334353637383940414243# The default serverserver &#123; listen 12345 default_server; server_name _; #charset koi8-r; #access_log logs/host.access.log main; # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; location / &#123; root /usr/share/nginx/cdn; index index.html index.htm; &#125; error_page 404 /404.html; location = /404.html &#123; root /usr/share/nginx/cdn; &#125; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/cdn; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht &#123; # deny all; #&#125;&#125; start service 1/etc/init.d/nginx start check 在/usr/share/nginx/cdn/下放个文件，for example：test 在浏览器键入：http://service_ip:12345/test ok,以上","categories":[],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://arvon.top/tags/Nginx/"}]},{"title":"python2.6升级2.7记录","slug":"python2-6升级2-7记录","date":"2016-02-12T04:08:36.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/02/12/python2-6升级2-7记录/","link":"","permalink":"http://arvon.top/2016/02/12/python2-6升级2-7记录/","excerpt":"Tips:由于想自己用Django搭建blog，需要2.7的python环境而centos6.7默认是2.6版本，所以升级了python","text":"Tips:由于想自己用Django搭建blog，需要2.7的python环境而centos6.7默认是2.6版本，所以升级了python 下载Python-2.7.3 1wget http://python.org/ftp/python/2.7.3/Python-2.7.3.tar.bz2 解压安装 1234567tar -jxvf Python-2.7.3.tar.bz2 cd Python-2.7.3./configuremake all make installmake cleanmake distclean 建立软连接，使系统默认的 python指向 python2.7 12345python -V /usr/local/bin/python2.7 -Vmv /usr/bin/python /usr/bin/python2.6.6 ln -s /usr/local/bin/python2.7 /usr/bin/python python -V 解决系统 Python 软链接指向 Python2.7 版本后，因为yum是不兼容 Python 2.7的，所以yum不能正常工作，我们需要指定 yum 的Python版本 12345#vi /usr/bin/yum #将文件头部的#!/usr/bin/python#改成#!/usr/bin/python2.6.6 以上","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://arvon.top/tags/Python/"}]},{"title":"selinux和iptables关闭","slug":"selinux和iptables关闭","date":"2016-02-01T11:08:28.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/02/01/selinux和iptables关闭/","link":"","permalink":"http://arvon.top/2016/02/01/selinux和iptables关闭/","excerpt":"iptablesCentos6.x 临时关闭 12service iptables stop/etc/init.d/iptabels stop 永久关闭 12chkconfig iptables offchkconfig iptables on #开启","text":"iptablesCentos6.x 临时关闭 12service iptables stop/etc/init.d/iptabels stop 永久关闭 12chkconfig iptables offchkconfig iptables on #开启 Centos7.x Centos7默认使用firewall 1234systemctl stop firewalld.service#停止firewallsystemctl disable firewalld.service#禁止firewall开机启动 使用iptables管理 12345678yum -y install iptables-services#vi /etc/sysconfig/iptables#修改iptables这个文件可以写入删除规则#-A INPUT -m state --state NEW -m tcp -p tcp --dport 3306 -j ACCEPTsystemctl restart iptables.service#重启防火墙使配置生效systemctl enable iptables.service#设置防火墙开机启动 selinux 查看状态 1getenforce 临时关闭 1setenforce 0 永久关闭 1#修改/etc/selinux/config文件中设置SELINUX=disabled ，然后重启服务器。 以上","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"},{"name":"网络安全","slug":"网络安全","permalink":"http://arvon.top/tags/网络安全/"}]},{"title":"CentOS搭建NTP服务器","slug":"centos搭建NTP服务器","date":"2016-01-20T04:15:50.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/01/20/centos搭建NTP服务器/","link":"","permalink":"http://arvon.top/2016/01/20/centos搭建NTP服务器/","excerpt":"简介：NTP（NetworkTime Protocol，网络时间协议）是用来在分布式时间服务器和客户端之间进行时间同步。–&gt;另一篇关于时间的 NTP基于UDP报文进行传输，使用的UDP端口号为123。使用NTP的目的是对网络内所有具有时钟的设备进行时钟同步，使网络内所有设备的时钟保持一致，从而使设备能够提供基于统一时间的多种应用。对于时间错误会导致服务器宕机，所以运行NTP的本地系统，既可以接受来自其他时钟源的同步，又可以作为时钟源同步其他的时钟，并且可以和其他设备互相同步。","text":"简介：NTP（NetworkTime Protocol，网络时间协议）是用来在分布式时间服务器和客户端之间进行时间同步。–&gt;另一篇关于时间的 NTP基于UDP报文进行传输，使用的UDP端口号为123。使用NTP的目的是对网络内所有具有时钟的设备进行时钟同步，使网络内所有设备的时钟保持一致，从而使设备能够提供基于统一时间的多种应用。对于时间错误会导致服务器宕机，所以运行NTP的本地系统，既可以接受来自其他时钟源的同步，又可以作为时钟源同步其他的时钟，并且可以和其他设备互相同步。 NTP服务器搭建Tips: hadoop对局域网时间同步要求非常高，所以适合搭建使用ntp服务器 主要包括两个文件 /etc/ntp/ntpserver.conf /etc/ntp.conf 服务器端（转自搭建ntp服务器） 1.搭建环境环境：CentOS6.4Server192.168.126.1Client192.168.126.2 2.安装NTP 1yum –y install ntp 3.修改配置文件添加一行 12#vim /etc/ntp.confrestrict 192.168.126.0255.255.255.0 nomodify notrap 4.找到以下两行去掉注释 123#vi /etc/ntp.confserver127.127.1.0fudge127.127.1.0stratum 10 5.开机启动服务 1chkconfig --level 345 ntpon 6.启动服务 1/etc/init.d/ntpd start 防火墙需要开123端口或者关闭防火墙1-A INPUT -m state --state NEW -m tcp -p tcp --dport 123 -jACCEPT 客户端 1.与服务端同步时间执行以下命令手工执行或用crontab来执行 1ntpdate 192.168.126.1 使用crontab –e 10 21 * * * ntpdate 192.168.126.1 &gt;&gt; /root/ntpdate.log 2&gt;&amp;1 2.查看同步状况 1ntpq –p 3.防火墙需要开123端口或者关闭防火墙 1-A INPUT -m state --state NEW -m tcp -p tcp --dport 123 -jACCEPT 手动脚本设置同步 通过crontab设置时间同步12/usr/sbin/ntpdate ntp.sjtu.edu.cn &gt;&gt; /var/log/ntp.log 2&gt;&amp;1; /sbin/hwclock –w#写入crontab，根据业务需要定制时间同步的频率","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"}]},{"title":"CentOS6.x挂载ntfs和xfs问题","slug":"CentOS6-x挂载ntfs和xfs问题","date":"2016-01-13T03:30:24.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2016/01/13/CentOS6-x挂载ntfs和xfs问题/","link":"","permalink":"http://arvon.top/2016/01/13/CentOS6-x挂载ntfs和xfs问题/","excerpt":"摘要： centos6.x默认是ext4的文件系统，而centos7.x是默认xfs的文件系统，所以直接挂载是会出问题的，另外ntfs是windows上的文件系统，挂载也会出问题，这里记录一下如何解决。 挂载xfs 安装支持包123wget http://mirrors.opencas.cn/epel/6/i386/epel-release-6-8.noarch.rpmyum install xfsprogs kmod-xfs xfsdump xfsprogs-develmount -t xfs /dev/sdc1 /data3/sdc1/","text":"摘要： centos6.x默认是ext4的文件系统，而centos7.x是默认xfs的文件系统，所以直接挂载是会出问题的，另外ntfs是windows上的文件系统，挂载也会出问题，这里记录一下如何解决。 挂载xfs 安装支持包123wget http://mirrors.opencas.cn/epel/6/i386/epel-release-6-8.noarch.rpmyum install xfsprogs kmod-xfs xfsdump xfsprogs-develmount -t xfs /dev/sdc1 /data3/sdc1/ 报错解决1234567891011121314151617181920212223242526#需要安装 lvm2: yum install lvm2#然后按一下步骤：#1、查看物理卷：pvsPV VG Fmt Attr PSize PFree /dev/sda2 VolGroup00 lvm2 a- 279.22G 32.00M#2、查看卷组：vgsVG #PV #LV #SN Attr VSize VFree VolGroup00 1 4 0 wz--n- 279.22G 32.00M#3、查看逻辑卷：lvdisplay--- Logical volume --- LV Name /dev/VolGroup00/LogVol03 VG Name VolGroup00 LV UUID YhG8Fu-ZGPk-qt8D-AxgC-DzOU-dg1F-z71feI LV Write Access read/write LV Status unenable # open 1 LV Size 245.97 GB Current LE 7871 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253:2#4、如未激活，需要激活逻辑卷：vgchange -ay /dev/VolGroup00LV Status available#5、挂载逻辑卷：mount /dev/VolGroup00/LogVol03 /home/lvm 挂载ntfs 安装支持包 12345wget http://pkgs.repoforge.org/rpmforge-release/rpmforge-release-0.5.3-1.el6.rf.x86_64.rpmchmod 755 rpmforge-release-0.5.3-1.el6.rf.x86_64.rpmrpm -ivh rpmforge-release-0.5.3-1.el6.rf.x86_64.rpmyum install fuse-ntfs-3g -ymount /dev/sdd1 /data3/sdd/ 补充此时挂载了ntfs也只是可以实现读取并不能写入，目前没有发现什么有效方法","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"}]},{"title":"Linux主机时间不准问题","slug":"Linux主机时间不准问题","date":"2015-12-13T03:20:56.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/12/13/Linux主机时间不准问题/","link":"","permalink":"http://arvon.top/2015/12/13/Linux主机时间不准问题/","excerpt":"摘要：linux主机时间过一段时间就会发现时间不对了，查阅资料这个大概是因为硬件时间和服务器时间不一致导致的。","text":"摘要：linux主机时间过一段时间就会发现时间不对了，查阅资料这个大概是因为硬件时间和服务器时间不一致导致的。 查看分析 查看服务器时间，即系统时间 1date 线上修改时区（不重启） 1tzselect #使用此命令，然后根据提示更改即可 查看服务器硬件时间 1hwclock --show 同步服务器硬件时间和系统时间 12hwclock --hctosys //把硬件时间同步到系统时间hwclock --systohc //系统时间同步到硬件时间 查看当前使用的时区 123456cat /etc/sysconfig/clock# The timezone of the system is defined by the contents of /etc/localtime.ZONE=\"America/New_York\"#UTC=true#ARC=false#这个时区采用的是美国时间，所以即时进行了时间同步，也是同步成美国时区的时间，而不是中国时区的时间。 时间同步操作命令 12/usr/sbin/ntpdate cn.pool.ntp.org/usr/sbin/ntpdate ntp.api.bz 小知识 常用时区介绍 CST：中国标准时间（China Standard Time），这个解释可能是针对RedHat Linux。UTC：协调世界时，又称世界标准时间，简称UTC，从英文国际时间/法文协调时间”Universal Time/Temps Cordonné”而来。中国大陆、香港、澳门、台湾 、蒙古国、新加坡、马来西亚、菲律宾、澳洲西部的时间与UTC的时差均为+8，也就是UTC+8。GMT：格林尼治标准时间（英语：Greenwich Mean Time，GMT）是指位于英国伦敦郊区的皇家格林尼治天文台的标准时间，因为本初子午线被定义在通过那里 的经线。 以上","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"}]},{"title":"linux解压rar及xshell的rz","slug":"linux解压rar及xshell的rz","date":"2015-12-05T03:53:05.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/12/05/linux解压rar及xshell的rz/","link":"","permalink":"http://arvon.top/2015/12/05/linux解压rar及xshell的rz/","excerpt":"摘要：有时会遇到要在linux下解压rar包，这就很不舒服，毕竟rar是win下不开源的东西，所以建议还是少用，另外rz这个小工具在使用xshell想上传个小东西的时候非常方便，大文件还是建议ftp或者弄个rsync。","text":"摘要：有时会遇到要在linux下解压rar包，这就很不舒服，毕竟rar是win下不开源的东西，所以建议还是少用，另外rz这个小工具在使用xshell想上传个小东西的时候非常方便，大文件还是建议ftp或者弄个rsync。 解压rar需要下载rar软件，yum源中并没有提供。原因为rar并不属于开源软件 12345678#downloadwget http://www.rarlab.com/rar/rarlinux-x64-5.3.0.tar.gz#installtar zxvf rarlinux-x64-5.3.0.tar.gzcp rar/rar /bin/cp rar/unrar /bin/#usage:rar e name.rar #会解压至当前文件夹 使用lrzsz需要直接向xshell拉小文件 12yum search rzyum install lrzsz.x86_64 0:0.12.20-27.1.el6 使用unzip 12yum install unzip -yunzip xxx.zip","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"}]},{"title":"cp和chown的几个好用选项","slug":"cp和chown的几个好用选项","date":"2015-11-14T02:09:54.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/11/14/cp和chown的几个好用选项/","link":"","permalink":"http://arvon.top/2015/11/14/cp和chown的几个好用选项/","excerpt":"摘要：生产环境中对权限什么的要求是很严格的，而使用root用户进行操作，难免会变更权限信息，修改权限就变得十分有必要，而以下的选线都是很实用的。","text":"摘要：生产环境中对权限什么的要求是很严格的，而使用root用户进行操作，难免会变更权限信息，修改权限就变得十分有必要，而以下的选线都是很实用的。 关于copy的几个选项123456-P#-p 或 --preserve 保留源文件或目录的属性，包括所有者、所属组、权限与时间-R#-R 或 --recursive 递归处理，将指定目录下的文件及子目录一并处理-a# -a 或 --archive 此参数的效果和同时指定\"-dpR\"参数相同 关于chown和chmod1234--reference=dir_file#修改文件权限与dir_file一致chmod --reference=xxx yyychown --reference=xxx yyy 补充copy命令123456789101112-a：此参数的效果和同时指定\"-dpR\"参数相同；-d：当复制符号连接时，把目标文件或目录也建立为符号连接，并指向与源文件或目录连接的原始文件或目录；-f：强行复制文件或目录，不论目标文件或目录是否已存在；-i：覆盖既有文件之前先询问用户；-l：对源文件建立硬连接，而非复制文件；-p：保留源文件或目录的属性；-R/r：递归处理，将指定目录下的所有文件与子目录一并处理；-s：对源文件建立符号连接，而非复制文件；-u：使用这项参数后只会在源文件的更改时间较目标文件更新时或是名称相互对应的目标文件并不存在时，才复制文件；-S：在备份文件时，用指定的后缀“SUFFIX”代替文件的默认后缀；-b：覆盖已存在的文件目标前将目标文件备份；-v：详细显示命令执行的操作。 chown命令1234567891011#chown -R arvon /usr/arvon-c或——changes：效果类似“-v”参数，但仅回报更改的部分；-f或--quite或——silent：不显示错误信息；-h或--no-dereference：只对符号连接的文件作修改，而不更改其他任何相关文件；-R或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理；-v或——version：显示指令执行过程；--dereference：效果和“-h”参数相同；--help：在线帮助；--reference=&lt;参考文件或目录&gt;：把指定文件或目录的拥有者与所属群组全部设成和参考文件或目录的拥有者与所属群组相同；--version：显示版本信息。 chmod命令12345678910111213141516171819#权限指定说明u User，即文件或目录的拥有者；g Group，即文件或目录的所属群组；o Other，除了文件或目录拥有者或所属群组之外，其他用户皆属于这个范围；a All，即全部的用户，包含拥有者，所属群组以及其他用户；r 读取权限，数字代号为“4”; w 写入权限，数字代号为“2”；x 执行或切换权限，数字代号为“1”；- 不具任何权限，数字代号为“0”；s 特殊功能说明：变更文件或目录的权限。#选项说明-c或——changes：效果类似“-v”参数，但仅回报更改的部分；-f或--quiet或——silent：不显示错误信息；-R或——recursive：递归处理，将指令目录下的所有文件及子目录一并处理；-v或——verbose：显示指令执行过程；--reference=&lt;参考文件或目录&gt;：把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同；&lt;权限范围&gt;+&lt;权限设置&gt;：开启权限范围的文件或目录的该选项权限设置；&lt;权限范围&gt;-&lt;权限设置&gt;：关闭权限范围的文件或目录的该选项权限设置；&lt;权限范围&gt;=&lt;权限设置&gt;：指定权限范围的文件或目录的该选项权限设置；#chmod u=rwx,g=rw,o=r hi.txt 参考地址Linux命令大全Hi doc儒峰的网站 以上","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"}]},{"title":"NFS服务安装部署记录","slug":"NFS服务安装部署记录","date":"2015-10-19T17:00:10.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/10/20/NFS服务安装部署记录/","link":"","permalink":"http://arvon.top/2015/10/20/NFS服务安装部署记录/","excerpt":"简述：其实在部署多台服务器时搭建一个nfs服务器是个共享资源非常好的方案，可以实现脚本共享，也可以实现数据集中备份，是个非常好用的网络文件系统。实验环境为多台服务器，其中一台用作NFSserver其余的主机作为client","text":"简述：其实在部署多台服务器时搭建一个nfs服务器是个共享资源非常好的方案，可以实现脚本共享，也可以实现数据集中备份，是个非常好用的网络文件系统。实验环境为多台服务器，其中一台用作NFSserver其余的主机作为client 安装yum源 安装yum源12wget http://mirrors.opencas.cn/epel/6/i386/epel-release-6-8.noarch.rpm#package: nfs-utils.x86_64 Server端配置 安装包权限 123456yum install nfs-utils.x86_64chkconfig rpcbind onchkconfig nfs onmkdir /data/nfschown -R nfsnobody:nfsnobody /data/nfs#为防止客户端的读取写入可以顺利进行，将共享目录权限设置为nfsnobody Server端修改配置文件 12345#vim /etc/exports/data/nfsd/deploy 192.168.1.2/32(rw,root_squash,all_squash)/data/nfsd/deploy 192.168.1.4/32(rw,root_squash,all_squash)/data/nfsd/deploy 192.168.1.5/32(rw,root_squash,all_squash)#指定哪些机器可以访问共享目录deploy，以及以何种方式访问 启动测试 1234service rpcbind startservice nfs startrpcinfo -p #確認nfs服務啟動成功exportfs #检查 NFS 服务器是否输出我们想共享的目录 Client端配置 客户端很简单123456yum install nfs-utils.x86_64chkconfig rpcbind onservice rpcbind startshowmount -e 192.168.1.3 #showmount -e nfs服務器的IP,检查 NFS 服务器端是否有目录共享#mount -t nfs4 192.168.1.3:/data/nfsd/deploy /mnt/deploy #默认使用的是udp协议,会存在丢数据问题mount -t nfs4 192.168.1.3:/data/nfsd/deploy /mnt/deploy -o proto=tcp -o nolock #使用tcp协议，速度慢 自动挂载 使用fstab实现启动挂载 1234#vim /etc/fstab#添加如下字段192.168.1.3:/data/nfsd/deploy /mnt/deploy nfs defaults 0 0192.168.1.3:/data/nfsd/backup /mnt/backup nfs defaults 0 0 好用命令 1234mount -a#这个-a参数会读取/etc/fstab文件的内容，对没有挂载的会再次瓜子啊umount -l mountmoint#强制解除挂载 以上","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"}]},{"title":"Ansible之HAVE_DECL_MPZ_POWM_SEC报错记录","slug":"Ansible之HAVE-DECL-MPZ-POWM-SEC报错记录","date":"2015-10-12T09:00:27.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/10/12/Ansible之HAVE-DECL-MPZ-POWM-SEC报错记录/","link":"","permalink":"http://arvon.top/2015/10/12/Ansible之HAVE-DECL-MPZ-POWM-SEC报错记录/","excerpt":"简述：在centOS6.5上使用pip安装ansible(version=2.1.1.0)时报错 报错解决 解决如下分析为PyCrypto模块安装有问题，所以就直接卸载重装。。有能力的话我会进一步探究这个报错，不过现在以下方法可以解决，先这样吧123pip uninstall python-keyczar pycrypto pyasn1yum erase python-keyczar python-crypto pyasn1yum install python-keyczar","text":"简述：在centOS6.5上使用pip安装ansible(version=2.1.1.0)时报错 报错解决 解决如下分析为PyCrypto模块安装有问题，所以就直接卸载重装。。有能力的话我会进一步探究这个报错，不过现在以下方法可以解决，先这样吧123pip uninstall python-keyczar pycrypto pyasn1yum erase python-keyczar python-crypto pyasn1yum install python-keyczar 报错如下 ERROR! Unexpected Exception: ‘module’ object has no attribute ‘HAVE_DECL_MPZ_POWM_SEC’the full traceback was:Traceback (most recent call last):File “/usr/bin/ansible”, line 81, in from ansible.cli.adhoc import AdHocCLI as mycliFile “/usr/lib/python2.6/site-packages/ansible/cli/adhoc.py”, line 28, in from ansible.executor.task_queue_manager import TaskQueueManagerFile “/usr/lib/python2.6/site-packages/ansible/executor/task_queue_manager.py”, line 29, in from ansible.executor.process.result import ResultProcessFile “/usr/lib/python2.6/site-packages/ansible/executor/process/result.py”, line 34, in from Crypto.Random import atforkFile “/usr/lib64/python2.6/site-packages/Crypto/Random/init.py”, line 29, in from Crypto.Random import _UserFriendlyRNGFile “/usr/lib64/python2.6/site-packages/Crypto/Random/_UserFriendlyRNG.py”, line 38, in from Crypto.Random.Fortuna import FortunaAccumulatorFile “/usr/lib64/python2.6/site-packages/Crypto/Random/Fortuna/FortunaAccumulator.py”, line 39, in import FortunaGeneratorFile “/usr/lib64/python2.6/site-packages/Crypto/Random/Fortuna/FortunaGenerator.py”, line 34, in from Crypto.Util.number import ceil_shift, exact_log2, exact_divFile “/usr/lib64/python2.6/site-packages/Crypto/Util/number.py”, line 56, in if _fastmath is not None and not _fastmath.HAVE_DECL_MPZ_POWM_SEC:AttributeError: ‘module’ object has no attribute ‘HAVE_DECL_MPZ_POWM_SEC’ 安装过程 命令脚本片段（摘自sam的脚本）123456789101112#安装PIPyum install python-pip.noarch python-devel libxslt-devel libffi-devel openssl-devel python-devel gcc-c++ gcc PyYAML python-babel python-crypto python-httplib2 python-jinja2 python-keyczar python-markupsafe python-paramiko python-pyasn1 python-simplejson python-six -ymkdir ~/.pip/echo \"[global]index-url = http://mirrors.aliyun.com/pypi/simple/[install]trusted-host=mirrors.aliyun.com\" &gt;&gt; ~/.pip/pip.conf#安装ansiblepip install --upgrade pip python-keyczarpip install ansible==2.1.1.0","categories":[],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"http://arvon.top/tags/Ansible/"}]},{"title":"crontab简记","slug":"crontab简记","date":"2015-09-16T09:01:58.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/09/16/crontab简记/","link":"","permalink":"http://arvon.top/2015/09/16/crontab简记/","excerpt":"今天刚好要备份线上的crontab，就顺便记下来，省得忘了以后还要去上网找。介绍:crontab命令常见于Unix和类Unix的操作系统之中，用于设置周期性被执行的指令。该命令从标准输入设备读取指令，并将其存放于“crontab”文件中，以供之后读取和执行。该词来源于希腊语 chronos(χρνο)，原意是时间。常，crontab储存的指令被守护进程激活， crond常常在后台运行，每一分钟检查是否有预定的作业需要执行。这类作业一般称为cron jobs。crontab文件包含送交cron守护进程的一系列作业和指令。每个用户可以拥有自己的crontab文件；同时，操作系统保存一个针对整个系统的crontab文件，该文件通常存放于/etc或者/etc之下的子目录中，而这个文件只能由系统管理员来修改。crontab文件的每一行均遵守特定的格式，由空格或tab分隔为数个领域，每个领域可以放置单一或多个数值。(ps:摘自百度百科)","text":"今天刚好要备份线上的crontab，就顺便记下来，省得忘了以后还要去上网找。介绍:crontab命令常见于Unix和类Unix的操作系统之中，用于设置周期性被执行的指令。该命令从标准输入设备读取指令，并将其存放于“crontab”文件中，以供之后读取和执行。该词来源于希腊语 chronos(χρνο)，原意是时间。常，crontab储存的指令被守护进程激活， crond常常在后台运行，每一分钟检查是否有预定的作业需要执行。这类作业一般称为cron jobs。crontab文件包含送交cron守护进程的一系列作业和指令。每个用户可以拥有自己的crontab文件；同时，操作系统保存一个针对整个系统的crontab文件，该文件通常存放于/etc或者/etc之下的子目录中，而这个文件只能由系统管理员来修改。crontab文件的每一行均遵守特定的格式，由空格或tab分隔为数个领域，每个领域可以放置单一或多个数值。(ps:摘自百度百科) 实例 常用命令12345678crontab -e #编辑，编辑后重启服务生效/etc/init.d/crond restart（stop/start） #重启服务crontab -l #查看crontab -l&gt;/home/backup/crontab/crontab\\`date '+%Y%m%d'`.bak #备份crontab#* 每分钟执行以下ls命令0 * * * * /bin/ls#* 周一到周五每天17点发email给arvon0 17 * * 1-5 mail -s \"hi\" alex@domain.name &lt; /tmp/maildata 命令介绍 参照man手册 f1 f2 f3 f4 f5 cmd/script 其中 f1 是表示分钟，f2 表示小时，f3 表示一个月份中的第几日，f4 表示月份，f5 表示一个星期中的第几天。program 表示要执行的程式。 当 f1 为 时表示每分钟都要执行 program，f2 为 时表示每小时都要执行程式，其余类推 当 f1 为 a-b 时表示从第 a 分钟到第 b 分钟这段时间内要执行，f2 为 a-b 时表示从第 a 到第 b 小时都要执行，其余类推 当 f1 为 /n 时表示每 n 分钟个时间间隔执行一次，f2 为 /n 表示每 n 小时个时间间隔执行一次，其余类推 当 f1 为 a, b, c,… 时表示第 a, b, c,… 分钟要执行，f2 为 a, b, c,… 时表示第 a, b, c…个小时要执行，其余类推 使用者也可以将所有的设定先存放在档案 file 中，用 crontab file 的方式来设定时程表。 由于unix版本不一样，所以部分语法有差别，例如在hp unix aix 中设定间隔执行如果采用*/n 方式将出现语法错误，在这类unix中 ，间隔执行只能以列举方式，详请见例子。 使用方法: 用VI编辑一个文件 cronfile，然后在这个文件中输入格式良好的时程表。编辑完成后，保存并退出。 在命令行输入crontab cronfile这样就将cronfile文件提交给cron进程，同时，新创建cronfile的一个副本已经被放在/var/spoll/cron目录中，文件名就是用户名。","categories":[],"tags":[{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"}]},{"title":"Zabbix使用Percona组件监控Mysql","slug":"Zabbix使用Percona组件监控Mysql","date":"2015-09-14T09:40:45.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/09/14/Zabbix使用Percona组件监控Mysql/","link":"","permalink":"http://arvon.top/2015/09/14/Zabbix使用Percona组件监控Mysql/","excerpt":"上周测试了使用percona Monitoring Plugins 监控mysql，确认可行。Percona Monitoring Plugins是一个高质量的组件，为mysql数据库添加企业级的监控和图表功能。该插件可以和Nagios或者是Cati等监控系统集成，从Percona1.1开始支持zabbix监控。其脚本由php实现，所以需安装php环境","text":"上周测试了使用percona Monitoring Plugins 监控mysql，确认可行。Percona Monitoring Plugins是一个高质量的组件，为mysql数据库添加企业级的监控和图表功能。该插件可以和Nagios或者是Cati等监控系统集成，从Percona1.1开始支持zabbix监控。其脚本由php实现，所以需安装php环境 实现环境 已经完成了zabbix基本环境部署 安装php环境 使用epel源，下载所需组件 下载Percona Monitoring Plugins12345678910111213#下载文件wget https://www.percona.com/downloads/percona-monitoring-plugins/1.1.1/percona-zabbix-templates-1.1.1-1.noarch.rpm#提取下载的rpm文件[root@localhost ~]# rpm2cpio percona-zabbix-templates-1.1.1-1.noarch.rpm |cpio -div&gt;目录文件说明#脚本文件路径var/lib/zabbix/percona/scripts/#key文件位置var/lib/zabbix/percona/templates/userparameter_percona_mysql.conf#模板文件位置var/lib/zabbix/percona/templates/zabbix_agent_template_percona_mysql_server_ht_2.0.9-sver1.1.1.xml# 安装percona monitoring plugins[root@localhost ~]# rpm -ivh percona-zabbix-templates-1.1.1-1.noarch.rpm 安装php环境1[root@localhost ~]# yum install php php-mysql 复制文件12345将key的子配置文件复制到/etc/zabbix/zabbix_agentd.d/``插曲``由于版本不同，所以实际文件位置也不同，这个位置可以在agent的包里看到rpm -ql zabbix22-agent-2.2.1-5.el6.x86_64/usr/share/doc/zabbix22-agent-2.2.1/userparameter_mysql.conf[root@localhost zabbix]# cp /var/lib/zabbix/percona/templates/userparameter_percona_mysql.conf /etc/zabbix/zabbix_agentd.d/ 重启zabbix-agent服务1/etc/init.d/zabbix-agent restart 修改脚本修改脚本中的用户名和密码,用户名和密码为本机数据库的账户和密码，并不需要特别的权限1[root@localhost ~]# vim /var/lib/zabbix/percona/scripts/ss_get_mysql_stats.php 没有用户添加数据库的用户12mysql&gt; grant process,super,select on *.* to zabbix@localhost identified by 'zabbix';mysql&gt; flush privileges; 脚本调试运行脚本，只要不报error就是没问题1[root@centos-01 percona]# /var/lib/zabbix/percona/scripts/get_mysql_stats_wrapper.sh gg","categories":[],"tags":[{"name":"Zabbix","slug":"Zabbix","permalink":"http://arvon.top/tags/Zabbix/"}]},{"title":"CentOS环境搭建zabbix2.2监控","slug":"CentOS环境搭建zabbix监控","date":"2015-09-09T02:48:06.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/09/09/CentOS环境搭建zabbix监控/","link":"","permalink":"http://arvon.top/2015/09/09/CentOS环境搭建zabbix监控/","excerpt":"##zabbix server更新：此版本为2.2版本，已更新3.0版本","text":"##zabbix server更新：此版本为2.2版本，已更新3.0版本 配置yum源12rpm -ivh http://repo.zabbix.com/zabbix/2.2/rhel/6/x86_64/zabbix-release-2.2-1.el6.noarch.rpmhttp://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm install zabbix server123yum install -y zabbix zabbix-get zabbix-server zabbix-web-mysql zabbix-web zabbix-agent #这个装出来是1.8的版本。所以重来#* 下面是2.2版本的安装包yum install -y zabbix22 zabbix22-web-mysql zabbix22-server zabbix22-agent zabbix22-web zabbix22-dbfiles-mysql zabbix22-server-mysql mysql config1yum install -y mysql-server 修改mysql配置文件123456789101112131415cp /etc/my.cnf /tmpegrep -v \"(^#|^$)\" #cat /etc/my.cnf | grep -v ^# | grep -v ^$#vi /etc/my.cnf#参照P27[Zabbix企业级分布式监控系统](http://item.jd.com/11522142.html?cu=true&amp;utm_source=baidu-search&amp;utm_medium=cpc&amp;utm_campaign=t_262767352_baidusearch&amp;utm_term=12777330128_0_0757319b492245849616c53bac31a23e)[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sockuser=mysql# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0character-set-server=utf8innodb_file_per_table=1[mysqld_safe]log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pid 启动mysql服务1234yum install mysql-server -yservice mysqld startps -ef | grep mysqlnetstat -nlput | grep 3306 mysql设置12345mysqladmin -uroot password adminmysql -uroot -padminmysql&gt; create database zabbix character set utf8;mysql&gt; grant all privileges on zabbix.* to zabbix@localhost identified by 'zabbix';mysql&gt; flush privileges; 创建zabbix的数据库 zabbix连接数据：zabbix/zabbix导入zabbix的数据库进入数据库导入mysql&gt;use zabbix###倒入sql一定按顺序一定 必须多说两句，这个地方卡了我整整一天，报错贴图在最后source /usr/share/zabbix-mysql/schema.sqlsource /usr/share/zabbix-mysql/images.sqlsource /usr/share/zabbix-mysql/data.sql在数据库外导入123mysql -uzabbix -pzabbix zabbix &lt; /usr/share/zabbix-mysql/schema.sqlmysql -uzabbix -p_zabbix_ zabbix &lt; /usr/share/zabbix-mysql/images.sqlmysql -uzabbix -p_zabbix_ zabbix &lt; /usr/share/zabbix-mysql/data.sql ###配置zabbix_server.conf 默认参数： egrep -v “(^#|^$)” /etc/zabbix/zabbix_server.conf 1234567LogFile=/var/log/zabbix/zabbix_server.logLogFileSize=0PidFile=/var/run/zabbix/zabbix.pidDBName=zabbixDBUser=zabbixDBSocket=/var/lib/mysql/mysql.sockAlertScriptsPath=/var/lib/zabbix/ 修改后参数： 主要添加字段为：DBpassword=zabbix iptables&amp;Selinux设置 iptables设置(注意顺序)1234567-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 10051 -j ACCEPT-A OUTPUT -m state --state NEW -m tcp -p tcp --dport 10050 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --sport 10050 -j ACCEPT#* Selinux关闭[root@localhost ~]# setenforce 0[root@localhost ~]# getenforce 启动zabbix服务&amp;http服务12sudo /etc/init.d/zabbix-server startsudo /etc/init.d/httpd start zabbix agentsofaware12rpm -ivh http://repo.zabbix.com/zabbix/2.2/rhel/6/x86_64/zabbix-release-2.2-1.el6.noarch.rpmyum install zabbix zabbix-agent configureiptables12-A INPUT -m state --state NEW -m tcp -p tcp --dport 10050 -j ACCEPT-A OUTPUT -m state --state NEW -m tcp -p tcp --dport 10051 -j ACCEPT zabbix_agentd.conf12345678[root@localhost ~]# egrep -v \"(^#|^$)\" /etc/zabbix/zabbix_agentd.confPidFile=/var/run/zabbix/zabbix_agentd.pidLogFile=/var/log/zabbix/zabbix_agentd.logLogFileSize=0Server=192.168.138.133 #server IPServerActive=192.168.138.133 #server IPHostname=Centos-01.hostnameInclude=/etc/zabbix/zabbix_agentd.d/ 启动1[root@localhost ~]# /etc/init.d/zabbix-agent restart zabbix添加主机手动添加 自动添加 ##报错解决 timezone问题php.ini配置文件设置123cp /etc/php.ini /tmp/sudo vim /etc/php.ini#添加字段：date.timezone = Asia/Shanghai web参数不满足要求问题12cp /etc/httpd/conf.d/zabbix.conf /tmp/sudo vim /etc/httpd/conf.d/zabbix.conf 修改其中字段为以下 Options FollowSymLinks AllowOverride None Order allow,deny Allow from all php_value date:timezone Asia/Shanghai php_value max_execution_time 300 php_value post_max_size 16M php_value max_input_time 300 php_value memory_limit 128M php_value upload_max_filesize 2M然后重启httpd服务 默认登陆账号密码账户：Admin密码：zabbix zabbix忘记密码，将旧密码修改为arvon登陆部署机数据库，修改数据库中的密码，步骤如下：1234567echo -n arvon | openssl md5(stdin)= 5c48aaaa95a1797ffb2dc32699b0c6c3&gt;mysql -uname -ppassword&gt;use zabbix;&gt;select * from users;#先在外面生成的md5码该用了&gt;update users set passwd='e2798af12a7a0f4f70b4d69efbc25f4d' where userid = '1'; 报错贴图“table not exist” epel源错误修改文件“/etc/yum.repos.d/epel.repo”， 第一项epel，将baseurl的注释取消， mirrorlist注释掉。即可 zabbixserver web 报错“Zabbix discoverer processes more than 75% busy” 参考 zabbix server is not running报错解决：仅限于我个人使用，打死都想不到密码写错了，zabbix_server.conf里面的密码加了两个单引号，真是作死呀，就是看不出来 参考文档：zabbix wike","categories":[],"tags":[{"name":"Zabbix","slug":"Zabbix","permalink":"http://arvon.top/tags/Zabbix/"}]},{"title":"mysql主从","slug":"mysql主从配置","date":"2015-09-07T07:38:14.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/09/07/mysql主从配置/","link":"","permalink":"http://arvon.top/2015/09/07/mysql主从配置/","excerpt":"这篇题目占了好长时间了，一直说补上也没补上，今天没事，就弄个实验环境来记录一下。正所谓好记性不如烂笔头，那就写一下。","text":"这篇题目占了好长时间了，一直说补上也没补上，今天没事，就弄个实验环境来记录一下。正所谓好记性不如烂笔头，那就写一下。 环境 两台服务器 123[root@localhost ~]# cat /etc/redhat-releaseCentOS release 6.7 (Final)#其中62做master，63做salve, 66为VIP mysql版本mysql版本说明：mysqlAB复制版本都要高于3.2，slave版本可以高于master但不可以低于master 12[root@localhost ~]# mysql -Vmysql Ver 14.14 Distrib 5.6.32, for Linux (x86_64) using EditLine wrapper Master配置 初始化mysql 12/etc/init.d/mysql startmysqladmin -uroot -parvon.top 配置文件 1234567891011121314#vim /etc/my.cfg[mysqld]server-id = 1#backup这台设置2,log-bin = arvon-mysql-bin#binlog文件名称binlog-ignore-db = mysql,information_schema#忽略写入binlog日志的库auto-increment-increment = 2#字段变化增量值auto-increment-offset = 1#初始字段ID为1slave-skip-errors = all#忽略所有复制产生的错误 添加slave读取master的用户和权限 123mysql&gt; grant replication slave on *.* to 'slave'@'172.17.18.63' identified by 'blog.arvon.top';mysql&gt; flush privileges;#给slave用户读取master的权限密码为blog.arvon.top Slave配置 测试salve用户登录是否正常 1mysql -h172.17.18.62 -uslave -pblog.arvon.top 配置文件先修改my.cnf文件，主要写入server-id,没有把后面server的主机端口密码信息直接写入配置文件是因为mysql5.5之后直接写会报错，mysql不认识，需要直接在mysql中用后面的命令指出。 1server-id=2 mysql中要指出的变量,后面MASTER_LOG_FILE和MASTER_LOG_POS就是上面server配置端binglog和post的值 12345678mysql&gt; CHANGE MASTER TO MASTER_HOST='172.17.18.62', -&gt; MASTER_PORT=3306, -&gt; MASTER_USER='slave', -&gt; MASTER_PASSWORD='blog.arvon.top', -&gt; MASTER_LOG_FILE='arvon-mysql-bin.000001', -&gt; MASTER_LOG_POS=120;#启动slave进程mysql&gt; slave start; 查看slave状态共有两个线程一个为IO线程，一个为SQL进程,如图都为yes说明成功 1mysql&gt; show slave status\\G; 测试一下在master创建一个database，然后去slave查看一下是否有了，我这反正测试时成功的，哈哈。","categories":[],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://arvon.top/tags/Mysql/"}]},{"title":"使用fio工具测试linux的磁盘性能","slug":"使用fio工具测试linux的磁盘性能","date":"2015-08-24T09:01:45.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/08/24/使用fio工具测试linux的磁盘性能/","link":"","permalink":"http://arvon.top/2015/08/24/使用fio工具测试linux的磁盘性能/","excerpt":"很多时候我们需要对服务器的性能进行评估，这就少不了运用合适的工具。其中fio是测试iops的一款很好用的工具，用来对硬件进行压力测试和验证，支持13中不同的I/O引擎，包括：sync、mmap、libaio、posixaio、SG v3、splice、null、network、syslet、guasi、solarisaio等等 简单测试写性能 使用dd简单测试12dd if=/dev/zero of=testfile bs=1M count=1024 oflag=dsync#采用dd创建一个1G的文件的磁盘写速度，IO引擎为dsync","text":"很多时候我们需要对服务器的性能进行评估，这就少不了运用合适的工具。其中fio是测试iops的一款很好用的工具，用来对硬件进行压力测试和验证，支持13中不同的I/O引擎，包括：sync、mmap、libaio、posixaio、SG v3、splice、null、network、syslet、guasi、solarisaio等等 简单测试写性能 使用dd简单测试12dd if=/dev/zero of=testfile bs=1M count=1024 oflag=dsync#采用dd创建一个1G的文件的磁盘写速度，IO引擎为dsync 准备&amp;安装 FIO官网 安装系统12345678910wget http://brick.kernel.dk/snaps/fio-2.0.7.tar.gz#download the softwareyum install libaio-devel#安装依赖包tar -xvf fio-2.0.7.tar.gz#decompressioncd fio-2.0.7#进入解压目录makemake install 使用说明 fio选项说明 filename=/dev/sdb1 测试文件名称，通常选择需要测试的盘的data目录。direct=1 测试过程绕过机器自带的buffer。使测试结果更真实。rw=randwrite 测试随机写的I/Orw=randrw 测试随机写和读的I/Obs=16k 单次io的块文件大小为16kbsrange=512-2048 同上，提定数据块的大小范围size=5g 本次的测试文件大小为5g，以每次4k的io进行测试。numjobs=30 本次的测试线程为30.runtime=1000 测试时间为1000秒，如果不写则一直将5g文件分4k每次写完为止。ioengine=psync io引擎使用pync方式rwmixwrite=30 在混合读写的模式下，写占30%group_reporting 关于显示结果的，汇总每个进程的信息。此外lockmem=1g 只使用1g内存进行测试。zero_buffers 用0初始化系统buffer。nrfiles=8 每个进程生成文件的数量。 例子12345678910#随机读fio -filename=/root/filename -direct=1 -iodepth 1 -thread -rw=randread -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=1000 -group_reporting -name=mytest#顺序读fio -filename=filename -direct=1 -iodepth 1 -thread -rw=read -ioengine=psync -bs=16k -size=2G -numjobs=30 -runtime=1000 -group_reporting -name=mytest#随机写fio -filename=filename -direct=1 -iodepth 1 -thread -rw=randwrite -ioengine=psync -bs=16k -size=2G -numjobs=30 -runtime=1000 -group_reporting -name=mytest#顺序写fio -filename=filename -direct=1 -iodepth 1 -thread -rw=write -ioengine=psync -bs=16k -size=2G -numjobs=30 -runtime=1000 -group_reporting -name=mytest#混合随机读写fio -filename=filename -direct=1 -iodepth 1 -thread -rw=randrw -rwmixread=70 -ioengine=psync -bs=16k -size=2G -numjobs=30 -runtime=100 -group_reporting -name=mytest -ioscheduler=noop","categories":[],"tags":[{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"}]},{"title":"yum源配置手册","slug":"yum源配置手册","date":"2015-08-21T07:07:07.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/08/21/yum源配置手册/","link":"","permalink":"http://arvon.top/2015/08/21/yum源配置手册/","excerpt":"yum（yellow dog updater， modified）是一个在Fedora和RedHat以及SUSE中的Shell前端软件包管理器，能够从指定的服务器自动下载RPM包并安装，可以自动处理依赖关系，并且一次安装所有的依赖关系软件包。系统管理过程中，难免要找软件装软件，如果定制性不高的话，使用yum安装不失为一个好办法，废话少说，开始介绍","text":"yum（yellow dog updater， modified）是一个在Fedora和RedHat以及SUSE中的Shell前端软件包管理器，能够从指定的服务器自动下载RPM包并安装，可以自动处理依赖关系，并且一次安装所有的依赖关系软件包。系统管理过程中，难免要找软件装软件，如果定制性不高的话，使用yum安装不失为一个好办法，废话少说，开始介绍 常用命令123456789101112131415161718192021#1. 升级系统yum updateyum -y update mysql #升级特定的软件#2. 安装指定的软件包yum -y install vsftpd#3. 卸载指定的软件包yum -y remore vsftpd mysql#4. 查看系统中已经安装的和可用的软件组，对于可用的软件组，你可以选择安装yum grouplist#5. 清除缓存中的rpm 头文件和包文件yum clean all#6. 搜索相关的软件包yum -y search Emacs#显示指定软件包的信息yum info Emacs#查询指定软件包的依赖信息，emacs依赖的模块不少啊yum deplist emacs#列出所有以 yum 开头的软件包yum list yum\\*#10. 列出已经安装的但是不包含在资源库中的rpm 包yum list extras Repo配置文件123456789101112131415161718192021#配置文件位于``/etc/yum.repos.d/*.repo``,扩展名都为Repo，要想不生效，可直接修改后缀，如改为*.reop_bak#配置文件解释arvon@Mo:~/arvon_work&gt; cat CentOS6-Base-163.repo | grep -v '^#'[base]#表示标识名称name=CentOS-$releasever - Base - 163.com#名字，其实可以随便起faliovermethod=priority#priority是默认值表示从列出的baseurl中顺序选择镜像服务器地址，roundrobin表示随机选择exclude=compiz**compiz*fusion-icon*#用来禁止这个仓库某些软件包的安装，可使用通配符，并以空格分隔。视情况添加baseurl=http://mirrors.163.com/centos/$releasever/os/$basearch/#引用的地址,http://网站 file://本地 ftp://文件服务器mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-$releasever$arch=$basearch#是指一个镜像服务器地址列表，通常是开启的，在浏览器打开据说可以看到可用的镜像服务器列表enabled=1#enabled=1为启用这个源，0为不启用这个源gpgcheck=1#表示这个repo中下载的rpm将惊醒gpg校验，以确定rpm包的来源是有效和安全的gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6#定义用于校验的gpg密匙 粘贴可用的epel的yum源123456789101112131415161718192021222324[epel]name=Extra Packages for Enterprise Linux 6 - $basearch#baseurl=http://download.fedoraproject.org/pub/epel/6/$basearchmirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-6&amp;arch=$basearchfailovermethod=priorityenabled=1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6[epel-debuginfo] name=Extra Packages for Enterprise Linux 6 - $basearch - Debug#baseurl=http://download.fedoraproject.org/pub/epel/6/$basearch/debugmirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-debug-6&amp;arch=$basearchfailovermethod=priorityenabled=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6gpgcheck=1 [epel-source]name=Extra Packages for Enterprise Linux 6 - $basearch - Source#baseurl=http://download.fedoraproject.org/pub/epel/6/SRPMSmirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-source-6&amp;arch=$basearchfailovermethod=priorityenabled=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6gpgcheck=1 常用的yum源站 EPEL源 123456#URLhttps://admin.fedoraproject.org/mirrormanager/#EPEL-REPO-RPMwget https://dl.fedoraproject.org/pub/epel/epel-release-latest-5.noarch.rpmwget https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpmaxel https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm ALI源 12345678910#URLhttp://mirrors.aliyun.com/repo/#ALI-REPOwget http://mirrors.aliyun.com/repo/Centos-6.repoaxel http://mirrors.aliyun.com/repo/Centos-7.repo#ALI-EPLE-REPOwget http://mirrors.aliyun.com/repo/epel-6.repoaxel http://mirrors.aliyun.com/repo/epel-7.repo#ALI-FEDORA-REPOwget http://mirrors.aliyun.com/repo/fedora.repo 可能错误 错误现象lock123456Loading \"installonlyn\" pluginExisting lock /var/run/yum.pid: another copy is running. Aborting.#解释这是因为yum在更新， 此时是kill不掉的该进程的， 正确的解决方法是让它停止更新即可。可以直接输入 rm -f /var/run/yum.pid也可以/etc/init.d/yum-updatesd stop","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"},{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"}]},{"title":"lsof命令","slug":"lsof命令","date":"2015-08-21T02:37:24.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/08/21/lsof命令/","link":"","permalink":"http://arvon.top/2015/08/21/lsof命令/","excerpt":"简介：lsof命令既（list open files）Lsof 是遵从Unix 哲学的典范，它只做一件事情，并且做的相当完美——它可以列出某个进程打开的所有文件信息。打开的文件可能是普通的文件，目录，NFS文件，块文件，字符文件，共享库，常规管道，明明管道，符号链接，Socket流，网络Socket，UNIX域Socket，以及其它更多。因为Unix系统中几乎所有东西都是文件，你可以想象lsof该有多有用。","text":"简介：lsof命令既（list open files）Lsof 是遵从Unix 哲学的典范，它只做一件事情，并且做的相当完美——它可以列出某个进程打开的所有文件信息。打开的文件可能是普通的文件，目录，NFS文件，块文件，字符文件，共享库，常规管道，明明管道，符号链接，Socket流，网络Socket，UNIX域Socket，以及其它更多。因为Unix系统中几乎所有东西都是文件，你可以想象lsof该有多有用。 安装 yum安装 1yum install lsof -y 源码安装 123456wget http://down1.chinaunix.net/distfiles/lsof_4.76.tar.gztar -zxvf lsof.tar.gzcd lsof_4.78tar xvf lsof_4.78_srv.tar./configure lunuxmake 例子 lsof不加参数的输出，是列出所有进程打开的所有文件 123456789arvon@Mo:~&gt; lsoflsof: WARNING: can't stat() fuse.gvfs-fuse-daemon file system /root/.gvfs Output information may be incomplete.COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME进程的名称 进程标识符 进程所有者 文件描述符 文件类型 指定磁盘的名称 文件的大小 索引节点 打开文件的确切名称init 1 root cwd unknown /proc/1/cwd (readlink: Permission denied)init 1 root rtd unknown /proc/1/root (readlink: Permission denied)init 1 root txt unknown /proc/1/exe (readlink: Permission denied)init 1 root NOFD /proc/1/fd (opendir: Permission denied) lsof /path/to/file /path/to/file2找出谁在使用某个文件,可一次指定多个文件 1234567arvon@Mo:~&gt; lsof /home/arvonlsof: WARNING: can't stat() fuse.gvfs-fuse-daemon file system /root/.gvfs Output information may be incomplete.COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEbash 24519 arvon cwd DIR 8,2 4096 463289 /home/arvonlsof 25011 arvon cwd DIR 8,2 4096 463289 /home/arvonlsof 25012 arvon cwd DIR 8,2 4096 463289 /home/arvon lsof +D /usr/lib加上+D参数，lsof会对指定目录进行递归查找，注意这个参数要比grep版本慢 1234567arvon@Mo:~&gt; lsof +D /usr/liblsof: WARNING: can't stat() fuse.gvfs-fuse-daemon file system /root/.gvfs Output information may be incomplete.COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEbash 24519 arvon mem REG 8,2 256324 445427 /usr/lib/locale/es_VE.utf8/LC_CTYPEbash 24519 arvon mem REG 8,2 54 445420 /usr/lib/locale/om_ET/LC_NUMERICbash 24519 arvon mem REG 8,2 2454 431855 /usr/lib/locale/en_US.utf8/LC_TIME 查看文件、设备被哪些进程占用 123456789# lsof /dev/tty1COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEbash 1770 jian 0u CHR 4,1 0t0 1045 /dev/tty1bash 1770 jian 1u CHR 4,1 0t0 1045 /dev/tty1bash 1770 jian 2u CHR 4,1 0t0 1045 /dev/tty1bash 1770 jian 255u CHR 4,1 0t0 1045 /dev/tty1startx 1845 jian 0u CHR 4,1 0t0 1045 /dev/tty1startx 1845 jian 1u CHR 4,1 0t0 1045 /dev/tty1... 监控文件系统：指定目录、挂载点，可以看到有哪些进程打开了其下的文件 1234# lsof /data/COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEbash 15983 jian cwd DIR 8,5 4096 8252 /data/backup... 列出被指定进程名打开的文件 123456789# lsof -c ssh -c initCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEinit 1 root txt REG 8,1 124704 917562 /sbin/initinit 1 root mem REG 8,1 1434180 1442625 /lib/i386-linux-gnu/libc-2.13.soinit 1 root mem REG 8,1 30684 1442694 /lib/i386-linux-gnu/librt-2.13.so...ssh-agent 1528 lakshmanan 1u CHR 1,3 0t0 4369 /dev/nullssh-agent 1528 lakshmanan 2u CHR 1,3 0t0 4369 /dev/nullssh-agent 1528 lakshmanan 3u unix 0xdf70e240 0t0 10464 /tmp/ssh-sUymKXxw1495/agent.1495 监控进程：指定进程号，可以查看该进程打开的文件： 12345678910# lsof -p 2064COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEfirefox 2064 jian cwd DIR 8,6 4096 1571780 /home/jianfirefox 2064 jian rtd DIR 8,6 4096 2 /firefox 2064 jian txt REG 8,6 44224 1985670 /usr/lib/firefox-12.0/firefoxfirefox 2064 jian mem REG 8,6 14707012 925361 /usr/share/fonts/chinese/msyhbd.ttffirefox 2064 jian mem REG 8,6 15067744 925362 /usr/share/fonts/chinese/msyh.ttffirefox 2064 jian mem REG 8,6 16791251 1701681 /usr/share/fonts/wenquanyi/wqy-zenhei.ttcfirefox 2064 jian mem REG 0,16 67108904 10203 /dev/shm/pulse-shm-3021850167... 当你想要杀掉某个用户所有打开的文件、设备，你可以这样： 1kill -9 `lsof -t -u lakshmanan` 监控网络 监控网络:查看指定端口有哪些进程在使用（lsof -i 列出所有的打开的网络连接）： 12345# lsof -i:22COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 1569 root 3u IPv4 10303 0t0 TCP *:ssh (LISTEN)sshd 1569 root 4u IPv6 10305 0t0 TCP *:ssh (LISTEN)... 监控网络：列出被某个进程打开所有的网络文件： 12lsof -i -a -p 234#lsof -i -a -c ssh 监控网络：列出所有 tcp、udp 连接： 12lsof -i tcp;lsof -i udp; 列出所有NFS文件 1lsof -N -u lakshmanan -a 查看指定网口有哪些进程在使用 123456# lsof -i@192.168.1.91COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEskype 1909 jian 54u IPv4 9116 0t0 TCP 192.168.1.91:40640-&gt;64.4.23.153:40047 (ESTABLISHED)pidgin 1973 jian 7u IPv4 6599 0t0 TCP 192.168.1.91:59311-&gt;hx-in-f125.1e100.net:https (ESTABLISHED)pidgin 1973 jian 13u IPv4 9260 0t0 TCP 192.168.1.91:54447-&gt;by2msg3010511.phx.gbl:msnp (ESTABLISHED)... 监控用户 查看指定用戶打开的文件（lsof -u ^lakshmanan 可以排除某用户）： 123456789# lsof -u messagebusCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEdbus-daem 1805 messagebus cwd DIR 8,6 4096 2 /dbus-daem 1805 messagebus rtd DIR 8,6 4096 2 /dbus-daem 1805 messagebus txt REG 8,6 1235361 1834948 /usr/bin/dbus-daemondbus-daem 1805 messagebus mem REG 8,6 210473 1700647 /lib/libnss_files-2.15.sodbus-daem 1805 messagebus mem REG 8,6 190145 1700642 /lib/libnss_nis-2.15.sodbus-daem 1805 messagebus mem REG 8,6 490366 1700636 /lib/libnsl-2.15.so... 查看指定程序打开的文件： 123456789# lsof -c firefoxCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEfirefox 2064 jian cwd DIR 8,6 4096 1571780 /home/jianfirefox 2064 jian rtd DIR 8,6 4096 2 /firefox 2064 jian txt REG 8,6 44224 1985670 /usr/lib/firefox-12.0/firefoxfirefox 2064 jian mem REG 8,6 14707012 925361 /usr/share/fonts/chinese/msyhbd.ttffirefox 2064 jian mem REG 8,6 15067744 925362 /usr/share/fonts/chinese/msyh.ttffirefox 2064 jian mem REG 8,6 16791251 1701681 /usr/share/fonts/wenquanyi/wqy-zenhei.ttc... 技巧 只有多个查询条件都满足， 用 “-a” 参数，默认是 -o 。 1234567# lsof -a -c bash -u rootCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEbash 1986 root cwd DIR 8,6 4096 1701593 /root/labbash 1986 root rtd DIR 8,6 4096 2 /bash 1986 root txt REG 8,6 1994157 1700632 /bin/bashbash 1986 root mem REG 8,6 9690800 405214 /usr/lib/locale/locale-archivebash 1986 root mem REG 8,6 210473 1700647 /lib/libnss_files-2.15.so 关于磁盘空间告警 df -h --max=1 与 du -hx --max=1 显示不一致的问题 最常见的的还是下面这种情况：lsof|grep -i delete看看被删除的文件：有些删了文件，但是进程没 reload，那些空间还是占用的，你可以理解为类似 windows 下的进程句柄没释放的概念吧~ 只是 windows 下如果有文件被进程使用，你一般是删不掉的，而 linux 虽然不做删除限制，但却要等到进程使用完文件才能完全释放，以防止进程奔溃， 这是操作系统对资源的管理差异吧~例如 nginx 会有很多临时文件占用了 /tmp 目录，删掉后，依然占用着空间，此时你可以：pkill -9 nginx &amp;&amp; /etc/init.d/nginx restart 参考使用lsof查找打开的文件实用系统工具lsof 以上","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"},{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"}]},{"title":"zabbix Too many open files错误","slug":"Zabbix_Too_many_open_files错误","date":"2015-08-21T02:10:18.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/08/21/Zabbix_Too_many_open_files错误/","link":"","permalink":"http://arvon.top/2015/08/21/Zabbix_Too_many_open_files错误/","excerpt":"问题描述：今天看了zabbix监控，又一个agent unreachable，然后惯例我要去重启agent，but并没有什么卵用。看报错，查log","text":"问题描述：今天看了zabbix监控，又一个agent unreachable，然后惯例我要去重启agent，but并没有什么卵用。看报错，查log 通过google有了一些眉目,以下摘自google linux系统出现Too many open files 错误，这是因为文件描述符大小不够，或者有不正常的网络连接(Socket也是一种特殊的文件)、文件IO没有关闭并释放出文件描述符（文件句柄，File Operator）。使用如下命令查看系统对允许打开最大文件描述符的配置：ulimit -u 查看open files设置ulimit -a 查看所有设置ulimit -u 65535(新的open files 值)修改设置ulimit -n 65536 设置用户可以同时打开的最大文件数（max open files） 如果本参数设置过小，对于并发访问量大的网站，可能会出现too many open files的错误 使用lsof -p pid [httpd进程的 pid、java的pid]来查看系统中apache进程和java运行时进程当前打开的文件资源，发现两者之和已经接近1024，大于了默认的设置。 修改配置： 修改/etc/security/limits.conf，在文件末加上 soft nofile 65536 hard nofile 65536 系统级文件描述符极限还可以通过将以下三行添加到 /etc/rc.d/rc.local 启动脚本中来设置： Increase system-wide file descriptor limit.echo 65536 &gt; /proc/sys/fs/file-maxecho 65536 &gt; /proc/sys/fs/inode-max","categories":[],"tags":[{"name":"Zabbix","slug":"Zabbix","permalink":"http://arvon.top/tags/Zabbix/"}]},{"title":"sed 命令实例","slug":"sed-命令实例","date":"2015-08-20T03:50:23.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/08/20/sed-命令实例/","link":"","permalink":"http://arvon.top/2015/08/20/sed-命令实例/","excerpt":"sed命令在很有目的性的针对文本进行操作的时候还是十分有优势的。sed 是一种在线编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有 改变，除非你使用重定向存储输出。Sed主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等。","text":"sed命令在很有目的性的针对文本进行操作的时候还是十分有优势的。sed 是一种在线编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有 改变，除非你使用重定向存储输出。Sed主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等。 sed使用参数 命令概览12345678910111213141516[root@www ~]# sed [-nefr] [动作]选项与参数：-n ：使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN 的数据一般都会被列出到终端上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。-e ：直接在命令列模式上进行 sed 的动作编辑；-f ：直接将 sed 的动作写在一个文件内， -f filename 则可以运行 filename 内的 sed 动作；-r ：sed 的动作支持的是延伸型正规表示法的语法。(默认是基础正规表示法语法)-i ：直接修改读取的文件内容，而不是输出到终端。动作说明： [n1[,n2]]functionn1, n2 ：不见得会存在，一般代表『选择进行动作的行数』，举例来说，如果我的动作是需要在 10 到 20 行之间进行的，则『 10,20[动作行为] 』function：a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚；i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)；p ：列印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～s ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！ 实例 将/etc/passwd内容列出，同事，将2-5行删除 123456789arvon@Mo:~&gt; nl /etc/passwd | sed '2, 5d' 1 at:x:25:25:Batch jobs daemon:/var/spool/atjobs:/bin/bash 6 gdm:x:107:112:Gnome Display Manager daemon:/var/lib/gdm:/bin/false 7 haldaemon:x:101:102:User for haldaemon:/var/run/hald:/bin/false 8 lp:x:4:7:Printing daemon:/var/spool/lpd:/bin/bash#只要删除第 2 行nl /etc/passwd | sed '2d'#删除3至最后一行nl /etc/passwd | sed '3,$d' 在第二行后(亦即是加在第三行)加上『drink tea?』字样！ 1234567arvon@Mo:~&gt; nl /etc/passwd | sed '2a drink tea' 1 at:x:25:25:Batch jobs daemon:/var/spool/atjobs:/bin/bash 2 bin:x:1:1:bin:/bin:/bin/bashdrink tea 3 daemon:x:2:2:Daemon:/sbin:/bin/bash#那如果是要在第二行前nl /etc/passwd | sed '2i drink tea' 如果是要增加两行以上，在第二行后面加入两行字，例如『Drink tea or …..』与『drink beer?』 12345678[root@www ~]# nl /etc/passwd | sed '2a Drink tea or ......\\&gt; drink beer ?'1 root:x:0:0:root:/root:/bin/bash2 bin:x:1:1:bin:/bin:/sbin/nologinDrink tea or ......drink beer ?3 daemon:x:2:2:daemon:/sbin:/sbin/nologin.....(后面省略)..... 将第2-5行的内容取代成为『No 2-5 number』呢？ 12345arvon@Mo:~&gt; nl /etc/passwd | sed '2,5c No 2-5 number' 1 at:x:25:25:Batch jobs daemon:/var/spool/atjobs:/bin/bashNo 2-5 number 6 gdm:x:107:112:Gnome Display Manager daemon:/var/lib/ ... 仅列出 /etc/passwd 文件内的第 5-7 行 1234arvon@Mo:~&gt; cat /etc/passwd | sed -n '5,7p'games:x:12:100:Games account:/var/games:/bin/bashgdm:x:107:112:Gnome Display Manager daemon:/var/lib/gdm:/bin/falsehaldaemon:x:101:102:User for haldaemon:/var/run/hald:/bin/false 搜索 /etc/passwd有root关键字的行 123456789101112arvon@Mo:~&gt; nl /etc/passwd | sed '/root/p' 1 at:x:25:25:Batch jobs daemon:/var/spool/atjobs:/bin/bash 2 bin:x:1:1:bin:/bin:/bin/bash 3 daemon:x:2:2:Daemon:/sbin:/bin/bash 4 ftp:x:40:49:FTP account:/srv/ftp:/bin/bash 5 games:x:12:100:Games account:/var/games:/bin/bash 6 gdm:x:107:112:Gnome Display Manager daemon:/var/lib/gdm:/bin/false 7 haldaemon:x:101:102:User for haldaemon:/var/run/hald:/bin/false 8 lp:x:4:7:Printing daemon:/var/spool/lpd:/bin/bash 9 mail:x:8:12:Mailer daemon:/var/spool/clientmqueue:/bin/false ...#如果root找到，除了输出所有行，还会输出匹配行。 使用-n的时候将只打印包含模板的行。 12arvon@Mo:~&gt; nl /etc/passwd | sed -n '/root/p' 19 root:x:0:0:root:/root:/bin/bash 删除/etc/passwd所有包含root的行，其他行输出 1234567arvon@Mo:~&gt; nl /etc/passwd | sed '/root/d' 1 at:x:25:25:Batch jobs daemon:/var/spool/atjobs:/bin/bash 2 bin:x:1:1:bin:/bin:/bin/bash 3 daemon:x:2:2:Daemon:/sbin:/bin/bash 4 ftp:x:40:49:FTP account:/srv/ftp:/bin/bash 5 games:x:12:100:Games account:/var/games:/bin/bash ... 搜索/etc/passwd,找到root对应的行，执行后面花括号中的一组命令，每个命令之间用分号分隔，这里把bash替换为blueshell，再输出这行： 12arvon@Mo:~&gt; nl /etc/passwd | sed -n '/root/&#123;s/bash/blueshell/;p&#125;' 19 root:x:0:0:root:/root:/bin/blueshell 如果只替换/etc/passwd的第一个bash关键字为blueshell，就退出 12arvon@Mo:~&gt; nl /etc/passwd | sed -n '/bash/&#123;s/bash/blueshell/;p;q&#125;' 1 at:x:25:25:Batch jobs daemon:/var/spool/atjobs:/bin/blueshell 获取主机ip 12arvon@Mo:~&gt; /sbin/ifconfig eth0 | grep 'inet addr' | sed 's/^.*addr://g' | sed 's/Bcast.*$//g'192.168.138.130 一条sed命令，删除/etc/passwd第三行到末尾的数据，并把bash替换为blueshell 123arvon@Mo:~&gt; nl /etc/passwd | sed -e '3,$d' -e 's/bash/blueshell/' 1 at:x:25:25:Batch jobs daemon:/var/spool/atjobs:/bin/blueshell 2 bin:x:1:1:bin:/bin:/bin/blueshell 直接修改文件内容 12#利用 sed 将 regular_express.txt 内每一行结尾若为 . 则换成 ![root@www ~]# sed -i 's/\\.$/\\!/g' regular_express.txt 利用 sed 直接在 regular_express.txt 最后一行加入『# This is a test』 1234567891011arvon@Mo:~/arvon_work&gt; sed -i '$a # THis is a test' helloarvon@Mo:~/arvon_work&gt; cat helloechoarvonisagoodnorandor# THis is a test 参考地址看的博客鸟哥私房菜","categories":[],"tags":[{"name":"正则","slug":"正则","permalink":"http://arvon.top/tags/正则/"}]},{"title":"Perl语言入门","slug":"Perl语言入门2","date":"2015-08-17T09:45:52.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/08/17/Perl语言入门2/","link":"","permalink":"http://arvon.top/2015/08/17/Perl语言入门2/","excerpt":"更新记录2015/08/17 拆开来记，start with foreach2015/09/01 更新到子程序，最近有点懒惰了","text":"更新记录2015/08/17 拆开来记，start with foreach2015/09/01 更新到子程序，最近有点懒惰了 ##结构 ###foreach控制结构 Example one12345#!/usr/bin/perluse 5.010;foreach $rock (qw/ my girlfriend is Mo /)&#123; print \"One word is named $rock\\n\";&#125; arvon@Mo:~/arvon_perl&gt; perl foreach_one.plOne word is named myOne word is named girlfriendOne word is named isOne word is named Mo Example two12345678#!/usr/bin/perluse 5.010;@rocks = qw/ hello world Mo /;foreach $rock(@rocks)&#123; $rock = \"\\t$rock\"; $rock .= \"\\n\"; print \"\\$rock now is $rock\";&#125; arvon@Mo:~/arvon_perl&gt; perl foreach_two.pl$rock now is hello$rock now is world$rock now is Mo ###Perl的默认变量$ Example one12345#!/usr/bin/perluse 5.010;foreach (1..10)&#123; #使用了默认变量$_ print \"I can count to $_!\\n\";&#125; arvon@Mo:~/arvon_perl&gt; perl variable_one.plI can count to 1!I can count to 2!I can count to 3!I can count to 4!I can count to 5!I can count to 6!I can count to 7!I can count to 8!I can count to 9!I can count to 10! ###reverse操作符 reverse操作符会读取列表的值，并按相反的次序返回该列表。1234567#!/usr/bin/perluse 5.010;@fred = 6..10;@barney = reverse(@fred);@wilma = reverse 6..10;@fred = reverse @fred;print \"@fred, @barney, @wilma, \\n\"; arvon@Mo:~/arvon_perl&gt; perl reverse_array.pl10 9 8 7 6, 10 9 8 7 6, 10 9 8 7 6, ###sort操作符 其实啊，就是排序12345678910111213#!/usr/bin/perluse 5.010;@rocks = qw/ happy birthday to Mo /;@sorted = sort(@rocks);@back = reverse sort @rocks;@rocks = sort @rocks;@numbers = sort 97..102;print \" @rocks \\ @sorted\\ @back\\ @rocks\\ @numbers\"; arvon@Mo:~/arvon_perl&gt; perl sort_string.pl Mo birthday happy to Mo birthday happy to to happy birthday Mo Mo birthday happy to 100 101 102 97 98 99 ###each操作符 每次对数组调用each，会返回数组中下一个元素所对应的两个值–该元素的索引以及该元素的值123456789#!/usr/bin/perluse 5.012;my @rocks = qw/ bedrock slate rubble granite /;while( my( $index, $value ) = each @rocks)&#123; say \"$index: $value\";&#125;foreach $index(0 .. $#rocks )&#123; print \"$index: $rocks[$index]\\n\";&#125; 子程序 子程序名称子程序名称以字母、数字、下划线组成，不能以数字开头，子程序名称属于独立的名字空间 定义子程序 定义子程序用sub、子程序名以及花括号封闭起来的代码块，for example：1234sub marine &#123; $n += 1; #全局变量$n print \"Hello, sailor number $n!\\n\";&#125; 调用子程序 在任意表达式中使用程序名（前面加上与号）来调用它123456789#!/usr/bin/perlsub marine&#123; $n += 1; print \"Hello, sailor number $n!\\n\";&#125;&amp;marine; #打印hello，sailor number 1！&amp;marine; #打印hello，sailor number 2！&amp;marine; #打印hello，sailor number 3！&amp;marine; #打印hello，sailor number 4！ 返回值 任何的perl子程序都有返回值，但不是所有的返回值都是有用的，Larry将之简化，在子程序的执行过程中，它会不断进行运算，而最后一次运算的结果（不管是什么）都会被自动当成子程序的返回值。123456789101112#!/usr/bin/perl$fred = 2;$barney = 3;$wilma = &amp;sum_of_fred_and_barney;sub sum_of_fred_and_barney&#123; print \"Hey, you called the sum_of_fred_and_barney subroutine!\\n\"; $fred + $barney;&#125;#&amp;sum_of_fred_and_barney;$betty = 3 * &amp;sum_of_fred_and_barney;print \"\\$wilma is $wilma.\\n\";print \"\\$betty is $betty.\\n\"; 1234567891011121314#!/usr/bin/perl$fred = 2;$barney = 3;sub larger_of_fred_or_barney&#123; if ($fred &gt; $barney)&#123; $fred; print \"\\$fred is $fred\\n\"; &#125; else&#123; $barney; print \"\\$barney is $barney\\n\"; &#125;&#125;&amp;larger_of_fred_or_barney; 参数 要传递参数列表到子程序里，只要在子程序调用的后面加上被括号圈引得列表表达式就可以了，for example：1$n = &amp;max(10, 15); 1234567891011#!/usr/bin/perlsub max&#123; if ($_[0] &gt; $_[1])&#123; $_[0]; print \"\\$_[0] is $_[0]\\n\"; &#125;else &#123; $_[1]; print \"\\$_[1] is $_[1]\\n\"; &#125;&#125;$n = &amp;max(3, 4); 子程序中的私有变量 默认情况下，perl里的所有变量都是全局变量，即在程序的任何地方都可以访问他们。随时可以借助my操作符来创建私有变量（lexical variable）123456#!/usr/bin/perlsub max &#123; my($m, $n); ($m, $n_) = @_; if ($m &gt; $n)&#123; $m &#125; else &#123; $n &#125;&#125; 变长的参数列表 打印最大值，“高水线（high-watermark）”算法123456789101112#!/usr/bin/perl$maximum = &amp;max(3, 5, 10, 4, 6);sub max&#123; my($max_so_far) = shift @_; foreach (@_)&#123; if ($_ &gt; $max_so_far)&#123; $max_so_far = $_; &#125; &#125; $max_so_far; print \"Max is $max_so_far\\n\";&#125; 空参数列表关于词法（my）变量 词法变量可以用在任何语句块内，而不仅限于子程序语句块。比如说，它可以在if、while或foreach的语句块里使用 求次方 12345#!/usr/bin/perlforeach (1..10)&#123; my($square) = $_ * $_; print \"$_ squared is $square.\\n\";&#125; my操作符不加括号时，只能用来声明单个词法变量 12my $fred, $barney; #只声明了fred这一个变量my（$fred, $barney); #两个都声明了 在日常perl编程中，你最好对每个新变量都使用my声明，最好对每个新变量都使用my声明，让它保持在自己所在的词法作用域内。 1234#!/usr/bin/perlforeach my $rock (qw/bedrock slate lava /)&#123; print \"One rock is $rock.\\n\";&#125; use strict编译指令（pragma） 告诉perl我愿意接受更严格的限制使用use strict这个编译指令放在程序开头 1use strict; 自perl5.12开始，如果使用编译指令指定最低perl版本号的话，就相当于隐式打开了约束指令 1use 5.012; return操作符 return操作符可以让子程序执行到一半的时候停止执行12345678910111213#!/usr/bin/perluse strict;my @names = qw/ fred barney betty dino wilma pebbles bamm-bam /;my $result = &amp;which_element_is(\"dino\", @names);sub which_element_is&#123; my($what, @array) = @_; foreach (0..$#array)&#123; if ($what eq $array[$_])&#123; return $_; &#125; &#125; -1;&#125;","categories":[],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://arvon.top/tags/学习笔记/"}]},{"title":"Perl语言入门（6th）课后习题","slug":"Perl语言入门（6th）课后习题","date":"2015-08-12T08:00:21.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/08/12/Perl语言入门（6th）课后习题/","link":"","permalink":"http://arvon.top/2015/08/12/Perl语言入门（6th）课后习题/","excerpt":"个人感觉课后习题单独分出来比较方便，这里有可能的话会多放些例子，ok，go","text":"个人感觉课后习题单独分出来比较方便，这里有可能的话会多放些例子，ok，go ###Chapter one 计算半径为12.5的园的周长，可自行输入，小于0输出为012345#!/usr/bin/perlprint \"Please input a number: \";$inputNumber = &lt;STDIN&gt;;$perimeter = $inputNumber * 2 * 3.14;print \"The perimeter is $perimeter\\n\"; ###Chapter two 半径12.5圆，求周长 12345#!/usr/bin/perluse 5.010;$pi = 3.141592654;$circ = 2 * $pi * 12.5;print \"The circumference of a circle of radius 12.5 is $circ.\\n\"; 交互算周长 123456#!/usr/bin/perl -w$pi = 3.141592654;print \"What is the radius?\\n\";chomp($radius = &lt;STDIN&gt;);$circ = 2 * $pi * $radius;print \"The circumference of a circlee of radius $radius is $circ.\\n\"; 加验证的交互算周长 123456789#!/usr/bin/perl$pi = 3.1415926;print \"What is the radius?\";chomp($radius = &lt;STDIN&gt;);$circ = 2 * $pi * $radius;if ($radius &lt; 0)&#123; $circ = 0;&#125;print \"The circumference of a circle of radius $radius is $circ.\\n\"; 交互求和 1234567#!/usr/bin/perlprint \"Enter first number: \";chomp($one = &lt;STDIN&gt;);print \"Enter second number: \";chomp($two = &lt;STDIN&gt;);$result = $one + $two;print \"The result is $result.\\n\" 字符串重复术，中间的是小写字母x 1234567#!/usr/bin/perlprint \"Enter a string: \";$str = &lt;STDIN&gt;;print \"Enter a number of times: \";chomp($num = &lt;STDIN&gt;);$result = $str x $num;print \"The result is:\\n$result\"; Chapter three 将输入的字符串倒序打印 123456#!/usr/binj/perluse 5.010;print \"Enter some lines, then press Ctrl-D:\\n\";#or try Ctrl-Z@lines = &lt;STDIN&gt;;@reverse_lines = reverse @lines;print \"@reverse_lines\"; 键入数字，输出对应的name 12345678910#!/usr/bin/perl@names = qw/ guo qiu arvon mo love /;print \"Enter some number from 1 to 5, one per line, then press Ctrl-D:\\n\";#print @names;chomp(@number = &lt;STDIN&gt;);#print \"\\@number is @number\\n\";foreach $num (@number)&#123;# print \"now \\$num is $num\"; print \"$names[ $num -1 ]\\n\";&#125; 注释的是在一行输入输入的，没注释的分行显示 12345#!/usr/bin/perl#chomp(@lines = &lt;STDIN&gt;);#@sorted = sort @lines;#print \"@sorted\\n\";print sort &lt;STDIN&gt;; ###小代码块 检查参数个数是否正确1234567sub max &#123; if (@_ != 2)&#123; print \"WAENING! &amp;max should get exactly two argumnets!\\n\"; &#125; # follow #...&#125;","categories":[],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://arvon.top/tags/学习笔记/"}]},{"title":"Perl语言入门（一）","slug":"Prel语言入门读书笔记","date":"2015-08-10T06:55:36.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/08/10/Prel语言入门读书笔记/","link":"","permalink":"http://arvon.top/2015/08/10/Prel语言入门读书笔记/","excerpt":"更新记录： 2015/08/10 DonePage45 2015/08/11 DonePage68 2015/08/17 Read list and array, do a new one.","text":"更新记录： 2015/08/10 DonePage45 2015/08/11 DonePage68 2015/08/17 Read list and array, do a new one. ##前言杂记 适合管理员使用的语言 Arvon’s读书笔记 sixth edtion for learning perl 12use 5.014;#该脚本需要在Perl 5.14或更高版本才能正常运行perl -v#查看perl的版本 Hello，World 1234#!/usr/bin/perlprintf \"Hello, world!\\n\";use 5.010;say \"Hello, world!\"; 替换字符演示 123456#!/usr/bin/perl@lines = `perldoc -u -fatan2`;foreach (@lines)&#123; s/\\w&lt;([^&gt;]+)&gt;/\\U$1/g; print; &#125; ##标量数据 标量数据是Perl里面最简单的一种数据类型。对大部分标量来说，它要么是数字（如123或3.25e23），要么是由字符组成的序列（如hello）。对Perl来说，数字和字符串大多情况下是可以在内部转换的。可以用操作符对标量进行操作（如加法或字符串连接），产生的结果通常也是一个标量。标量可以存储在标量变量里，也可以从文件和设备读取或写入这些位置。 ###数字 ####所有数字的内部格式都相同 Perl内部，总是按“双精度浮点数（double-precision floating-point）”的要求来保存数字并进行运算的。也就是说，Perl内部并不存在整数值–程序中用到的整数型常量会被转换成等效的浮点数值。 ####浮点数直接量 直接量（literal）是指某个数字在Perl源代码中的写法。直接量并非运算结果，也不是I/O（输入/输出）操作的结果，它只是直接键入源程序代码中的数据。 Perl浮点数直接量的写法，小数点与前置的正负号都是可选的，数字后面也可以加上用“e”表示的10的次方表示符（即指数表示法）。 如下列写法 1.25255.000255.07.25e45 #7.25乘以10的45次方，其中e可以大写-6.5e123 ####整数直接量 整数直接量： 01234-432141023789873 #可以写成41_023_789_873,Perl允许在整数直接量插入下划线，方便辨认 ####非十进制的整数直接量 Perl语言和其他许多程序语言一样，允许使用十进制（decimal）意外的其他进制表示数字。八进制（octal）直接量以0开头，十六进制（hexadecimal）直接量以ox开头，二进制（binary）直接量以ob开头。十六进制的A到F（可以写成小写的a到f，来代表十进制的10到15）。例如 o377 #八进制的377，等于十进制的2550xff #十六进制的FF，等于十进制的255ob11111111 #二进制的，等于十进制的255 这三个数字虽然看起来不同，但对Perl来说都是同一个数字 ####数字操作符 Perl提供了各种常见的数字操作符，如加、减、乘、除、取模、次方。例如： 2+35.1-2.431214/210.2/0.310/310%3 #取模，结果为12*3 #次方，结果为8 ###字符串 字符串就是一个字符序列，如hello。字符串可以各种字符任意组合而成。最短的字符串不包含任何字符，也叫做空字符串。最长的字符串的长度没有限制。这符合Perl遵守的“无内置限制（nobuilt-inlimits）”的原则。字符串通常是由可输出地字母、数字及标点符号组成，其范围介于ASCII编码的32到126之间。由于字符串可以包含任何字符，所以可用它来创建、扫描或操控二进制数据，这是许多其他工具语言望尘莫及的。例如：你可以将一个图形文件或编译过的可执行文件读进Perl的字符串变变量，修改它的内容再写回去。 Perl完全支持Unicode，所以在字符串中可以使用任意一个合法的Unicode字符。不过由于Perl的历史原因它不会自动将程序源代码当做Unicode编码的文本读入，所以如果你想要在源代码中使用Unicode书写直接量的话，需手工加上utf8编译指令： 1use utf8; 和数字一样，字符串也有直接量记法，也就是Perl程序中字符串的书写方式。包括单引号内的字符串和双引号内的字符串。 ####单引号内的字符串直接量 除了单引号和反斜线之外，单引号内所有的字符串都代表他们自己。 ‘fred’‘hello’‘’‘#$@%’‘Don\\’t let me go!’‘the last character is a backslash:\\‘‘\\’\\‘ #单引号后面紧接着反斜线‘hi\\n’ #单引号内的\\n并不是换行符，而是表示字面上的两个字符 ####双引号内的字符串直接量 双引号中的反斜线更为强大 “hi\\n” #换行符“love\\tyou” #水平制表符“\\r” #回车“\\f” #换页符“\\b” #退格“\\e” #Esc（ASCII编码的转义字符）“\\cC” #控制符，就是COntrol键的代码（此例表示同时按下Ctrl和c键的返回码）“\\l” #将下个字母转换为小写“\\L” #将它后面的所有字母都转换为小写的，直到\\E为止“\\u” #将下个字符转换为大写“\\U” #将它后面所有的字母都转换为大写，直到\\E为止“\\E” #结束\\L、\\U和\\Q开始的作用范围 ####字符串操作符 字符串可以用.操作符链接起来 “hello”.”world” #等同于”helloworld”“hello” . ‘ ‘ . “world” #等同于’hello world’‘hello world’ . “\\n” #等同于”hello world\\n” 注意：链接运算符必须显示使用连接操作符（concatenation operator），而不是像其他一些语言只需要把两个字符串放在一起就行。 特殊的重复操作符，小写字母x，此操作符会将其左边的操作数与它本身重复连接，重复次数由右边的操作数（某个数字）指定。例子： “fred” x 3 #得到“fredfredfred”“barney” x (4+1) #得到“barneybarneybarneybarneybarney”5 x 4.8 #相当于5乘以4，它会把4.8当做4，因为这里是小写字母x而不是* ####数字与字符串之间的自动转换 Perl根据操作符来确定你需要的是数字还是字符串。如操作符（比如+号）需要的是数字，Perl会将操作数视为数字；在操作符（比如.）需要字符串时，Perl便会将数视为字符串。1234567891011121314#!/usr/bin/perl#Todo: concatenation operatoruse 5.010;printf \"5*4.8\\n\";printf 5 * 4.8 . \"\\n\";printf 5 x 4.8 . \"\\n\";printf \"z\" . 5*7 . \"\\n\";###Output[root@Mo arvon_perl]# perl string_repetition5*4.8245555z35 ###Perl的内置警告信息 从Perl的5.6版本开始，可以通过编译指令开启警告功能 12#!/usr/bin/perluse warnings; 也可以在命令行上使用-w选项对要运行的程序开启警告功能 $ perl -w program.pl 还可以在shebang行上指定命令行选项 1#!/usr/bin/perl -w 如果看不懂某个警告信息，可以利用diagnostics这个编译命令报告更为详尽的问题描述。在perdiag文档中列有简要警告信息和详细诊断说明，该文档时理解diagnostics输出信息的最佳参考： 12#!/usr/bin/perluse diagnostics; #会使程序变慢，如果熟悉，尽量不使用 ###标量变量 变量（variable）就是存储一个或多个值的容器的名称。而标量变量就是单单存储一个值的变量。变量的名称在整个程序中保持不变，但它所持有的值是可以在程序运行时不断修改变化的。 标量变量存储的是单个标量值。标量变量的名称是以$开头的，这个符号也称为魔符（sigil），然后是变量Perl的标识符：由一个字母或下划线开头，后接多个字母、数字、下划线。标识符是区分大小写的：$Fred和$fred是完全不同的变量。 $name$Name$NAME$a_very_long_variable_that_ends_in_i Perl通过变量标识符的魔符来区分它是什么类型的变量。所以不管取什么名字，都不会和Perl自带的函数或操作符的写法相冲突。$的确切意思是“取单个东西”或者“取标量”。 给变量取个好名字，例如：$my_name or $myName ###标量的赋值（assignment） 和其他程序语言差不多，Perl的赋值操作符为等号，等号的左边是变量名称，右边为某个表达式。1234$fred = 17;$barney = 'hello';$barney = $fred+3;$barney = $barney*2; ####双目赋值的操作符 例子一 123$fred = $fred + 5; #相当于$fred += 5;$barney = $barney * 3; #相当于$barney *= 3;$str = $str. \" \"; #相当于$str .= \"\"; 例子二 1234567891011121314#!/usr/bin/perl$fred = 1;$fred = $fred + 5;printf \"now + 5 fred is \" . $fred . \"\\n\";$fred += 2;printf \"then + 2 fred is \" . $fred . \"\\n\";$str = 'hello';$str = $str . \" \";print \"the string \\$str is \" . $str . \"\\n\";###Outputroot@Mo arvon_perl]# perl double.plnow + 5 fred is 6then + 2 fred is 8 字符串中的标量变量内插 12345678910#!/usr/bin/perl$meal = \"love\";$things = \"arvon $meal mo\";print \"$things\\n\";$newThings = ' arvon ' . $meal . ' mo';print 'now the $newThings is' . $newThings . \"\\n\";###Output[root@Mo arvon_perl]# perl varInsert.plarvon love monow the $newThings is arvon love mo 如果变量从未被赋值过，就会用空字符串来替换 如果只是打印这个变量值，则不必使用变量内插的方式： 12print \"$fred\"；print $fred； #用这个比较好 可以直接键入一些字符的代码点(code point)，再通过chr（）函数转换成对应字符,反过来我们可以通过ord（）函数把字符转换为代码点 12$alef = chr( 0x05Do ）；$codePoint = ord('?'); ###操作符的优先级与结合性 在复杂的表达式里，先执行哪个操作再执行哪个操作，取决于操作符的优先级。在Perl里乘法的优先级高于加法，可以使用括号改变执行的优先级 当两个优先级相同的操作符抢着使用三个操作数时，优先级便交由结合性解决：1234 ** 3 ** 2 #4 ** (3 ** 2),得4 ** 9，向右结合72 / 12 / 3#(72 / 12） / 3,得6 / 3，向左结合36 / 6 * 3 #(36 / 6) * 3 ###比较操作符 perl的比较操作符类似于代数系统：&lt;,&lt;=,==,&gt;=,&gt;,!=。这些操作符的返回值要么是true要么是false。 字符串比较时，使用lt、le、eq、ge、gt、ne。注意:字符在ASCII编码中的顺序并不总是对应于字符本身意义上的顺序。 ##控制结构 ###if控制结构 ####例子123456789#!/usr/bin/perl#$name = 'fred';$name = 'Nfred';if ($name eq \"fred\")&#123; print \"'$name' is 'fred' in\\n\";&#125;else&#123; print \"'$name' is not 'fred' is $name\\n\";&#125; ####布尔值 任何标量值都可以成为if控制结构里的判断条件。如果把表达式返回的真假值保存到变量中，那在判断时可以直接检查该变量的值，读起来也方便： 12345678910111213#!/usr/bin/perl#$name = 'fred';$name = 'Nfred';if ($name eq \"fred\")&#123; print \"'$name' is 'fred' in\\n\";&#125;else&#123; print \"'$name' is not 'fred' is $name\\n\";&#125;###Output[root@Mo arvon_perl]# perl Boolean_value.plhello, world Perl和其他语言不同，Perl并没有专用的“布尔（boolean）”数据类型，它是靠一些简单的规则来判断的： 如果是数字，0为假，所有其他数字都为真 如果是字符串，空字符串（’’)为假；所有其他字符串为真。 如果既不是数字也不是字符串，那就先转换成数字或字符串再进行判断。 ###获取用户输入 用户输入的例子1234567891011121314151617#!/usr/bin/perlprint \"Input something please: \";$line = &lt;STDIN&gt;;if ($line eq \"\\n\")&#123; print \"That was just a blank line!\\n\";&#125;else&#123; print \"That line of input was: $line\";&#125;###Output[root@Mo arvon_perl]# perl stdin01.plInput something please:That was just a blank line![root@Mo arvon_perl]# perl stdin01.pl ]Input something please: helloThat line of input was: hello ###chomp操作符 chomp（）操作符只能用于单个变量，且该变量的内容必须为字符串，如果该字符串的末尾是换行符，chomp（）的任务就是去掉它。 12345678#!/usr/bin/perl$text = \"a line of text\\n\"; #or input by &lt;STDIN&gt;chomp($text); #remove the \\nprint $text;###Output[root@Mo arvon_perl]# perl chompTraining.pla line of text[root@Mo arvon_perl]# 处理字符串变量 12345678910#!/usr/bin/perlchomp($text = &lt;STDIN&gt;); #读入文字，略过最后的换行符#$text = &lt;STDIN&gt;; #等同于上面的写法#chomp($text):print $text;###Output[root@Mo arvon_perl]# perl chompTraining02.plhello,worldhello,world[root@Mo arvon_perl]# ###while控制结构1234567891011121314#!/usr/bin/perl$count = 0;while ($count &lt; 10)&#123; $count += 2; print \"Now the number is $count\\n\";&#125;###Output[root@Mo arvon_perl]# perl whileCount.plNow the number is 2Now the number is 4Now the number is 6Now the number is 8Now the number is 10 ###undef值 我们未赋值时就用到了某个不存在标量变量，并不会让程序停止运行，当成数字使用，它会表现的像0；当做字符串使用，它会表现的像空字符串。123456789101112#!/usr/bin/perl#累加奇数$n = 1;while ($n &lt; 10)&#123; $sum += $n; $n += 2;#准备奇数&#125;print \"The total was $sum.\\n\"###Output[root@Mo arvon_perl]# perl accumulation.plThe total was 25. ###definded函数 行输入操作符有时候会返回undef。要判断某个字符串是undef而不是空字符串，可以使用defined函数。如果是undef，该函数返回假，否则返回真：12345678910111213141516171819#!/usr/bin/perl$madonna = &lt;STDIN&gt;;if (defined($madonna))&#123; print \"The input was $madonna\";&#125;else&#123; print \"No input available\\n\";&#125;#$madonna = undef;###Output[root@Mo arvon_perl]# perl undef01.plNo input available[root@Mo arvon_perl]# perl undef01.plMoThe input was Mo[root@Mo arvon_perl]# perl undef01.plThe input was ##列表和数组 如果Perl的标量代表单数（singular），那么列表和数组就表示复数（plural）。 列表（list）是标量的有序集合，而数组（array）则是存储列表的变量。列表指的是数据，而数组指的是变量。列表里的值不一定放在数组里，但每个数组变量都包含一个列表（即便是不含任何元素的空列表。 数组或列表中的每个元素 （element）都是单独的标量变量，拥有独立的标量值。这些值是有序的，从开始到终止元素的先后次序是固定的。 ###访问数组中的元素 Example one1234567891011#!/usr/bin/perluse 5.010;#print \"Hello world\\n\";$fred[0] = \"Hello\";$fred[1] = \"My\";$fred[2] = \"Name\";print \"$fred[0]\\n\";###Output&gt;arvon@Mo:~/arvon_perl&gt; perl array.plHello ###特殊的数字索引 Example two123456789101112131415161718192021#!/usr/bin/perluse 5.010;$rocks[0] = 'hello';$rocks[1] = 'list';$rocks[2] = 'array';$rocks[3] = 'element';$rocks[4] = 'four';$rocks[9] = 'nine';#$end = $#rocks;print \"\\$end is $end\\n\";#$number_of_rocks = $end + 1;print \"The number is $number_of_rocks\\n\";#$rocks[ $#rocks ] = 'hard rodk';###Outputarvon@Mo:~/arvon_perl&gt; perl array_print.pl$end is 9The number is 10 ###列表直接量 列表直接量（list literal），可以由圆括号内用逗号隔开的一组数据表示，而这些数据就称为元素。for example12345678(1, 2, 3) #(1, 2, 3,)相同的逗号会被忽略(\"fred\", 4.5) #两个元素，'fred'和4.5() #空列表，0个元素(1..100) #100个整数组成的列表(1..5) #..是范围操作符（range operator）(1.7..5.7) #同上，但两个数字的小数部分会被去掉(5..1) #表示空列表，只能正向计数($m..$n) #由$m和$n决定 ###qw简写 在perl程序中，经常会需要建立简单的单词列表。使用qw简写，可以省去键入的引号。123456789101112131415161718192021#!/usr/bin/perluse 5.010;(\"Mo\", \"have\", \"rose\", \"you\", \"known\");#qw( Mo have rose you known );#qw(Mohava roseyou known);#qw( Mo have rose you known);#qw! Mo have rose you known!;qw/ Mo have rose you known/;qw# Mo have rose you known#; ###列表的赋值 和标量赋值一样，列表值可以被赋值给变量1234#!/usr/bin/perluse 5.010;($fred, $barney, $dino) = (\"flintstone\", \"rubble\", undef);#左侧列表中的三个变量会依次被赋予右侧列表中对应的值，相当于分别做了三次独立的赋值操作； ###pop和push操作符 要增加元素到数组尾端时，只要将它存放到更高索引的新位置就可以了。12345678910111213141516171819202122#!/usr/bin/perl@array = 5..9;$mo = pop(@array);printf \"now \\$mo is $mo\\n\";$arvon = pop(@array);printf \"now \\$arvon is $arvon\\n\";push(@array, 0);printf \"now \\@array is @array\\n\";push@array,3;printf \"now \\@array is @array\\n\";push@array,\"Hello\";printf \"now \\@array is @array\\n\";@newOne = qw/ my name is arvon /;print \"@newOne\\n\";###Outputarvon@Mo:~/arvon_perl&gt; perl assignment_array.plnow $mo is 9now $arvon is 8now @array is 5 6 7 0now @array is 5 6 7 0 3now @array is 5 6 7 0 3 Hellomy name is arvon ###shift和unshift操作符 相反，unshift和shift操作符是对数组的开头进行处理1234567891011121314151617181920212223242526#!/usr/bin/perluse 5.010;@array = qw/ why are you so diao /;$one = shift@array;#$one is \"why\", @array = are you so diaoprint \"$one is not @array\\n\";$two = shift(@array);#$two is \"are\", @array = you so diaoprint \"$two is not @array\\n\";$three = shift@array;#$three is \"you\", @array= so diaoprint \"$three is not @array\\n\";##unshift(@array, 4);print \"Now \\@array is @array\\n\";unshift(@array, 5);print \"Now \\@array is @array\\n\";@others = 1..3;unshift @array, @others; #@array变成了(1, 2, 3, 4, 5, )print \"\\@others is @others, But now \\@array is already @array\";###Outputarvon@Mo:~/arvon_perl&gt; perl unshift_str.plwhy is not are you so diaoare is not you so diaoyou is not so diaoNow @array is 4 so diaoNow @array is 5 4 so diao@others is 1 2 3, But now @array is already 1 2 3 5 4 so diao ###splice操作符 push-pop和shift-unshift操作符都是对数组首尾进行操作的，中间的话就用splice Example one 12345678910111213#!/usr/bin/perluse 5.010;@array = qw( pebbles dino fred barney betty );@removed = splice @array, 2;#在原来的数组中删掉fred及其后的元素#@removed变成qw（fred barney betty）#而原来的@array则变成qw（pebbles dino）print \"\\@array now is @array\\n\";print \"\\@removed is @removed\\n\";###Outputarvon@Mo:~/arvon_perl&gt; perl splice_string.pl@array now is pebbles dino@removed is fred barney betty Example two 1234567891011121314151617#!/usr/bin/perluse 5.010;@array = qw( pebbles dino fred barney betty );#@removed = splice @array, 2;#在原来的数组中删掉fred及其后的元素##@removed变成qw（fred barney betty）##而原来的@array则变成qw（pebbles dino）#print \"\\@array now is @array\\n\";#print \"\\@removed is @removed\\n\";#@removed = splice @array, 1, 2;##This time @removed is (pebbles, barney, betty )@removed = splice @array, 1, 0, qw(wilema);##Just known @array is became (pebbles wilema dino fred barney betty)print \"now \\@removed is @removed, and \\@array is @array.\\n\"###Outputarvon@Mo:~/arvon_perl&gt; perl splice_string.plnow @removed is , and @array is pebbles wilema dino fred barney betty. ###字符串的内插 和标量一样，数组的内容同样可以被内插到双引号中。内插时，会在数组的各个元素之间自动添加分隔用的空格 12345678#!/usr/bin/perluse 5.010;@rocks = qw/ flintstone slate rubble /;print \"Quartz @rocks limestone\\n\"###Outputarvon@Mo:~/arvon_perl&gt; perl array_one.plQuartz flintstone slate rubble limestone Example for index_expression 12345678#!/usr/bin/perluse 5.010;@fred = qw(eating rocks is wrong);$fred = \"right\";print \"This is $fred[3]\\n\"; #just rightprint \"This is $&#123;fred&#125;[3]\\n\"; #wrongprint \"This is $fred\".\"[3]\\n\"; #wrongprint \"This is $fred\\[3]\\n\"; #still wrong ##后续下一篇，太长了不方便查阅 So， Let’s go","categories":[],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://arvon.top/tags/学习笔记/"}]},{"title":"Linux挂载LVM分区","slug":"Linux挂载LVM分区","date":"2015-08-07T10:02:24.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/08/07/Linux挂载LVM分区/","link":"","permalink":"http://arvon.top/2015/08/07/Linux挂载LVM分区/","excerpt":"简介：LVM是 Logical Volume Manager（逻辑卷管理）的简写，它是Linux环境下对磁盘分区进行管理的一种机制，它由Heinz Mauelshagen在Linux 2.4内核上实现，目前最新版本为：稳定版1.0.5，开发版 1.1.0-rc2，以及LVM2开发版。Linux用户安装Linux操作系统时遇到的一个常见的难以决定的问题就是如何正确地评估各分区大小，以分配合适的硬盘空间。普通的磁盘分区管理方式在逻辑分区划分好之后就无法改变其大小，当一个逻辑分区存放不下某个文件时，这个文件因为受上层文件系统的限制，也不能跨越多个分区来存放，所以也不能同时放到别的磁盘上。而遇到出现某个分区空间耗尽时，解决的方法通常是使用符号链接，或者使用调整分区大小的工具，但这只是暂时解决办法，没有从根本上解决问题。随着Linux的逻辑卷管理功能的出现，这些问题都迎刃而解，用户在无需停机的情况下可以方便地调整各个分区大小。","text":"简介：LVM是 Logical Volume Manager（逻辑卷管理）的简写，它是Linux环境下对磁盘分区进行管理的一种机制，它由Heinz Mauelshagen在Linux 2.4内核上实现，目前最新版本为：稳定版1.0.5，开发版 1.1.0-rc2，以及LVM2开发版。Linux用户安装Linux操作系统时遇到的一个常见的难以决定的问题就是如何正确地评估各分区大小，以分配合适的硬盘空间。普通的磁盘分区管理方式在逻辑分区划分好之后就无法改变其大小，当一个逻辑分区存放不下某个文件时，这个文件因为受上层文件系统的限制，也不能跨越多个分区来存放，所以也不能同时放到别的磁盘上。而遇到出现某个分区空间耗尽时，解决的方法通常是使用符号链接，或者使用调整分区大小的工具，但这只是暂时解决办法，没有从根本上解决问题。随着Linux的逻辑卷管理功能的出现，这些问题都迎刃而解，用户在无需停机的情况下可以方便地调整各个分区大小。 手动将LVM挂载至linux系统12345678910111213141516171819202122rpm -qa lvm2#查看是否安装了lvmyum install lvm2#安装lvm软件vgscan#查找并显示系统中存在的LVN卷组lvscan#查看系统中存在哪些卷组以及它们的激活状态（注意：如果有两个卷组名一样，其中一个是无法挂载的，要想挂载，需要重命名一个卷组）vgdisplay LVMname | more#显示所有名为LVMname卷组的详细信息，请记住你将要挂载卷组的 VG UUID。 vgrename o236o6-MGAQ-erKG-k6cH-12Zs-8NwX-CCqPG3 vgNewName#卷组重新命名为vgNewNamevgchange -ay /dev/vgNewName#激活卷组vsNewName，此时你也并不能使用，需要挂载lvscan#查看现在的激活状态mount /dev/vgNewName /somePoint#挂载LVM到挂载点，这样就可以访问了umount /dev/vgNewName# or usr 'umount /somePoint'#卸载LVM卷组vgchange -an /dev/vgNewName#将卷组修改成不激活状态","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"}]},{"title":"nc命令","slug":"nc命令","date":"2015-07-30T04:10:00.000Z","updated":"2018-02-23T09:41:32.000Z","comments":true,"path":"2015/07/30/nc命令/","link":"","permalink":"http://arvon.top/2015/07/30/nc命令/","excerpt":"由于telnet不能测试udp端口联通性，nc命令可以检测udp端口连接状态，这里记录一下用法nc命令介绍:可以检测tcp/udp端口连通性、端口扫描、远程传输文件、克隆硬盘或分区、保存web页面、模拟http Headers、简单聊天等等","text":"由于telnet不能测试udp端口联通性，nc命令可以检测udp端口连接状态，这里记录一下用法nc命令介绍:可以检测tcp/udp端口连通性、端口扫描、远程传输文件、克隆硬盘或分区、保存web页面、模拟http Headers、简单聊天等等 参数介绍1234567891011121314-g&lt;网关&gt; 设置路由器跃程通信网关，最多可设置8个。-G&lt;指向器数目&gt; 设置来源路由指向器，其数值为4的倍数。-h 在线帮助。-i&lt;延迟秒数&gt; 设置时间间隔，以便传送信息及扫描通信端口。-l 使用监听模式，管控传入的资料。-n 直接使用IP地址，而不通过域名服务器。-o&lt;输出文件&gt; 指定文件名称，把往来传输的数据以16进制字码倾倒成该文件保存。-p&lt;通信端口&gt; 设置本地主机使用的通信端口。-r 乱数指定本地与远端主机的通信端口。-s&lt;来源位址&gt; 设置本地主机送出数据包的IP地址。-u 使用UDP传输协议。-v 显示指令执行过程。-w&lt;超时秒数&gt; 设置等待连线的时间。-z 使用0输入/输出模式，只在扫描通信端口时使用。 检测端口连通性12345678#TCP#TCP单个端口或范围扫描nc -vz -w2 192.168.20.133 1870nc -vz -w2 192.168.20.133 1-1870#UDP#UDP单个端口或范围扫描nc -uz -w2 192.168.20.133 5000nc -uz -w2 192.168.20.133 4990-5000 模拟接收端口数据12nc -lk 10.17.0.167 2003#如判断数据是否传送到达 传输文件1234#目标文件主机命令，这个需要在文件传输之前执行，file.txt为文件传输过来后的文件名nc -l 1234 &gt; file.txt#源文件主机命令，hello.txt为传输文件，63为目标主机nc 172.17.18.63 1234 &lt; hello.txt 聊天与传输文件基本一样12nc -l 1234 #主机1nc 192.168.2.34 1234 #主机2 操作memcache123456789101112#存储数据printf “set key 0 10 6rnresultrn” |nc 192.168.2.34 11211#获取数据printf “get keyrn” |nc 192.168.2.34 11211#删除数据printf “delete keyrn” |nc 192.168.2.34 11211#查看状态printf “statsrn” |nc 192.168.2.34 11211#模拟top命令查看状态watch “echo stats” |nc 192.168.2.34 11211#清空缓存printf “flush_allrn” |nc 192.168.2.34 11211 操作redis1redis-cli","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"},{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"}]},{"title":"简明Python读书笔记","slug":"简明Python学习笔记","date":"2015-07-30T03:25:39.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/07/30/简明Python学习笔记/","link":"","permalink":"http://arvon.top/2015/07/30/简明Python学习笔记/","excerpt":"更新记录 20150730 总结介绍 20150731 控制流 20150804 module 20150806 脚本练手 20150807 模块方法 20150810 是时候结束了","text":"更新记录 20150730 总结介绍 20150731 控制流 20150804 module 20150806 脚本练手 20150807 模块方法 20150810 是时候结束了 介绍 简记 这里使用的是CentOS release 6.6 (Final) 12345678910[root@Mo ~]# python -V #查看python的版本Python 2.6.6[root@Mo arvon_python]# which python #查看python命令的安装的位置/usr/bin/python[root@Mo arvon_python]# echo $PATH #查看$PATH变量，可以通过which命令查看一个命令的位置/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin #缩进可以使用Tab、2个空格、4个空格，选择一个长期用下去 12345678#!/usr/bin/python#-*- coding: utf-8 -*-#the script is about expressionlength = float(raw_input(\"Enter the length:\"))breadth = float(raw_input(\"Enter the breadth:\"))area = length * breadthprint 'Area is', areaprint 'Perimeter is', 2* (length + breadth) 控制流简介 在python中有三种控制流语句if、for、while if语句实例1234567891011121314#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: show how to use if#number = int(raw_input(\"Input a number:\"))number = 25guess = int(input(\"Enter a number which you are think:\"))print guessprint numberif guess == number: print \"Your number is right\"elif guess &gt; 25: print \"Your number is:\", guess, \"It,s too big\"elif guess &lt; 25: print \"You number is:\", guess, \"is small\" while语句实例123456789101112131415161718#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: whilenumber=23running=Truewhile running: guess=int(raw_input('Enteraninteger:')) if guess == number: print 'Congratulations, you guessed it.' running = False #this causes the while loop to stop elif guess &lt; number: print 'No, it is a litte higher than that' else: print 'No, it is a little lower than that'else: print 'The while loop is over.' #Do anything else you want to do hereprint 'done' for语句123456#!/usr/bin/python#-*- coding: utf-8 -*-for i in range(1, 15, 2): print ielse: print 'THe for loop is over' break语句 break语句是用来终止循环语句的，哪怕循环条件没有称为False或序列还没有被完全递归，也停止执行循环语句。 12345678910#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: break.pyruning = 'True'while runing: s = raw_input('Enter your name:') if s == 'quit': break print 'Length of the string is', len(s)print 'Done' continue语句 跳过当前循环块中的剩余语句，然后进行下一轮循环。 12345678910#!/usr/bin/python#-*- coding: utf-8 -*-runing = Truewhile runing: s = raw_input('Enter something:') if s == 'quit': break if len(s) &lt; 3: continue print 'Input is of sufficient length' 函数 函数是重用的程序段。它们允许你给一块语句一个名称，然后你可以在你的程序的任何地方使用这个名称任意多次地运行这个语句块，这称为调用函数。函数通过def关键字定义。def关键字后跟一个函数的标识符名称，然后跟一对圆括号。圆括号可以包括一些变量名，该行以冒号结尾，接下来的是一块语句，它们是函数体。 定义函数12345#!/usr/bin/python#-*- coding: utf-8 -*-def sayHello(): print 'Hello World！'#block belonging to the functionsayHello()#call the function 使用函数形参123456789101112#!/usr/bin/python#-*- coding: utf-8 -*-#Filename:func_param.pydef printMax(a,b): if a&gt;b: print a, 'is max number' else: print b, 'is max number'printMax(3,4)#directly give litter valuesx = 5y = 7printMax(x,y)#give variables as arguments 局部变量123456789#!/usr/bin/python#Filename: func_local.pydef func(x): print 'x is', x x = 2 print 'Changed local x to', xx = 50func(x)print 'x is still', x global语句1234567891011#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: func_global.pydef func(): global x print 'x is', x x = 2 print 'Changed local x to', xx = 50func()print 'Value of x is', x 使用默认参数值 默认参数值是一个参数，默认参数值应该是不可变的，只有在形参表末尾的那些参数可以有默认参数值，即你不能在声明函数形参的时候，先声明有默认值的形参而后声明没有默认值的形参，例如：def func（a，b=5）是有效地，但是def func（a=5， b）是无效的 1234567#!/usr/bin/python#-*- coding: utf-8#Todo: func_default.pydef say (message, times=1): print message*timessay('Hello')say('World',5) 关键参数123456789101112#!/usr/bin/python#-*- coding: utf-8 -*-def func(a,b = 5, c=10): print 'a is', a, 'and b is', b, 'and c is', cfunc(3, 7)func(25, c=24)func(c= 50, a= 100)###Output[root@Mo arvon_python]# python func_key.pya is 3 and b is 7 and c is 10a is 25 and b is 5 and c is 24a is 100 and b is 5 and c is 50 return语句 return语句用来从一个函数返回即跳出函数。也可选函数返回一个值 123456789101112#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: func_return.pydef maximum(x, y): if x&gt;y: return x else: return yprint maximum(2, 3)###Output[root@Mo arvon_python]# python func_return.py3 文档字符串（DocStrings） 在函数的第一个逻辑行的字符串是这个函数的文档那个字符串 12345678910111213141516171819#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: func_doc.pydef printMax(x, y): '''Prints the max num of tow numbers. The two values must be in tetgers.''' x = int(x)#convert to in tegers, if possible y = int(y) if x&gt;y: print x, 'is max num' else: print y, 'is max num'printMax(3, 5)print printMax.__doc__###Out put[root@Mo arvon_python]# python func_doc.py5 is max numPrints the max num of tow numbers. The two values must be in tetgers. 模块模块概述 模块的用处在于它能为你在别的程序中重用它提供的服务和功能。Python附带的标准库就是这样一组模块的例子。 使用sys模块12345678910111213141516#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: using_sys.pyimport sys #sys是system的缩写print 'The command line arguments are:'for i in sys.argv:#脚本的名称总是sys.argv列表的第一个参数 print iprint '\\n\\nThe PYTHON PATH is', sys.path, '\\n'###Output[root@Mo arvon_python]# python using_sys.pyThe command line arguments are:using_sys.pyThe PYTHON PATH is ['/root/arvon_python', '/usr/lib64/python26.zip', '/usr/lib64/python2.6', '/usr/lib64/python2.6/plat-linux2', '/usr/lib64/python2.6/lib-tk', '/usr/lib64/python2.6/lib-old', '/usr/lib64/python2.6/lib-dynload', '/usr/lib64/python2.6/site-packages', '/usr/lib/python2.6/site-packages'] 字节编译的.pyc文件 输入一个模块相对来说是一个比较费时的事情，所以Python做了一些技巧，以便使输入模块更加快一些。一种方法是创建 字节编译的文件 ，这些文件以.pyc作为扩展名。字节编译的文件与Python变换程序的中间状态有关（是否还记得Python如何工作的介绍？）。当你在下次从别的程序输入这个模块的时候，.pyc文件是十分有用的——它会快得多，因为一部分输入模块所需的处理已经完成了。另外，这些字节编译的文件也是与平台无关的。所以，现在你知道了那些.pyc文件事实上是什么了。 from..import语句 如果你想要直接输入argv变量到你的程序中（避免在每次使用它时打sys.），那么你可以使用from sys import argv语句。如果你想要输入所有sys模块使用的名字，那么你可以使用from sysimport *语句。这对于所有模块都适用。一般说来，应该避免使用from..import而使用import语句，因为这样可以使你的程序更加易读，也可以避免名称的冲突。 不建议使用，尽量使用sys.argv这样的 模块的name(这里左右都是两个下划线)1234567#!/usr/bin/python#-*- coding: utf-8 -*-if __name__ == ' __main__ ':#模块是对象，并且所有的模块都有一个内置属性 __name__。一个模块的 __name__ 的值取决于您如何应用模块。如果 import 一个模块，那么模块__name__ 的值通常为模块文件名，不带路径或者文件扩展名。但是您也可以像一个标准的程序样直接运行模块，在这 种情况下, __name__ 的值将是一个特别缺省\"__main__\" print 'This program is being run by itself'else: print 'I am being imorted form another module' 创造自己的module1234567#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: creat own module.pydef sayHi(): print 'Hi, this is my wife pikachu.'version = '0.1'#End of my module.py 123456789101112#!/usr/bin/python #使用自己创建的模块#-*- coding: utf-8 -*-#Todo: use my own moduleimport myModulemyModule.sayHi()print 'Version',myModule.version### Output[root@Mo arvon_python]# python usemyModule.pyHi, this is my wife pikachu.Version 0.1 使用dir函数12345678910111213141516171819202122232425&gt;&gt;&gt; import sys&gt;&gt;&gt; dir(sys)['__displayhook__', '__doc__', '__excepthook__', '__name__', '__package__', '__stderr__', '__stdin__', '__stdout__', '_clear_type_cache', '_current_frames', '_getframe', 'api_version', 'argv', 'builtin_module_names', 'byteorder', 'call_tracing', 'callstats', 'copyright', 'displayhook', 'dont_write_bytecode', 'exc_clear', 'exc_info', 'exc_type', 'excepthook', 'exec_prefix', 'executable', 'exit', 'flags', 'float_info', 'getcheckinterval', 'getdefaultencoding', 'getdlopenflags', 'getfilesystemencoding', 'getprofile', 'getrecursionlimit', 'getrefcount', 'getsizeof', 'gettrace', 'hexversion', 'maxint', 'maxsize', 'maxunicode', 'meta_path', 'modules', 'path', 'path_hooks', 'path_importer_cache', 'platform', 'prefix', 'ps1', 'ps2', 'py3kwarning', 'setcheckinterval', 'setdlopenflags', 'setprofile', 'setrecursionlimit', 'settrace', 'stderr', 'stdin', 'stdout', 'subversion', 'version', 'version_info', 'warnoptions']&gt;&gt;&gt; dir()['__builtins__', '__doc__', '__name__', '__package__', 'sys']&gt;&gt;&gt; dir()['__builtins__', '__doc__', '__name__', '__package__', 'a', 'sys']&gt;&gt;&gt; delaTraceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;NameError: name 'dela' is not defined&gt;&gt;&gt; dir()['__builtins__', '__doc__', '__name__', '__package__', 'a', 'sys']&gt;&gt;&gt; del a&gt;&gt;&gt; dir ()['__builtins__', '__doc__', '__name__', '__package__', 'sys']###说明#首先，我们来看一下在输入的sys模块上使用dir。我们看到它包含一个庞大的属性列表。#接下来，我们不给dir函数传递参数而使用它——默认地，它返回当前模块的属性列表。注#意，输入的模块同样是列表的一部分。#为了观察dir的作用，我们定义一个新的变量a并且给它赋一个值，然后检验dir，我们观察到在#列表中增加了以上相同的值。我们使用del语句删除当前模块中的变量/属性，这个变化再一次#反映在dir的输出中。#关于del的一点注释——这个语句在运行后被用来 删除 一个变量/名称。在这个例子中，del a，#你将无法再使用变量a——它就好像从来没有存在过一样。 数据结构简介 数据结构基本上就是它们可以处理一些数据的结构，或说，它们是用来存储一组相关数据的。在Python中有三种内建的数据结构——列表、元组和字典。 列表 list是处理一组有序项目的数据结构，即你可以在一个列表中存储一个 序列 的项目。假想你有一个购物列表，上面记载着你要买的东西，你就容易理解列表了。只不过在你的购物表上，可能每样东西都独自占有一行，而在Python中，你在每个项目之间用逗号分割。列表中的项目应该包括在方括号中，这样Python就知道你是在指明一个列表。一旦你创建了一个列表，你可以添加、删除或是搜索列表中的项目。由于你可以增加或删除项目，我们说列表是 可变的 数据类型，即这种类型是可以被改变的。 eggs:1234567891011121314151617181920212223242526272829#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: using_list.pylikelist = ['apple', 'mango', 'pikachu', 'mo']print 'I have', len(likelist), 'Just do.'print 'These things include:',for eachone in likelist: print eachone,print '\\nI just like live with her.'likelist.append('Arvon')print 'My life now is include:', likelistprint 'I want sort mylist now'likelist.sort()print 'Have done, Now the list is:', likelistprint 'The first item now is', likelist[0]oldlist = likelist[0]del likelist[0]print 'I am', oldlistprint 'My list now is:', likelist##[root@Mo arvon_python]# python useing_list.pyI have 4 Just do.These things include: apple mango pikachu moI just like live with her.My life now is include: ['apple', 'mango', 'pikachu', 'mo', 'Arvon']I want sort mylist nowHave done, Now the list is: ['Arvon', 'apple', 'mango', 'mo', 'pikachu']The first item now is ArvonI am ArvonMy list now is: ['apple', 'mango', 'mo', 'pikachu'] 对象和类简介 列表是使用对象和类的一个例子。当你使用变量i并给它赋值的时候，比如赋整数5，你可以认为你创建了一个类（类型）int的对象（实例）i。事实上，你可以看一下help(int)以更好地理解这一点。类也有方法，即仅仅为类而定义地函数。仅仅在你有一个该类的对象的时候，你才可以使用这些功能。例如，Python为list类提供了append方法，这个方法让你在列表尾添加一个项目。例如mylist.append(‘an item’)列表mylist中增加那个字符串。注意，使用点号来使用对象的方法。一个类也有域，它是仅仅为类而定义的变量。仅仅在你有一个该类的对象的时候，你才可以使用这些变量/名称。类也通过点号使用，例如mylist.field。 元组 元组和列表十分类似，只不过元组和字符串一样是 不可变的 即你不能修改元组。元组通过圆括号中用逗号分割的项目定义。元组通常用在使语句或用户定义的函数能够安全地采用一组值的时候，即被使用的元组的值不会改变。12345678910111213141516#!/usr/bin/python#-*- coding: utf-8 -*-home = ('Mo', 'Arvon', 'Pikachu')print 'Numble of home is', len(home)ourhome = ('Mo', 'Arvon', 'SmallArvon')print 'Number of ourhome is', len(ourhome)print 'All member in ourhome are', ourhomeprint 'come here in later', ourhome[2]print 'Last member is', ourhome[2][2]### Outputroot@Mo arvon_python]# python using_tuple.pyNumble of home is 3Number of ourhome is 3All member in ourhome are ('Mo', 'Arvon', 'SmallArvon')come here in later SmallArvonLast member is a 元组与打印语句123456789101112131415#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: print_tuple.pyage = 23name = 'Arvon'print '%s is %d years lold' % (name, age)#print语句可以使用跟着%符号的项目元组的字符串。这些字符串具备定制的功能。定制让输出#满足某种特定的格式。定制可以是%s表示字符串或%d表示整数。元组必须按照相同的顺序来#对应这些定制。print 'Why is %s playing with that python?' % name###Output[root@Mo arvon_python]# python print_tuple.pyArvon is 23 years loldWhy is Arvon playing with that python? ###字典 只能使用不可变的对象（比如字符串）来作为字典的键，但是你可以把不可变或可变的对象作为字典的值。基本说来就是，你应该只使用简单的对象作为键。 键值对在字典中以这样的方式标记：d = {key1 : value1, key2 : value2 }。注意它们的键/值对用冒号分割，而各个对用逗号分割，所有这些都包括在花括号中。 字典中的键/值对是没有顺序的。如果你想要一个特定的顺序，那么你应该在使用前自己对它们排序。 字典是dict类的实例/对象。 ####使用字典实例123456789101112131415161718#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: script's name is using_dict.pyad = &#123;'arvon':'126@126.com', 'Mo':'Mo@mo.com', 'Pikachu':'pikachu@love.com', 'marry':'love@you.com' &#125;print \"Arvon's email is %s\" % ad['arvon']#Adding a key/value pairad['life'] = 'travl@world.com'#Deleting a key/value pairdel ad['marry']print '\\n There are %d contacts in the address-book\\n' % len(ad)for name,address in ad.items(): print 'Contact %s at %s' % (name, address)if 'life' in ad: print \"\\n life's address is %s\" % ad['life'] 1234567891011[root@Mo arvon_python]# python using_dict.pyArvon's email is 126@126.com There are 4 contacts in the address-bookContact Mo at Mo@mo.comContact arvon at 126@126.comContact life at travl@world.comContact Pikachu at pikachu@love.com life's address is travl@world.com ###序列1234567891011121314151617181920212223#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: The script's name is seq.py#shoplist = ['apple', 'mango', 'carrot', 'banana']#print 'Item 0 is', shoplist[0]print 'Item 1 is', shoplist[1]print 'Item 2 is', shoplist[2]print 'Item 3 is', shoplist[3]print 'Item -1 is', shoplist[-1]print 'Item -2 is', shoplist[-2]# Slicing on a listprint 'Item 1 to 3 is', shoplist[1:3]print 'Item 2 to end is', shoplist[2:]print 'Item 1 to -1 is', shoplist[1:-1]print 'Item start to end is', shoplist[:]# Slicing on a stringname = 'swaroop'print 'characters 1 to 3 is', name[1:3]print 'characters 2 to end is', name[2:]print 'characters 1 to -1 is', name[1:-1]print 'characters start to end is', name[:] 输出：123456789101112131415[root@Mo arvon_python]# python seq.pyItem 0 is appleItem 1 is mangoItem 2 is carrotItem 3 is bananaItem -1 is bananaItem -2 is carrotItem 1 to 3 is ['mango', 'carrot']Item 2 to end is ['carrot', 'banana']Item 1 to -1 is ['mango', 'carrot']Item start to end is ['apple', 'mango', 'carrot', 'banana']characters 1 to 3 is wacharacters 2 to end is aroopcharacters 1 to -1 is waroocharacters start to end is swaroop ###引用 当你创建一个对象并给它赋一个变量的时候，这个变量仅仅 引用 那个对象，而不是表示这个对象本身！也就是说，变量名指向你计算机中存储那个对象的内存。这被称作名称到对象的绑定。 1234567891011121314151617#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: refernce.pyprint 'Simple Assignment'shoplist = ['apple', 'mango', 'carrot', 'banana']mylist = shoplist # mylist is just another name pointing to the same object!del shoplist[0]print 'shoplist is', shoplistprint 'mylist is', mylist# notice that both shoplist and mylist both print the same list without# the 'apple' confirming that they point to the same objectprint 'Copy by making a full slice'mylist = shoplist[:] # make a copy by doing a full slicedel mylist[0] # remove first itemprint 'shoplist is', shoplistprint 'mylist is', mylist# notice that now the two lists are different Output:1234567[root@Mo arvon_python]# python referen.pySimple Assignmentshoplist is ['mango', 'carrot', 'banana']mylist is ['mango', 'carrot', 'banana']Copy by making a full sliceshoplist is ['mango', 'carrot', 'banana']mylist is ['carrot', 'banana'] ##编写脚本实例 ###备份脚本Question： 需要备份的文件和目录由一个列表指定。 备份应该保存在主备份目录中。 文件备份成一个zip文件。 zip存档的名称是当前的日期和时间。 我们使用标准的zip命令，它通常默认地随Linux/Unix发行版提供。Windows用户可以使用Info-Zip程序。注意你可以使用任何地存档命令，只要它有命令行界面就可以了，那样的话我们可以从我们的脚本中传递参数给它。 ####Answer_1123456789101112131415161718192021#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: backup_verl.pyimport osimport time# 1. The files and directories to be backed up are specified in a list.#source = ['/home/swaroop/byte', '/home/swaroop/bin']source = [ '/etc']# If you are using Windows, use source = [r'C:\\Documents', r'D:\\Work'] or something like that# 2. The backup must be stored in a main backup directorytarget_dir = '/mnt/e/backup/' # Remember to change this to what you will be using# 3. The files are backed up into a zip file.# 4. The name of the zip archive is the current date and timetarget = target_dir + time.strftime('%Y%m%d%H%M%S') + '.zip'# 5. We use the zip command (in Unix/Linux) to put the files in a zip archivezip_command = \"zip -qr '%s' %s\" % (target, ' '.join(source))# Run the backupif os.system(zip_command) == 0: print 'Successful backup to', targetelse: print 'Backup FAILED' ####Answer_2 注意os.sep变量的用法——这会根据你的操作系统给出目录分隔符，即在Linux、Unix下它是’/‘，在Windows下它是’\\‘，而在Mac OS下它是’:’。使用os.sep而非直接使用字符，会使我们的程序具有移植性，可以在上述这些系统下工作。12345678910111213141516171819202122232425262728#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: backup_ver2.pyimport osimport time#1.The files and directoryes to be backed up are specified in a list.source = ['/etc/fstab', 'etc']#if you are using Windows, use source = [r'C:\\Documents',r'D:\\work']#2.The backup must be stored in a main backup directorytarget_dir = '/mnt/'#3.The files are backed up into a zip file.#4.The current_dir + time.strftime('%Y%m%d')today = target_dir + time.strftime('%Y%m%d')#The current time is the name of the zip archivenow = time.strftime('%H%M%S')#Create the subdirectory if it isn't already threreif not os.path.exists(today): os.mkdir(today) print 'Successfully creatd directory', today#The name of the zip filetarget = today + os.sep + now + '.zip'#5.We use the zip command(in Unix/Linux) to put the files in a zip archivezip_command = \"zip -qr '%s' %s\" % (target, ' '.join(source))#Run the backupif os.system(zip_command) == 0: print 'Successful backup to', targetelse: print 'Backup FAILED' ####Answer_3123456789101112131415161718192021222324#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: backup_ver3.pyimport osimport time#1.The files and directories to be backed up are specified in a list.source = ['/etc/', '/etc/fstab']target_dir = '/mnt/'today = target_dir + time.strftime('%Y%m%d')now = time.strftime('%H%M%S')comment = raw_input(\"Enter a commnet--&gt;\")if len(comment) == 0:#check if a comment was entered target = today + os.sep + now + '.zip'else: target = today + os.sep + now + '_' +\\ comment.replace(' ', ' ') + '.zip'if not os.path.exists(today): os.mkdir(today) print 'Successfully created directory', todayzip_command = \"zip -qr '%s' %s\" % (target, ' '.join(source))if os.system(zip_command) == 0: print \"Successfule backup to\", targetelse: print 'Backup FAILED' ##面向对象的编程 ###面向对象介绍到目前为止，在我们的程序中，我们都是根据操作数据的函数或语句块来设计程序的。这被称为面向过程的编程。还有一种把数据和功能结合起来，用称为对象的东西包裹起来组织程序的方法。这种方法称为面向对象的编程理念。在大多数时候你可以使用过程性编程，但是有些时候当你想要编写大型程序或是寻求一个更加合适的解决方案的时候，你就得使用面向对象的编程技术。类和对象是面向对象编程的两个主要方面。类创建一个新类型，而对象是这个类的实例 。这类似于你有一个int类型的变量，存储整数的变量是int类的实例（对象）。给C/C++/Java/C#程序员的注释注意，即便是整数也被作为对象（属于int类）。这和C++、Java（1.5版之前）把整数纯粹作为类型是不同的。通过help(int)了解更多这个类的详情。 C#和Java 1.5程序员会熟悉这个概念，因为它类似与 封装与解封装 的概念。对象可以使用普通的 属于 对象的变量存储数据。属于一个对象或类的变量被称为域。对象也可以使用 属于 类的函数来具有功能。这样的函数被称为类的方法。这些术语帮助我们把它们与孤立的函数和变量区分开来。域和方法可以合称为类的属性。域有两种类型——属于每个实例/类的对象或属于类本身。它们分别被称为实例变量和类变量。类使用class关键字创建。类的域和方法被列在一个缩进块中。 ###self类的方法与普通的函数只有一个特别的区别——它们必须有一个额外的第一个参数名称，但是在调用这个方法的时候你不为这个参数赋值，Python会提供这个值。这个特别的变量指对象本身，按照惯例它的名称是self。虽然你可以给这个参数任何名称，但是 强烈建议 你使用self这个名称——其他名称都是不赞成你使用的。使用一个标准的名称有很多优点——你的程序读者可以迅速识别它，如果使用self的话，还有些IDE（集成开发环境）也可以帮助你。给C++/Java/C#程序员的注释Python中的self等价于C++中的self指针和Java、C#中的this参考。你一定很奇怪Python如何给self赋值以及为何你不需要给它赋值。举一个例子会使此变得清晰。假如你有一个类称为MyClass和这个类的一个实例MyObject。当你调用这个对象的方法MyObject.method(arg1, arg2)的时候，这会由Python自动转为MyClass.method(MyObject, arg1,arg2)——这就是self的原理了。这也意味着如果你有一个不需要参数的方法，你还是得给这个方法定义一个self参数。 ###类 ####创建一个类1234567891011#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: simplest_class.pyclass Person: pass#Anempty blockp = Person()print p#**输出：**[root@Mo arvon_python]# python simplestclass.py&lt;__main__.Person instance at 0x7fdec3490d88&gt; 说明：我们使用class语句后跟类名，创建了一个新的类。这后面跟着一个缩进的语句块形成类体。在这个例子中，我们使用了一个空白块，它由pass语句表示。我们简单地打印了这个变量的类型。它告诉我们我们已经在main模块中有了一个Person类的实例。可以注意到存储对象的计算机内存地址也打印了出来。这个地址在你的计算机上会是另外一个值，因为Python可以在任何空位存储对象。 ###对象的方法 ####使用对象的方法12345678910111213#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: method.pyclass Person: def sayHi(self): print 'Hello, how are you?'p = Person()p.sayHi()#This short example can also be written as Person().sayHi()###Output[root@Mo arvon_python]# python method.pyHello, how are you? ###使用init方法 init方法在类的一个对象被建立时，马上运行。这个方法可以用来对你的对象做一些你希望的 初始化 。注意，这个名称的开始和结尾都是双下划线。 123456789101112131415#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: class_init.pyclass Person: def __init__(self, name): self.name = name def sayHi(self): print 'Hello, my name is', self.namep = Person('Arvon')p.sayHi()#This short example can also be written as Person('Arvon').sayHi()###Output[root@Mo arvon_python]# python class_init.pyHello, my name is Arvon ###类与对象的方法 事实上，类与对象的数据部分只是与类和对象的名称空间绑定的普通变量，即这些名称只在这些类与对象的前提下有效。有两种类型的域 ——类的变量和对象的变量，它们根据是类还是对象拥有这个变量而区分。类的变量由一个类的所有对象（实例）共享使用。只有一个类变量的拷贝，所以当某个对象对类的变量做了改动的时候，这个改动会反映到所有其他的实例上。对象的变量由类的每个对象/实例拥有。因此每个对象有自己对这个域的一份拷贝，即它们不是共享的，在同一个类的不同实例中，虽然对象的变量有相同的名称，但是是互不相关的。通过一个例子会使这个易于理解。 12345678910111213141516171819202122232425262728293031323334353637383940414243#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: obj_var.pyclass Person: '''Represents a person.''' population = 0 def __init__(self, name): '''INitializes the person's data.''' self.name = name print '(Initializing %s)' % self.name #When this person is created, he/she #adds to population Person.population += 1 def __del__(self): '''I am dying.''' print '%s says bye.'% self.name Person.population == 1 if Person.population == 0: print 'I am the lase one.' else: print 'These are still %d people left.'% Person.population def sayHi(self): '''Greeting by the person. Really, that's all it does.''' print 'Hi, my name is %s.'% self.name def howMany(self): '''Prints the curent population.''' if Person.population == 1: print 'I am the only person here.' else: print \"We have %d persons here.\"% Person.populationswaroop = Person('Arvon')swaroop.sayHi()swaroop.howMany()###Output[root@Mo arvon_python]# python obj_var.py(Initializing Arvon)Hi, my name is Arvon.I am the only person here.Arvon says bye.These are still 1 people left. How to work:这是一个很长的例子，但是它有助于说明类与对象的变量的本质。这里，population属于Person类，因此是一个类的变量。name变量属于对象（它使用self赋值）因此是对象的变量。观察可以发现init方法用一个名字来初始化Person实例。在这个方法中，我们让population增加1，这是因为我们增加了一个人。同样可以发现，self.name的值根据每个对象指定，这表明了它作为对象的变量的本质。记住，你只能使用self变量来参考同一个对象的变量和方法。这被称为 属性参考 。在这个程序中，我们还看到docstring对于类和方法同样有用。我们可以在运行时使用Person.doc和Person.sayHi.doc来分别访问类与方法的文档字符串。就如同init方法一样，还有一个特殊的方法del，它在对象消逝的时候被调用。对象消逝即对象不再被使用，它所占用的内存将返回给系统作它用。在这个方法里面，我们只是简单地把Person.population减1。当对象不再被使用时，del方法运行，但是很难保证这个方法究竟在 什么时候 运行。如果你想要指明它的运行，你就得使用del语句，就如同我们在以前的例子中使用的那样。给C++/Java/C#程序员的注释Python中所有的类成员（包括数据成员）都是 公共的 ，所有的方法都是 有效的 。只有一个例外：如果你使用的数据成员名称以 双下划线前缀 比如privatevar，Python的名称管理体系会有效地把它作为私有变量。这样就有一个惯例，如果某个变量只想在类或对象中使用，就应该以单下划线前缀。而其他的名称都将作为公共的，可以被其他类/对象使用。记住这只是一个惯例，并不是Python所要求的（与双下划线前缀不同）。同样，注意del__方法与 destructor 的概念类似。 ###继承面向对象的编程带来的主要好处之一是代码的重用，实现这种重用的方法之一是通过 继承 机制。继承完全可以理解成类之间的类型和子类型关系。假设你想要写一个程序来记录学校之中的教师和学生情况。他们有一些共同属性，比如姓名、年龄和地址。他们也有专有的属性，比如教师的薪水、课程和假期，学生的成绩和学费。你可以为教师和学生建立两个独立的类来处理它们，但是这样做的话，如果要增加一个新的共有属性，就意味着要在这两个独立的类中都增加这个属性。这很快就会显得不实用。一个比较好的方法是创建一个共同的类称为SchoolMember然后让教师和学生的类 继承 这个共同的类。即它们都是这个类型（类）的子类型，然后我们再为这些子类型添加专有的属性。使用这种方法有很多优点。如果我们增加/改变了SchoolMember中的任何功能，它会自动地反映到子类型之中。例如，你要为教师和学生都增加一个新的身份证域，那么你只需简单地把它加到SchoolMember类中。然而，在一个子类型之中做的改动不会影响到别的子类型。另外一个优点是你可以把教师和学生对象都作为SchoolMember对象来使用，这在某些场合特别有用，比如统计学校成员的人数。一个子类型在任何需要父类型的场合可以被替换成父类型，即对象可以被视作是父类的实例，这种现象被称为多态现象。另外，我们会发现在 重用 父类的代码的时候，我们无需在不同的类中重复它。而如果我们使用独立的类的话，我们就不得不这么做了。在上述的场合中，SchoolMember类被称为 基本类 或 超类 。而Teacher和Student类被称为 导出类 或 子类 。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: inherit.pyclass SchooMember: '''Represents any school member.''' def __init__(self, name, age): self.name = name self.age = age print '(Initialized SchooMember: %s)'% self.name def tell(self): '''Tell my details.''' print 'Name: \"%s\" Age: \"%s\"'% (self.name, self.age)class Teacher(SchooMember): '''Represents a teacher.''' def __init__(self, name, age, salary): SchooMember.__init__(self, name, age) self.salary = salary print '(Initialized Teacher: %s)'% self.name def tell(self): SchooMember.tell(self) print 'Salary: \"%d\"'% self.salaryclass Student(SchooMember): '''Represents a student.''' def __init__(self, name, age, marks): SchooMember.__init__(self, name, age) self.marks = marks print '(Initialized Student: %s)'% self.name def tell(self): SchooMember.tell(self) print 'Marks: \"%d\"'% self.markst = Teacher('Mo', 23, 5201314)s = Student('Arvon', 24, 75)print#prints a blank linemembers = [t, s]for member in members: member.tell()#works for both Teachers and Students###Output[root@Mo arvon_python]# python inherit.py(Initialized SchooMember: Mo)(Initialized Teacher: Mo)(Initialized SchooMember: Arvon)(Initialized Student: Arvon)Name: \"Mo\" Age: \"23\"Salary: \"5201314\"Name: \"Arvon\" Age: \"24\"Marks: \"75\" How tow work为了使用继承，我们把基本类的名称作为一个元组跟在定义类时的类名称之后。然后，我们注意到基本类的init方法专门使用self变量调用，这样我们就可以初始化对象的基本类部分。这一点十分重要——Python不会自动调用基本类的constructor，你得亲自专门调用它。我们还观察到我们在方法调用之前加上类名称前缀，然后把self变量及其他参数传递给它。注意，在我们使用SchoolMember类的tell方法的时候，我们把Teacher和Student的实例仅仅作为SchoolMember的实例。另外，在这个例子中，我们调用了子类型的tell方法，而不是SchoolMember类的tell方法。可以这样来理解，Python总是首先查找对应类型的方法，在这个例子中就是如此。如果它不能在导出类中找到对应的方法，它才开始到基本类中逐个查找。基本类是在类定义的时候，在元组之中指明的。一个术语的注释——如果在继承元组中列了一个以上的类，那么它就被称作 多重继承 。 ##输入/输出 ###文件 通过创建一个file类的对象来打开一个文件，分别使用file类的read、readline或write方法来恰当地读写文件。对文件的读写能力依赖于你在打开文件时指定的模式。最后，当你完成对文件的操作的时候，你调用close方法来告诉Python我们完成了对文件的使用。 ####使用文件12345678910111213141516171819202122232425262728#!/usr/bin/python#-*- coding: utf-8 -*-poem = '''\\Programming is funWhen the work is doneif you wanna make your work also fun: use Python!'''f = file('poem.txt', 'w')#open for 'w'ritingf.write(poem)#write text to filef.close()#close the filef = file('poem.txt')#if no mode is specified, 'r'ead mode is assumed by defaultwhile True: line = f.readline() if len(line) == 0:#Zero length indicatesEOF break print line, #Notice comma to avoid automatice new line added by Pythonf.close()#close the file###Output[root@Mo arvon_python]# !ppython using_file.pyProgramming is funWhen the work is doneif you wanna make your work also fun: use Python! ###储存器 Python提供一个标准的模块，称为pickle。使用它你可以在一个文件中储存任何Python对象，之后你又可以把它完整无缺地取出来。这被称为 持久地 储存对象。还有另一个模块称为cPickle，它的功能和pickle模块完全相同，只不过它是用C语言编写的，因此要快得多（比pickle快1000倍）。你可以使用它们中的任一个，而我们在这里将使用cPickle模块。记住，我们把这两个模块都简称为pickle模块。 123456789101112131415161718192021#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: pickling.pyimport cPickle as p#import pickleaspshoplistfile = 'shoplist.data'#the name of the file where we will store the objectshoplist = ['apple', 'mango', 'carrot']#Write to the filef = file(shoplistfile, 'w')p.dump(shoplist, f)#dump the object to a filef.close()del shoplist#remove the shoplist#Read back from the storagef = file(shoplistfile)storedlist = p.load(f)print storedlist###Output[root@Mo arvon_python]# python pickling.py['apple', 'mango', 'carrot'] How to work首先，请注意我们使用了import..as语法。这是一种便利方法，以便于我们可以使用更短的模块名称。在这个例子中，它还让我们能够通过简单地改变一行就切换到另一个模块（cPickle或者pickle）！在程序的其余部分的时候，我们简单地把这个模块称为p。为了在文件里储存一个对象，首先以写模式打开一个file对象，然后调用储存器模块的dump函数，把对象储存到打开的文件中。这个过程称为 储存 。接下来，我们使用pickle模块的load函数的返回来取回对象。这个过程称为 取储存 。 ##异常 ###处理异常 使用try..except语句来处理异常。我们把通常的语句放在try-块中，而把我们的错误处理语句放在except-块中。 123456789101112131415161718192021#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: try_except.pyimport systry: s = raw_input(\"Entersomething --&gt;\")except EOFError: print '\\nWhy did you do an EOF on me?' sys.exit()#exit the programexcept: print '\\nSome error/exception occurred.' #here, we are not exiting the programprint 'Done'###Output[root@Mo arvon_python]# python try_except.pyEntersomething --&gt;helloDone[root@Mo arvon_python]# python try_except.pyEntersomething --&gt;Why did you do an EOF on me? How to work我们把所有可能引发错误的语句放在try块中，然后在except从句/块中处理所有的错误和异常。except从句可以专门处理单一的错误或异常，或者一组包括在圆括号内的错误/异常。如果没有给出错误或异常的名称，它会处理 所有的 错误和异常。对于每个try从句，至少都有一个相关联的except从句。如果某个错误或异常没有被处理，默认的Python处理器就会被调用。它会终止程序的运行，并且打印一个消息，我们已经看到了这样的处理。你还可以让try..catch块关联上一个else从句。当没有异常发生的时候，else从句将被执行。我们还可以得到异常对象，从而获取更多有个这个异常的信息。 ###引发异常 可以使用raise语句 引发 异常。你还得指明错误/异常的名称和伴随异常 触发的 异常对象。你可以引发的错误或异常应该分别是一个Error或Exception类的直接或间接导出类。 12345678910111213141516171819202122232425262728293031#!/usr/bin/python#-*- coding: utf-8 -*-class ShortINputException(Exception): '''A user defined exception class.''' def __init__(self, length, atleast): Exception.__init__(self) self.length = length self.atleast = atleasttry: s = raw_input('Enter something --&gt;') if len(s) &lt; 3: raise ShortINputException(len(s), 3) #Other work can continue as usual hereexcept EOFError: print '\\nWhy did you do an EOF on me?'except ShortINputException, x: print 'ShortINputException: The Input was of length %d,\\ was expecting at least %d' % (x.length, x.atleast)else: print 'No exception was raised.'###Output[root@Mo arvon_python]# python raising.pyEnter something --&gt;Why did you do an EOF on me?[root@Mo arvon_python]# python raising.pyEnter something --&gt;aShortINputException: The Input was of length 1, was expecting at least 3[root@Mo arvon_python]# python raising.pyEnter something --&gt;abcNo exception was raised. ###Try finally 假如你在读一个文件的时候，希望在无论异常发生与否的情况下都关闭文件，该怎么做呢？这可以使用finally块来完成。注意，在一个try块下，你可以同时使用except从句和finally块。如果你要同时使用它们的话，需要把一个嵌入另外一个。 1234567891011121314151617181920212223#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: finally.pyimport timetry: f = file('poem.txt') while True:#our usual file-reading idiom(成语、习语、土话) line = f.readline() if len(line) == 0: break time.sleep(2) print line,finally: f.close() print 'Cleaning up... closed the file'###Output[root@Mo arvon_python]# python finally.pyProgramming is funWhen the work is doneif you wanna make your work also fun: use Python!Cleaning up... closed the file How to work我们进行通常的读文件工作，但是我有意在每打印一行之前用time.sleep方法暂停2秒钟。这样做的原因是让程序运行得慢一些（Python由于其本质通常运行得很快）。在程序运行的时候，按Ctrl-c中断/取消程序。我们可以观察到KeyboardInterrupt异常被触发，程序退出。但是在程序退出之前，finally从句仍然被执行，把文件关闭 ##Python的标准库 Python标准库是随Python附带安装的，它包含大量极其有用的模块。熟悉Python标准库是十分重要的，因为如果你熟悉这些库中的模块，那么你的大多数问题都可以简单快捷地使用它们来解决。 ###sys模块 sys模块包含系统对应的功能。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#!/usr/bin/python#-*- coding: utf-8 -*-#Todo: cat.pyimport sysdef readfile(filename): '''Print a file to the standard output''' f = file(filename) while True: line = f.readline() if len(line) == 0: break print line,#notice commma f.close()#Script starts from hereif len(sys.argv) &lt; 2: print 'No action specified.' sys.exit()if sys.argv[1].startswith('--'): option = sys.argv[1][2:] #fetch sys.argv[1] but without the first two characters if option == 'version': print 'Version 1.2' elif option == 'help': print '''\\This program prints files to the standard output.Any number of files can be specified.Options include: --version : Prints the version number --help : Display this help''' else: print 'Unknown option.' sys.exit()else: for filename in sys.argv[1:]: readfile(filename)###Output[root@Mo arvon_python]# python cat.py --helpThis program prints files to the standard output.Any number of files can be specified.Options include: --version : Prints the version number --help : Display this help[root@Mo arvon_python]# python cat.py --versionVersion 1.2[root@Mo arvon_python]# python cat.py --loveUnknown option.[root@Mo arvon_python]# python cat.py poem.txtProgramming is funWhen the work is doneif you wanna make your work also fun: use Python! ###OS模块 简略● os.name字符串指示你正在使用的平台。比如对于Windows，它是’nt’，而对于Linux/Unix用户，它是’posix’。● os.getcwd()函数得到当前工作目录，即当前Python脚本工作的目录路径。● os.getenv()和os.putenv()函数分别用来读取和设置环境变量。● os.listdir()返回指定目录下的所有文件和目录名。● os.remove()函数用来删除一个文件。● os.system()函数用来运行shell命令。● os.linesep字符串给出当前平台使用的行终止符。例如，Windows使用’\\r\\n’，Linux使用’\\n’而Mac使用’\\r’。● os.path.split()函数返回一个路径的目录名和文件名。os.path.split(‘/home/swaroop/byte/code/poem.txt’)(‘/home/swaroop/byte/code’, ‘poem.txt’)● os.path.isfile()和os.path.isdir()函数分别检验给出的路径是一个文件还是目录。类似地，os.path.exists()函数用来检验给出的路径是否真地存在。 ##参考 ###特殊的方法 init(self, …)这个方法在新建对象恰好要被返回使用之前被调用 del(self)恰好在对象要被删除之前调用 str(self, other)当使用小于运算符（&lt;）的时候调用。类似地，对于说有的运算符（+， &gt;等等）都有特殊的方法 getitem(self, key)使用x[key]索引操作符的时候调用 len(self)对序列对象使用内建的len（）函数的时候调用 ###参考书籍Python标准文档（英文）Python实用大全（英文）Python常用类库（Blog）Python类库手册（中文）Python官方文档（中文）","categories":[],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://arvon.top/tags/学习笔记/"}]},{"title":"awk用法实例","slug":"awk用法实例","date":"2015-07-29T02:45:32.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/07/29/awk用法实例/","link":"","permalink":"http://arvon.top/2015/07/29/awk用法实例/","excerpt":"简介：awk一种优秀的文本处理工具，Linux和Unix环境中现有的功能最强大的数据处理引擎之一。这个编程及数据操作语言其名称得自于它的创始人阿尔佛雷德·艾侯、彼得·温伯格和布莱恩·柯林汉姓氏的首个字母。awk的处理文本和数据的方式是这样的，它逐行扫描文件，从第一行到最后一行，寻找匹配的特定模式的行，并在这些行上进行你想要的操作。如果没有指定处理动作，则把匹配的行显示到标准输出(屏幕)，如果没有指定模式，则所有被操作所指定的行都被处理，awk有3个不同版本: awk、nawk和gawk，未作特别说明，一般指gawk，gawk 是 AWK 的 GNU 版本。","text":"简介：awk一种优秀的文本处理工具，Linux和Unix环境中现有的功能最强大的数据处理引擎之一。这个编程及数据操作语言其名称得自于它的创始人阿尔佛雷德·艾侯、彼得·温伯格和布莱恩·柯林汉姓氏的首个字母。awk的处理文本和数据的方式是这样的，它逐行扫描文件，从第一行到最后一行，寻找匹配的特定模式的行，并在这些行上进行你想要的操作。如果没有指定处理动作，则把匹配的行显示到标准输出(屏幕)，如果没有指定模式，则所有被操作所指定的行都被处理，awk有3个不同版本: awk、nawk和gawk，未作特别说明，一般指gawk，gawk 是 AWK 的 GNU 版本。 简单使用方法 使用方法： 1awk '&#123;pattern + action&#125;' &#123;filenames&#125; 尽管操作可能会很复杂，但语法总是这样，其中 pattern 表示 AWK 在数据中查找的内容，而 action 是在找到匹配内容时所执行的一系列命令。花括号（{}）不需要在程序中始终出现，但它们用于根据特定的模式对一系列指令进行分组。 pattern就是要表示的正则表达式，用斜杠括起来。awk语言的最基本功能是在文件或者字符串中基于指定规则浏览和抽取信息，awk抽取信息后，才能进行其他文本操作。完整的awk脚本通常用来格式化文本文件中的信息。通常，awk是以文件的一行为处理单位的。awk每接收文件的一行，然后执行相应的命令，来处理文本。 三种调用方式12345671. 命令行方式#tail /etc/fstab |awk '&#123;print $1&#125;'#cat /etc/passwd |awk -F ':' '&#123;print $1&#125;'#cat /etc/passwd |awk -F ':' '&#123;print $1\"\\t\"$7&#125;'2. shell脚本方式3. awk命令插入为单独文件的方式#awk -f awk-script-file input-file(s) 大实例 显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以逗号分割,而且在所有行添加列名name,shell,在最后一行添加”blue,/bin/nosh”。 12345678cat /etc/passwd |awk -F ':' 'BEGIN &#123;print \"name,shell\"&#125; &#123;print $1\",\"$7&#125; END &#123;print \"blue,/bin/nosh\"&#125;'name,shellroot,/bin/bashdaemon,/bin/shbin,/bin/shsys,/bin/sh....blue,/bin/nosh 搜索/etc/passwd有root关键字的所有行 12345678910#awk -F: '/root/' /etc/passwdroot:x:0:0:root:/root:/bin/bash``` * 统计/etc/passwd:文件名，每行的行号，每行的列数，对应的完整行内容:```perl#awk -F ':' '&#123;print \"filename:\" FILENAME \",linenumber:\" NR \",columns:\" NF \",linecontent:\"$0&#125;' /etc/passwdfilename:/etc/passwd,linenumber:1,columns:7,linecontent:root:x:0:0:root:/root:/bin/bashfilename:/etc/passwd,linenumber:2,columns:7,linecontent:daemon:x:1:1:daemon:/usr/sbin:/bin/shfilename:/etc/passwd,linenumber:3,columns:7,linecontent:bin:x:2:2:bin:/bin:/bin/shfilename:/etc/passwd,linenumber:4,columns:7,linecontent:sys:x:3:3:sys:/dev:/bin/sh 使用printf替代print,可以让代码更加简洁，易读 1awk -F ':' '&#123;printf(\"filename:%10s,linenumber:%s,columns:%s,linecontent:%s\\n\",FILENAME,NR,NF,$0)&#125;' /etc/passwd 参考这里awk官网awk学习笔记awk手册参考博客","categories":[],"tags":[{"name":"正则","slug":"正则","permalink":"http://arvon.top/tags/正则/"}]},{"title":"SA运维资源","slug":"SA运维资源","date":"2015-07-28T08:07:50.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/07/28/SA运维资源/","link":"","permalink":"http://arvon.top/2015/07/28/SA运维资源/","excerpt":"运维是个杂学呀~~","text":"运维是个杂学呀~~ 需学习技能 系统管理语言 Shell脚本语言 perl python 监控工具 zabbix nagios 运维自动化工具 saltstack Ansible puppet 大数据 云计算 OPenStack Docker 虚拟化 KVM VMware Linux系统技能要求 Keepalived、lvs高可用及负载均衡技术 Mysql 正则表达式 sed awk 常用协议（SSH、telnet、rlogin、RDP、VNC） 熟悉Kernel调优 Web服务 appache nginx ###资源网站系统运维之家兴趣看看中文Linux社区英文版参考学习主站","categories":[],"tags":[{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"}]},{"title":"Linux下搭建VSFTP服务器","slug":"Linux下搭建vsftp服务器","date":"2015-07-28T07:04:06.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/07/28/Linux下搭建vsftp服务器/","link":"","permalink":"http://arvon.top/2015/07/28/Linux下搭建vsftp服务器/","excerpt":"简介：VSFTP（Very Secure FTP）是一个基于GPL发布的类Unix系统上使用的FTP服务器软件，设计者的初衷是代码的安全，另外速度在使用ASCII代码的模式下在数据时，VSFTP的速度为Wu-FTp的两倍，如果Linux主机使用2.4.x的内核，在千兆以太网的下的下载速度可以达到86MB/S。而且很稳定，在单机上支持4000以上的并发用户同时连接，根据RedHat和Ftp服务器的数据，VSFTP服务器可以支持15000个并发用户","text":"简介：VSFTP（Very Secure FTP）是一个基于GPL发布的类Unix系统上使用的FTP服务器软件，设计者的初衷是代码的安全，另外速度在使用ASCII代码的模式下在数据时，VSFTP的速度为Wu-FTp的两倍，如果Linux主机使用2.4.x的内核，在千兆以太网的下的下载速度可以达到86MB/S。而且很稳定，在单机上支持4000以上的并发用户同时连接，根据RedHat和Ftp服务器的数据，VSFTP服务器可以支持15000个并发用户 VSFTP的优点 它是一个安全、高速、稳定的FTP服务器； 它可以做基于多个IP的虚拟FTP主机服务器； 匿名服务设置十分方便； 匿名FTP的根目录不需要任何特殊的目录结构，或系统程序或其它的系统文件； 不执行任何外部程序，从而减少了安全隐患； 支持虚拟用户，并且每个虚拟用户可以具有独立的属性配置； 可以设置从inetd中启动，或者独立的FTP服务器两种运行方式； 支持两种认证方式（PAP或xinetd/ tcp_wrappers）； 支持带宽限制；VSFTP市场应用十分广范，很多国际性的大公司和自由开源组织在使用，如：Red Hat, Suse，Debian，OpenBSD VSFTP的缺点 VSFTP从没有处理文件编码转换的windows客户端访问时有乱码现象 解决方法： 使用vsftp的补丁 转换服务器目录文件编码 安装VSFTP 检查是否已安装vsftp服务 12[root@Mo ~]# rpm -q vsftpdpackage vsftpd is not installed 安装vsftp服务 1234[root@Mo ~]# yum install -y vsftpd db4-utils #安装vsftpd服务[root@Mo ~]# rpm -q vsftpd #vsftpd服务已安装vsftpd-2.2.2-13.el6_6.1.x86_64[root@Mo vsftp]# yum install lrzsz #安装后可以把windows的文件直接通过xshell拉到目标主机 vsftp配置文件 官方wiki有处理脚本：脚本地址 将脚本放至linux主机，然后执行该脚本","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"}]},{"title":"Linux查看系统信息命令","slug":"Linux系统查看命令","date":"2015-07-27T02:49:08.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/07/27/Linux系统查看命令/","link":"","permalink":"http://arvon.top/2015/07/27/Linux系统查看命令/","excerpt":"Command Include：disk、memory、hostname、find","text":"Command Include：disk、memory、hostname、find 查看磁盘空间 常用命令df -hl 查看磁盘剩余空间df -h 查看每个根路径的分区大小du -sh [目录名] 返回该目录的大小du -sm [文件夹] 返回该文件夹总M数 命令演示123456789101112131415161718[root@cloud ~]# cat /proc/cpuinfo #查看cpu的核心数[root@cloud ~]# df -h #磁盘使用情况[root@cloud ~]# df -hlFilesystem Size Used Avail Use% Mounted on/dev/sda1 9.1G 2.2G 6.4G 26% /tmpfs 487M 0 487M 0% /dev/shm/dev/sdb1 4.5G 1.9M 4.3G 1% /data[root@cloud ~]# du --max-depth=1 -h #查看当前文件夹下的磁盘使用情况25M ./.nvm8.0K ./.pki4.0K ./Blog25M .[root@cloud ~]# du -sh #显示当前目录的大小25M .[root@cloud ~]# du -sh /opt/ #显示指定路径的大小4.0K /opt/[root@cloud ~]# du -h test.txt #显示指定文件的大小0 test.txt 查看内存 常用命令cat /proc/meminfo #查看内存使用信息 命令实例空闲内存 =free+buffers+cached =total-used1234567891011121314151617[root@cloud ~]# free -m #查看内存使用状况 total used free shared buffers cachedMem: 972 667 304 0 41 497-/+ buffers/cache: 128 843Swap: 999 0 999说明： shared：多个进程共享的内存总额 Buffers/cached [root@cloud ~]# cat /proc/meminfoMemTotal: 995648 kBMemFree: 312228 kBBuffers: 42836 kBCached: 509164 kBSwapCached: 352 kBActive: 413060 kBInactive: 145136 kB [root@cloud ~]# ll -h /proc/kcore #查看/proc/kcore文件的大小（内存镜像）-r-------- 1 root root 4.0K Jul 27 16:31 /proc/kcore 查看cpu使用率 使用top命令具体介绍：top命输出解释 产看内核信息 12[root@cloud ~]# uname -aLinux cloud 2.6.32-504.1.3.el6.x86_64 #1 SMP Tue Nov 11 17:57:25 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux 查看发行版信息 123[root@cloud ~]# cat /etc/issueCentOS release 6.6 (Final)Kernel \\r on an \\m Find命令 命令实例 1234567891011[root@cloud ~]# find / -name fstab/etc/fstab[root@cloud ~]# find / -name access_log 2&gt;/dev/null #无错误查找[root@cloud ~]# find / etc -name '*fst*' #模糊查找，支持通配符/lib/udev/fstab_import/lib/udev/rules.d/79-fstab_import.rules/etc/fstab/usr/bin/psfstriptablefind /home -size +512k #查大于512k的文档$ find logs -type f -mtime +5 -exec -ok rm &#123;&#125; \\; #在/ l o g s目录中查找更改时间在5日以前的文档并删除他们：# A=`find ./ -name \"*php\"` | ls -l --full-time $A 2&gt;/dev/null | grep \"2004-11-30 16:36:37\" #查找2004-11-30 16:36:37时更改过的文档 参考网址参考实例与或非的find介绍 修改常见配置 修改主机名参考这里:[北南南北的博客，修改Linux主机名]1234[root@cloud ~]# hostname Mo #临时修改hostname[root@Mo ~]# vim /etc/sysconfig/network #两个都要改[root@Mo ~]# vim /etc/hosts[root@linuxsir01 ~]# hostname -i #显示主机名的IP 以上","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"}]},{"title":"mysql自用命令手册","slug":"mysql自用命令手册","date":"2015-07-22T08:10:15.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/07/22/mysql自用命令手册/","link":"","permalink":"http://arvon.top/2015/07/22/mysql自用命令手册/","excerpt":"连接登录数据库12345678mysql –u用户名 [–h主机名或者IP地址] –p密码eggs: mysql -uusername -hIPaddress -p(password)#1. 连接到本机上的MYSQL。#首先打开DOS窗口，然后进入目录mysql\\bin，再键入命令mysql -u root -p，回车后提示你输密码.注意用户名前可以有空格也可以没有空格，但是密码前必须没有空格，否则让你重新输入密码.#如果刚安装好MYSQL，超级用户root是没有密码的，故直接回车即可进入到MYSQL中了，MYSQL的提示符是： mysql&gt;#2. 连接到远程主机上的MYSQL。假设远程主机的IP为：110.110.110.110，用户名为root,密码为abcd123。则键入以下命令：#mysql -h110.110.110.110 -u root -p 123;（注:u与root之间可以不用加空格，其它也一样）#3. 退出MYSQL命令： exit （回车） 或者 \\q","text":"连接登录数据库12345678mysql –u用户名 [–h主机名或者IP地址] –p密码eggs: mysql -uusername -hIPaddress -p(password)#1. 连接到本机上的MYSQL。#首先打开DOS窗口，然后进入目录mysql\\bin，再键入命令mysql -u root -p，回车后提示你输密码.注意用户名前可以有空格也可以没有空格，但是密码前必须没有空格，否则让你重新输入密码.#如果刚安装好MYSQL，超级用户root是没有密码的，故直接回车即可进入到MYSQL中了，MYSQL的提示符是： mysql&gt;#2. 连接到远程主机上的MYSQL。假设远程主机的IP为：110.110.110.110，用户名为root,密码为abcd123。则键入以下命令：#mysql -h110.110.110.110 -u root -p 123;（注:u与root之间可以不用加空格，其它也一样）#3. 退出MYSQL命令： exit （回车） 或者 \\q 修改密码1234#**格式：**mysqladmin -u用户名 -p旧密码 password 新密码(注：因为开始时root没有密码，所以-p旧密码一项就可以省略)mysqladmin -u root password ab12再将root的密码改为djg345mysqladmin -u root -p ab12 password djg345 查看mysql数据库中所有用户及拥有的权限1SELECT DISTINCT CONCAT('User: ''',user,'''@''',host,''';') AS query FROM mysql.user; 添加新用户12345678#**格式：**grant select on 数据库.* to 用户名@登录主机 identified by “密码”#1.增加一个用户test1密码为abc，让他可以在任何主机上登录，并对所有数据库有查询、插入、修改、删除的权限。首先用root用户连入MYSQL，然后键入以下命令：grant select,insert,update,delete on *.* to test1@”%” Identified by “abc”;#2.增加一个用户test2密码为abc,让他只可以在localhost上登录，并可以对数据库mydb进行查询、插入、修改、删除的操作（localhost指本地主机，即MYSQL数据库所在的那台主机），这样用户即使用知道test2的密码，他也无法从internet上直接访问数据库，只能通过MYSQL主机上的web页来访问了。#grant select,insert,update,delete on mydb.* to test2@localhost identified by “abc”;#如果你不想test2有密码，可以再打一个命令将密码消掉:grant select,insert,update,delete on mydb.* to test2@localhost identified by “”; 删除用户123mysql&gt;Delete FROM user Where User='test' and Host='localhost';mysql&gt;flush privileges;mysql&gt;drop database testDB; //删除用户的数据库 查看命令1234567891011121314151617181920mysql&gt; SHOW DATABASES; #显示当前数据库服务器中的数据库列表(注意：mysql库里面有MYSQL的系统信息，我们改密码和新增用户，实际上就是用这个库进行操作。)mysql&gt; USE 库名； ##显示数据库中的数据表mysql&gt; SHOW TABLES;mysql&gt; DESCRIBE 表名; #显示数据表的结构mysql&gt; CREATE DATABASE 库名; #建立数据库mysql&gt; USE 库名; #;建立数据表mysql&gt; CREATE TABLE 表名 (字段名 VARCHAR(20), 字段名 CHAR(1))mysql&gt; DROP DATABASE #库名;删除数据库mysql&gt; DROP TABLE 表名； #删除数据表mysql&gt; DELETE FROM 表名; #将表中记录清空mysql&gt; SELECT * FROM 表名; #显示表中的记录mysql&gt; INSERT INTO 表名 VALUES (”hyq”,”M”); #往表中插入记录mysql&gt; UPDATE 表名 SET 字段名1='a',字段名2='b' WHERE 字段名3='c'; #更新表中数据mysql&gt; LOAD DATA LOCAL INFILE “D:/mysql.txt” INTO TABLE 表名; #用文本方式将数据装入数据表中mysql&gt; USE 数据库名;mysql&gt; SOURCE d:/mysql.sql; #导入.sql文件命令：mysql&gt; UPDATE mysql.user SET password=PASSWORD('新密码') WHERE User='root';mysql&gt; FLUSH PRIVILEGES; #命令行修改root密码mysql&gt; SELECT DATABASE(); #显示use的数据库名mysql&gt; SELECT USER(); #显示当前的user 命令实例123456789101112drop database if exists school; #如果存在SCHOOL则删除create database school; #建立库SCHOOLuse school; #打开库SCHOOLcreate table teacher; #(进入SCHOOL库后)简历表teacher ( id int(3) auto_increment not null primary key, name char(10) not null, address varchar(50) default ‘深圳', year date ); #建表结束insert into teacher values(”,'allen','大连一中','1976-10-10′); #插入字段insert into teacher values(”,'jack','大连二中','1975-12-23′); 导出数据库1234567891011#1. 导出整个数据库（导出文件默认是存在mysql\\bin目录下；在linux下为/var/lib/mysql）mysqldump -u 用户名 -p 数据库名 &gt; 导出的文件名mysqldump -u user_name -p123456 database_name &gt; outfile_name.sql#2. 导出一个表mysqldump -u 用户名 -p 数据库名 表名&gt; 导出的文件名mysqldump -u user_name -p database_name table_name &gt; outfile_name.sql#3. 导出一个数据库结构mysqldump -u user_name -p -d –add-drop-table database_name &gt; outfile_name.sql#-d 没有数据 –add-drop-table 在每个create语句之前增加一个drop table#4. 带语言参数导出mysqldump -uroot -p –default-character-set=latin1 –set-charset=gbk –skip-opt database_name &gt; outfile_name.sql 备份恢复数据库 备份数据库 1234mysqldump -uroot -p test_db &gt; test_db.sqlmysqldump -h$&#123;ipaddr&#125; -u$&#123;username&#125; -p$&#123;password&#125; --default-character-set=utf8 \\--comments=FALSE --tables --no-create-info=FALSE --add-drop-table=TRUE --no-data=FALSE \\$&#123;area&#125; | sed 's/AUTO_INCREMENT=[0-9]*\\s//g' &gt;$&#123;dest_dir&#125;/$&#123;area&#125;.sql 恢复数据库 1234mysql -uroot -p test_db &lt; test_db.sqlmysql -h$&#123;ipaddr&#125; -u$&#123;username&#125; -p$&#123;password&#125; --default-character-set=utf8 --execute=\"DROP DATABASE IF EXISTS $&#123;area&#125;\";mysql -h$&#123;ipaddr&#125; -u$&#123;username&#125; -p$&#123;password&#125; --default-character-set=utf8 --execute=\"CREATE DATABASE IF NOT EXISTS $&#123;area&#125; DEFAULT CHARACTER SET utf8\";mysql -h$&#123;ipaddr&#125; -u$&#123;username&#125; -p$&#123;password&#125; --default-character-set=utf8 --database= $&#123;area&#125; &lt;$&#123;area&#125;.sql; 创建权限 12345grant all privileges on test_db.* to test_db@'localhost' identified by '123456';#兼容4.1之前的版本：update mysql.user set password=old_password('123456') where user='test_db';#4. 忘记密码在“my.cnf”或“my.ini”文件的“mysqld”配置段添加“skip-grant-tables”，然后重新启动mysql即可登录修改root密码。 参考文档mysql常用命令集锦Mysql常用命令 详细整理版Mysql 用户权限Mysql 命令大全","categories":[],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://arvon.top/tags/Mysql/"}]},{"title":"Tmux 使用简记","slug":"Tmux-使用简记","date":"2015-07-22T01:49:22.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/07/22/Tmux-使用简记/","link":"","permalink":"http://arvon.top/2015/07/22/Tmux-使用简记/","excerpt":"简述：tmux是指通过一个终端登录远程主机并运行后，在其中可以开启多个控制台的终端复用软件。tmux是一个优秀的终端复用软件，类似GNU Screen，但来自于OpenBSD，采用BSD授权。使用它最直观的好处就是，通过一个终端登录远程主机并运行tmux后，在其中可以开启多个控制台而无需再“浪费”多余的终端来连接这台远程主机；个人更注重工作环境的连续性，可以进行会话保持(保证可以在网络情况不佳的情况下工作现场不丢失)。","text":"简述：tmux是指通过一个终端登录远程主机并运行后，在其中可以开启多个控制台的终端复用软件。tmux是一个优秀的终端复用软件，类似GNU Screen，但来自于OpenBSD，采用BSD授权。使用它最直观的好处就是，通过一个终端登录远程主机并运行tmux后，在其中可以开启多个控制台而无需再“浪费”多余的终端来连接这台远程主机；个人更注重工作环境的连续性，可以进行会话保持(保证可以在网络情况不佳的情况下工作现场不丢失)。 常用操作12345tmux new-session -s session-name #创建一个新的tmux对话tmux ls #列出已有的tmux列表，相当于Ctrl-B stmux attach-session -t sessionname = tmux a -t sessionname（支持缩写匹配） #进入一个tmux会话窗使用Ctrl-B d返回主shell界面，tmux仍旧在后台运行，里面的命令也保持运行状态tmux kill-session -t session-name #关闭开启在后台的终端；在终端下也可以使用exit进行退出 快捷键操作 C-b c #创建一个新的窗口C-b n #切换到下一个窗口C-b p #切换到上一个窗口C-b d #临时断开会话，还可以连上的C-b pageup/pagedown #向上或向下翻页 参考教程使用大全（桔子空间）不明觉厉（命令行神器） Tmux源码安装 yum install tmux -y #wget http://downloads.sourceforge.net/tmux/tmux-1.6.tar.gzcd tmux-1.6./configuremakemake install 配置手册个性配置参考配置地址二","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"},{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"}]},{"title":"Markdown Introduction","slug":"Markdown-Introduction","date":"2015-07-21T00:17:54.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/07/21/Markdown-Introduction/","link":"","permalink":"http://arvon.top/2015/07/21/Markdown-Introduction/","excerpt":"简介：Markdown是一个web上使用的文本到HTML的转换工具，目前github、Stackoverflow等网站均支持这种格式，还有简书也是很支持的呦。它存在的宗旨就是实现易读易写，称为适用于网络的书写语言。自我感觉也确实不错，很适合做IT的人记笔记用。","text":"简介：Markdown是一个web上使用的文本到HTML的转换工具，目前github、Stackoverflow等网站均支持这种格式，还有简书也是很支持的呦。它存在的宗旨就是实现易读易写，称为适用于网络的书写语言。自我感觉也确实不错，很适合做IT的人记笔记用。 语法介绍基本介绍 在Markdown中，、-、+3个符号的效果一致，称为Markdown符号，我习惯用\\号 空白行表示另起一个段落 -是表示inline代码，tab是用来标记代码段，分别对应html的code、pre标签 标题 可以用#、-、=一样的效果，个人偏爱# # 一级标题 ## 二级标题 … 以此类推 ###### 六级标题 文本强调 斜体 or 强调 加粗 or 加粗 粗斜体 or 粗斜体 标签 like this tag: [tag1, tag2] List列表 *无序列表（unordered） +无序列表 -无序列表 第一行（ordered有序列表） 2.或- 第二行 3.或- 第三行 删除线~~文字~~ 效果为 文字 组合用法 产品介绍（子项目符号） 此时子项，要以一个制表符或者四个空格缩进 产品特点 特点1 特点2 特点3 产品功能 功能1 功能2 功能3 可能有时会出现这样的情况，首行内容是以日期或数字开头：2013.公司的年度目标，为了避免也转换成有序列表，可以在.前面加上\\:2013. 公司的年度目标 Links网站链接 Inline-style 内嵌方式：[link text](https://www.google.com “title text”) Reference-style 引用方式：[link text][id][id]: https://www.mozilla.org “title text” Relative reference to a repository file 引用存储文件：[link text](../path/file/readme.text “title text”) 还能这样使用：[link text][][link text]: http://www.reddit.com Email 邮件：&#101;&#120;&#x61;&#109;&#112;&#x6c;&#x65;&#64;&#x65;&#x78;&#97;&#x6d;&#112;&#108;&#101;&#x2e;&#99;&#111;&#109; images图片链接图片外链必须记着，很好用贴图 Inline-style 内嵌方式：![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png “title text”) Reference-style 引用方式：![alt text][logo][logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png “title text”Inline-style 内嵌方式： Reference-style 引用方式：![alt text][logo] [logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png “title text” ###code and Syntax HIghlighting 代码语法高亮： 使用`` 包裹起来 ``效果为： 包裹起来 语法高亮 例如： 12s = \"Python syntax highlighting\"print s Block Code代码分组（代码区块）：在该行开头缩进4个空格或一个制表符(tab) Blockquotes 引用： Email-style angle bracketsare used for blockquotes. And, they can be nested. Headers in blockquotes You can quote a list. Etc. Hard LIne Breaks换行：在一行的结尾处加上2个或2个以上的空格，也可以使用标签第一行文字，第二行文字 ###水平分割线：**\\ - - - Escape charecter转义符（反斜杠）：Markdown 可以利用反斜杠来插入一些在语法中有其它意义的符号，例如：如果你想要用星号加在文字旁边的方式来做出强调效果，你可以在星号的前面加上反斜杠：*literal asterisks*Markdown 支持以下这些符号前面加上反斜杠来帮助插入普通的符号：\\反斜杠 `反引号 *星号 _下划线 {}花括号 []方括号 ()括弧 #井字号 +加号 -减号 .英文句 !感叹号 参考文档Markdown官网Markdown语法说明参照地址good","categories":[],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://arvon.top/tags/Hexo/"},{"name":"运维工具","slug":"运维工具","permalink":"http://arvon.top/tags/运维工具/"}]},{"title":"Hexo简明","slug":"Hexo简明","date":"2015-07-20T08:42:47.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/07/20/Hexo简明/","link":"","permalink":"http://arvon.top/2015/07/20/Hexo简明/","excerpt":"简介：出自台湾某大学生之手，说是叫tommy351，感觉是叼了个扎天。是一个基于Node.js的静态博客程序，其编译上百篇文字只需要几秒。hexo生成静态网页可以直接放到Github Pages，BAE，SAE等平台上。 ###常用的命令 hexo n “我的博客” == hexo new “我的博客” #新建文章 hexo p == hexo publish #将_drafts下的文件放到_post下，也就是发布草稿 hexo g == hexo generate #生成静态网页 hexo d == hexo deploy #发布到远程服务器，开启–generate选项可以在deploy前自动generate","text":"简介：出自台湾某大学生之手，说是叫tommy351，感觉是叼了个扎天。是一个基于Node.js的静态博客程序，其编译上百篇文字只需要几秒。hexo生成静态网页可以直接放到Github Pages，BAE，SAE等平台上。 ###常用的命令 hexo n “我的博客” == hexo new “我的博客” #新建文章 hexo p == hexo publish #将_drafts下的文件放到_post下，也就是发布草稿 hexo g == hexo generate #生成静态网页 hexo d == hexo deploy #发布到远程服务器，开启–generate选项可以在deploy前自动generate ###服务器命令 hexo server #Hexo 会监视文件变动并自动更新，您无须重启服务器。 hexo server -s #静态模式 hexo server -p 5000 #更改端口 hexo server -i 192.168.1.1 #自定义 IP hexo clean #清除缓存 网页正常情况下可以忽略此条命令 hexo g #生成静态网页 hexo d #开始部署 ###监视文件是否变动 hexo generate #使hexo生成静态文件快速而且简单 hexo generate –watch #监视文件变动 ###完成后部署 hexo deploy -g 或hexo server -g hexo henerate –deploy 或hexo deploy –generate ###草稿 hexo publish [layout] ###模板 hexo new “postName” #新建文章 hexo new page “pageName” #新建页面 hexo generate #生成静态页面至public目录 hexo server #开启预览访问端口（默认端口4000，’ctrl + c’关闭server） hexo deploy #将.deploy目录部署到GitHub hexo new [layout] hexo new photo “My Gallery” hexo new “Hello World” –lang tw 问题记录 Question：如何设置首页的卡片摘要效果Answer:将_config.yml文件中的index：下的expand设置为false，然后在文中添加&lt;!–more–&gt;字段来截取摘要 Question:出现了很大的问题，描述：本人准备做一个看起来很厉害的简历，but我按教程做的结果是hexo反复报错报错如下1234567891011121314151617181920FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlError: expected end of comment, got end of file at Tokenizer.nextToken (/home/fenghao/blog/node_modules/hexo/node_modules/nunjucks/src/lexer.js:282:23) at Object.extend.nextToken (/home/fenghao/blog/node_modules/hexo/node_modules/nunjucks/src/parser.js:32:27) at Object.extend.peekToken (/home/fenghao/blog/node_modules/hexo/node_modules/nunjucks/src/parser.js:44:43) at Object.extend.parseNodes (/home/fenghao/blog/node_modules/hexo/node_modules/nunjucks/src/parser.js:1124:38) at Object.extend.parseAsRoot (/home/fenghao/blog/node_modules/hexo/node_modules/nunjucks/src/parser.js:1177:42) at Object.module.exports.parse (/home/fenghao/blog/node_modules/hexo/node_modules/nunjucks/src/parser.js:1199:18) at Object.module.exports.compile (/home/fenghao/blog/node_modules/hexo/node_modules/nunjucks/src/compiler.js:1118:48) at Obj.extend._compile (/home/fenghao/blog/node_modules/hexo/node_modules/nunjucks/src/environment.js:444:35) at Obj.extend.compile (/home/fenghao/blog/node_modules/hexo/node_modules/nunjucks/src/environment.js:433:18) at null.&lt;anonymous&gt; (/home/fenghao/blog/node_modules/hexo/node_modules/nunjucks/src/environment.js:378:22) at Object.exports.withPrettyErrors (/home/fenghao/blog/node_modules/hexo/node_modules/nunjucks/src/lib.js:24:16) at Obj.extend.render (/home/fenghao/blog/node_modules/hexo/node_modules/nunjucks/src/environment.js:374:20) at Obj.extend.renderString (/home/fenghao/blog/node_modules/hexo/node_modules/nunjucks/src/environment.js:261:21) at /home/fenghao/blog/node_modules/hexo/lib/extend/tag.js:56:9 at tryCatcher (/home/fenghao/blog/node_modules/hexo/node_modules/bluebird/js/main/util.js:24:31) at Promise._resolveFromResolver (/home/fenghao/blog/node_modules/hexo/node_modules/bluebird/js/main/promise.js:427:31) at new Promise (/home/fenghao/blog/node_modules/hexo/node_modules/bluebird/js/main/promise.js:53:37) at Tag.render (/home/fenghao/blog/node_modules/hexo/lib/extend/tag.js:55:10) 诸如此类报错，还原之前备份后再倒入md文件，问题依旧，也怪自己没想法，本该料到是markdown文件出错了的，ba~la~ba~la,最后还是google拯救了我，不得不吐槽一下某度，真不是东西，屁都查不出来、、、解决方法只要将一个markdown文件中的内容去掉以后就好了，好像是#的问题，真是邪了门了，我都打不出来，只能嵌在代码里了。他们讲版本2.8.3没有这个问题。1“Error $&#123;#v0&#125; $&#123;#v1&#125;” ###摘抄地址 地址一：Hexo命令详解 地址二：Hexo搭建博客","categories":[],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://arvon.top/tags/Hexo/"}]},{"title":"使用Hexo和Github搭建Blog","slug":"使用Hexo和Github搭建Blog","date":"2015-07-17T04:10:00.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/07/17/使用Hexo和Github搭建Blog/","link":"","permalink":"http://arvon.top/2015/07/17/使用Hexo和Github搭建Blog/","excerpt":"Update:添加了云音乐外链，方法非常简单粗暴，直接在网易云音乐上生成外链然后放进文章里就行了，如图Tips：最近几天都在折腾这个东西，网上有很多的教程，但终究还是有点费劲，在这里总结一下。我是在win7上做的，我尽可能把我出问题的地方着重描述，希望让有兴趣玩这个伙伴们可以顺利点，嘻嘻。特别鸣谢陈素封的博客，帮助很大，话不多说，开始整理。win上多有不便，还会在linux上进行搭建。","text":"Update:添加了云音乐外链，方法非常简单粗暴，直接在网易云音乐上生成外链然后放进文章里就行了，如图Tips：最近几天都在折腾这个东西，网上有很多的教程，但终究还是有点费劲，在这里总结一下。我是在win7上做的，我尽可能把我出问题的地方着重描述，希望让有兴趣玩这个伙伴们可以顺利点，嘻嘻。特别鸣谢陈素封的博客，帮助很大，话不多说，开始整理。win上多有不便，还会在linux上进行搭建。 更新记录 2015/07/17第一次记录，因对markdown语法不熟练，Blog功能尚不完善，所以待补充修改 准备工作 github账户设置平台为win，需注册github account登陆github后需要创建一个repository,然后就写个REpositoryname，注意要和Owner的name一致（注意），然后勾选那个Initaalize this…什么的选项，然后就这样成了，不然就是像我一样不停的犯错，为何还有待考量，反正现在听话就ok。 软件设置需要的软件有node.js和git，实际最好再来个Visual Studio Code，你会发现有语法高亮就是舒心 待补充昂为了可以清晰一点，决定把Markdown和这个Blog分开记录。恩，就是这样 开始搭建安装node.js软件 下载地址戳：node.js,然后就开始一路下一步吧，然后你的发现带cmd多了一条np 安装git软件 下载地址戳：git,依旧下一步到底。昨天做着的时候没记录，真是一大败笔 安装Hexo 进入Git bash,输入命令 123456npm install -g hexo #hexo表示全局安装hexo#创建hexo文件夹，如/user/arvon/hexo-lxchexo init #初始化hexonpm installnpm generatenpm server 然后可以通过访问localhost查看预览http：//localhost:4000/ 部署 将在github上创建的Repository，找到https那个url将其复制 然后编辑位于你创建的文件夹下的文件，例如我就是：/user/arvon/hexo-lxc/_config.yml 123456#deploy:type: gitrepository: https://github.com/arvon2014/arvon2014.github.combranch: mastergithub上的地址：https://github.com/Arvon2014/arvon2014.github.com.git#web上访问的为：http://arvon2014.github.io 设置ssh免密码访问 先查看本地是否已经存在ssh文件，如不存在直接跳过下面第二步 删除已有的ssh的文件 输入命令 12345ssh-keygen -t rsa -C \"邮件地址@youremail.com\" Generating public/private rsa key pair. Enter file in which to save the key (/Users/your_user_directory/.ssh/id_rsa):Enter passphrase (empty for no passphrase):&lt;输入加密串&gt; Enter same passphrase again:&lt;再次输入加密串&gt;ssh -T git@github.com #测试The authenticity of host 'github.com (207.97.227.239)' can't be established. RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48. Are you sure you want to continue connecting (yes/no)? #说明成功 $ git config --global user.name \"你的名字\" $ git config --global user.email \"your_email@youremail.com\" #设置你的信息 现在已经可以顺利将环境搭建完毕 进入你的Hexo目录，开始试验吧123hexo generate = hexo ghexo server = hexo s #部署本地预览hexo deploy = hexo d #生成部署 以上，歌曲为一万次悲伤，推荐使用网易云音乐生成外链","categories":[],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://arvon.top/tags/Hexo/"}]},{"title":"安装hexo记录","slug":"CentOS安装hexo记录","date":"2015-06-13T02:00:02.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/06/13/CentOS安装hexo记录/","link":"","permalink":"http://arvon.top/2015/06/13/CentOS安装hexo记录/","excerpt":"简要记录一下在centos上安装hexo的过程记录在CentOS环境下配置hexo博客环境记录 Install Git12yum remove gityum install git-core","text":"简要记录一下在centos上安装hexo的过程记录在CentOS环境下配置hexo博客环境记录 Install Git12yum remove gityum install git-core Install node.js1234wget -qO- https://raw.github.com/creationix/nvm/master/install.sh | sh#需要重新加载环境变量nvm install 0.10nvm use 0.10 12345#mac 下的方法 curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.31.3/install.sh | bash export NVM_DIR=\"$HOME/.nvm\"[ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; . \"$NVM_DIR/nvm.sh\" # This loads nvm nvm install stable npm install -g hexo-cli Install hexo 123456 npm install hexo -g hexo init blog#初始化blog目录 hexo s --debug#测试页面","categories":[],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://arvon.top/tags/Hexo/"}]},{"title":"Linux下关于文件属性的三个时间","slug":"Linux下关于文件属性的三个时间","date":"2015-05-13T02:36:23.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/05/13/Linux下关于文件属性的三个时间/","link":"","permalink":"http://arvon.top/2015/05/13/Linux下关于文件属性的三个时间/","excerpt":"Tips: 主要区分linux下文件的三个时间属性，atime、mtime、ctime 文件的 Access time，atime 是在读取文件或者执行文件时更改的任何对inode的访问都会使此处改变。 文件的Modified time，mtime 是在写入文件时随文件内容的更改而更改的。 文件的 Change time，ctime 是在写入文件、更改所有者、权限或链接设置时随 Inode 的内容更改而更改的。只要stat出来的内容发生改变就会发生改变。mtime的改变必然导致ctime的改变。","text":"Tips: 主要区分linux下文件的三个时间属性，atime、mtime、ctime 文件的 Access time，atime 是在读取文件或者执行文件时更改的任何对inode的访问都会使此处改变。 文件的Modified time，mtime 是在写入文件时随文件内容的更改而更改的。 文件的 Change time，ctime 是在写入文件、更改所有者、权限或链接设置时随 Inode 的内容更改而更改的。只要stat出来的内容发生改变就会发生改变。mtime的改变必然导致ctime的改变。 关于directory 文件夹的 Access time，atime 是在读取文件或者执行文件时更改的（只cd进入一个目录然后cd ..不会引起atime的改变，但ls一下就不同了）。 文件夹的 Modified time，mtime 是在文件夹中有文件的新建、删除才会改变（如果只是改变文件内容不会引起mtime的改变，换句话说如果ls -f 的结果发生改变mtime就会被刷新。这里可能有人要争论了：我进入dd这个文件夹vi了一个文件然后退出，前后ls -f 的结果没有改变但是文件夹的mtime发生改变了……这点请主意vi命令在编辑文件时会在本文件夹下产生一 个”.file.swp”临时文件，该文件随着vi的退出而被删除……这就导致了mtime的改变 [Auxten:p]不信你可以用nano修改文件来试验）。 文件夹的 Change time，ctime 基本同文件的ctime，其体现的是inode的change time。 这里还要补充一点：mount -o noatime(mount -o remount,atime / 可以在线重新挂载根目录) 可以选择不记录文件的atime的改变，这意味着什么呢？当你创建了这个文件后这个文件的atime就定格了，除非你用touch或者touch -a强制刷新文件的atime。这样在可以在一定程度上提升文件系统的读写性能，特别是网站这种系统中在fstab里面加上noatime是个好主意 关于find 含义：文件的 Access time，atime 是在读取文件或者执行文件时更改的；文件的 Modified time，mtime 是在写入文件时随文件内容的更改而更改的；文件的 Create time，ctime 是在写入文件、更改所有者、权限或链接设置时随 Inode 的内容更改而更改的。 文件各种事件标记的显示方法ls -lc filename 列出文件的 ctimels -lu filename 列出文件的 atimels -l filename 列出文件的 mtime 以上","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"}]},{"title":"Linux查看主机信息","slug":"Linux查看主机信息","date":"2015-04-13T03:08:25.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/04/13/Linux查看主机信息/","link":"","permalink":"http://arvon.top/2015/04/13/Linux查看主机信息/","excerpt":"这篇包括（系统版本、内核信息、核心数、主机名）还有一个关于系统信息查看的，–&gt;飞机票","text":"这篇包括（系统版本、内核信息、核心数、主机名）还有一个关于系统信息查看的，–&gt;飞机票 查看主机版本信息 系统信息 123456789101112131415[root@localhost ~]# cat /etc/redhat-releaseCentOS release 6.7 (Final)#or[root@localhost ~]# cat /etc/issueCentOS release 6.7 (Final)Kernel \\r on an \\m#or[root@localhost ~]# lsb_release -aLSB Version: :base-4.0-ia32:base-4.0-noarch:core-4.0-ia32:core-4.0-noarchDistributor ID: CentOSDescription: CentOS release 6.7 (Final)Release: 6.7Codename: Final#yum provides */lsb_release#yum install -y redhat-lsb-core-4.0-7.el6.centos.i686 内核信息 12345678[root@localhost ~]# cat /proc/versionLinux version 2.6.32-573.el6.x86_64 (mockbuild@c6b9.bsys.dev.centos.org) (gcc version 4.4.7 20120313 (Red Hat 4.4.7-16) (GCC) ) #1 SMP Thu Jul 23 15:44:03 UTC 2015#or[root@localhost ~]# uname -aLinux localhost.localdomain 2.6.32-573.el6.x86_64 #1 SMP Thu Jul 23 15:44:03 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux#or[root@localhost ~]# uname -r2.6.32-573.el6.x86_64 查看当前运行环境是32还是64，是32也不代表不支持64 12345678910111213141516171819202122[root@localhost ~]# getconf LONG_BIT64#or[root@localhost ~]# file /sbin/init/sbin/init: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.18, stripped[root@localhost ~]# file /bin/ls/bin/ls: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.18, stripped#or[root@localhost ~]# uname -aLinux localhost.localdomain 2.6.32-573.el6.x86_64 #1 SMP Thu Jul 23 15:44:03 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux[root@localhost ~]# uname -mx86_64#or[root@localhost ~]# archx86_64#or[root@localhost ~]# echo $HOSTTYPEx86_64#or[root@localhost ~]# getconf -a#or[root@localhost ~]# more /proc/cpuinfo 查看其他 查看cpu核心数1cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 以上","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"}]},{"title":"top输出详解","slug":"top输出详解","date":"2015-03-30T04:10:00.000Z","updated":"2018-02-23T12:14:49.000Z","comments":true,"path":"2015/03/30/top输出详解/","link":"","permalink":"http://arvon.top/2015/03/30/top输出详解/","excerpt":"","text":"常用快捷 按键“1”：查看每颗cpu负载按键“c”：查看进程启动命令 top输出解释 Introduction title task explanation 02:41:47 系统当前时间 up 系统运行时间64天 user 当前登录用户数,可以使用w系统命令进行查看具体用户登录信息 load average 系统cpu负载，表示任务队列1分钟、5分钟、15分钟的平均负载,该数值理论上应该小于cpu核心数 Tasks 系统当前共有99个进程，1个处于运行状态（running），98个处于睡眠状态（sleeping），0个处于停止状态（stopped），0个处于僵尸状态（zombie） us user,用户进程占用cpu sy system，系统进程占用cpu ni nice，用户的进程nice值，表示优先级，没有改变过就是0 id idle，处于空闲的cpu wa wait，等待的io输入输出 hi hardware interrupt,硬件中断请求占cpu的时间 si software interrupt,软件中断请求占cpu的时间 st steal time,虚拟服务占用cpu时间的百分比，一般应用的机器上都是0， IBM解释为当 hypervisor 服务另一个虚拟处理器的时候，虚拟 CPU 等待实际 CPU 的时间的百分比 Mem 总共的内存为16，已使用15G，还剩1G空闲，buffers和cached都属于缓存，后面写buffer和cache的区别 Swap 交换分区大小为0 Steal 值比较高的话，你需要向主机供应商申请扩容虚拟机。服务器上的另一个虚拟机可能拥有更大更多的 CPU时间片，你可能需要申请升级以与之竞争。另外，高 steal 值可能意味着主机供应商在服务器上过量地出售虚拟机。如果升级了虚拟机， steal 值还是不降的话，你应该寻找另一家服务供应商。低 steal 值意味着你的应用程序在目前的虚拟机上运作良好。因为你的虚拟机不会经常地为了 CPU 时间与其它虚拟机激烈竞争，你的虚拟机会更快地响应。这一点也暗示了，你的主机供应商没有过量地出售虚拟服务，绝对是一件好事情。 processes info name explanation PID 进程标识符 USER 进程所有者 PR 进程执行优先级 NI nice值，负值表示高优先级 VIRT 进程使用的虚拟内存,单位kb,VIRT=swap+RES RES 进程使用的未被换出的内存大小，RES=CODE+DATA SHR 共享内存大小 S 进程状态 %CPU 上次更新到现在的cpu时间占用百分比 %MEM 进程使用武力内存百分比 TIME+ 进程使用cpu内存总计，单位1/100s COMMAND 进程被执行的命令名称","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"}]},{"title":"Linux下hexo配置","slug":"Linux下hexo配置","date":"2015-03-13T03:25:11.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/03/13/Linux下hexo配置/","link":"","permalink":"http://arvon.top/2015/03/13/Linux下hexo配置/","excerpt":"本地环境 CentOS6 下载nvm（Node.js的版本管理器）软件wget -qO- https://raw.github.com/creationix/nvm/master/install.sh | sh #下载安装nvm（安装目录在/root/.nvm/） 安装nvm软件nvm install 0.10 #选择Node.js的安装版本nvm ls #查看node.js的版本","text":"本地环境 CentOS6 下载nvm（Node.js的版本管理器）软件wget -qO- https://raw.github.com/creationix/nvm/master/install.sh | sh #下载安装nvm（安装目录在/root/.nvm/） 安装nvm软件nvm install 0.10 #选择Node.js的安装版本nvm ls #查看node.js的版本 安装Hexo 安装hexo 12345npm install -g hexo #使用npm安装hexo(此处安装并未完成)#如果显示command not found，就重启终端，然后使用nvm ls检查当前版本，如果没有使用命令nvm use 0.10 #即使用方才装的版本（使用此命令后安装成功）#或使用一下命令直接设置全局默认的node.js版本nvm alias default 0.10.26 安装失败的原因可能是无法连接到官方服务器，或者是GFW（Great Firewall，中国国家防火墙或长城防火墙）的问题。需修改npm镜像源使用config命令 1234$ npm config set registry http://registry.cnpmjs.org $ npm info underscore (如果上面配置成功，使用这个命令会有以下提示) $ npm http GET http://registry.cnpmjs.org/underscore $ npm http 200 http://registry.cnpmjs.org/underscore 命令行指定 123$ npm --registry http://registry.cnpmjs.org info underscore 编辑 ~/.npmrc 加入以下内容可以使配置永久生效，就不用每次npm安装时都要运行指定源命令了 $ registry = http://registry.cnpmjs.org 使用以下命令对hexo进行初始化这里我位于~目录，而且我想把我的个人博客放在~/hexo目录，需要放在其他目录直接改一下自己需要的目录就可以了 12$ hexo install hexo --save$ hexo init hexo 本地环境到此结束，有木有很easy 初步预览 12hexo generate 或 hexo g #生成静态文件hexo server 或 hexo s #打开本地预览（http:localhost:4000） 配置git并发布配置git并发布基于hexo和github的个人博客首先编辑个人安装目录的_config.yml文件，找到一下内容并修改为git deploy:type: gitrepository: https://github.com/Arvon2014/arvon2014.github.com.git(直接将github上的https项复制并粘贴)branch: master运行以下命令设置git全局变量，即设置用户名和邮箱(需要先和github建立免密码登陆)git config –global user.name “Arvon”git config –global user.email “yafeng2011@126.com” 免密码ssh公钥设置 是否安装了ssh，未装执行 yum install ssh 检查公钥 cd ~/.ssh #存在就删了 生成公钥私钥对 ssh-keygen -t rsa -C “you_email@youremail.com”cd ~/.ssh #验证 添加ssh公钥到github 将公钥（pub）的内容复制到Deploykeys里面 测试是否生效 ssh -T git@github.com","categories":[],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://arvon.top/tags/Hexo/"}]},{"title":"Centos搭建nginx+php环境","slug":"Centos搭建nginx-php环境","date":"2015-02-13T04:58:28.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/02/13/Centos搭建nginx-php环境/","link":"","permalink":"http://arvon.top/2015/02/13/Centos搭建nginx-php环境/","excerpt":"Tips：线上pfsense中添加bandwidthd监控流量的功能存在问题，需要配置一个支持php的web环境来进行支持，这里选择了nginx+php。 Environment yum 1wget http://mirrors.opencas.cn/epel/6/i386/epel-release-6-8.noarch.rpm CentOS6.3_x64","text":"Tips：线上pfsense中添加bandwidthd监控流量的功能存在问题，需要配置一个支持php的web环境来进行支持，这里选择了nginx+php。 Environment yum 1wget http://mirrors.opencas.cn/epel/6/i386/epel-release-6-8.noarch.rpm CentOS6.3_x64 Install software Install nginx 123456789yum install nginx#GeoIP.x86_64 0:1.6.5-1.el6 #GeoIP-GeoLite-data.noarch 0:2015.04-2.el6 #GeoIP-GeoLite-data-extra.noarch 0:2015.04-2.el6 #geoipupdate.x86_64 0:2.2.1-2.el6 #libxslt.x86_64 0:1.1.26-2.el6_3.1 #nginx-filesystem.noarch 0:1.0.15-12.el6chkconfig nginx on/etc/init.d/nginx restart Install php-fpm 1234567yum install php-fpm#Installing : php-common-5.3.3-46.el6_6.x86_64 1/2#Installing : php-fpm-5.3.3-46.el6_6.x86_64 2/2#Verifying : php-fpm-5.3.3-46.el6_6.x86_64 1/2#Verifying : php-common-5.3.3-46.el6_6.x86_64 2/2chkconfig php-fpm on/etc/init.d/php-fpm restart Edit config 备份配置文件 1cp /etc/nginx/conf.d/default.conf /tmp/ 修改配置文件 123456789101112131415161718192021#vim /etc/nginx/conf.d/default.confserver &#123; listen 80; server_name localhost; autoindex on; #charset koi8-r; #access_log /var/log/nginx/log/host.access.log main; location / &#123; root /var/www/html; index index.html index.htm index.php; &#125; location ~ \\.php$ &#123; root /var/www/html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /var/www/html$fastcgi_script_name; include fastcgi_params; &#125; &#125; Test 需确认/var/www/html存在，在此目录下创建测试文件 1234#vim /var/www/html/index.php #内容如下&lt;?php phpinfo(); ?&gt; 从浏览器访问查看效果url：IP","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://arvon.top/tags/Linux/"},{"name":"Nginx","slug":"Nginx","permalink":"http://arvon.top/tags/Nginx/"}]},{"title":"windows批量修改文件后缀","slug":"windows批量修改文件后缀","date":"2015-01-23T02:55:28.000Z","updated":"2017-05-26T13:17:57.000Z","comments":true,"path":"2015/01/23/windows批量修改文件后缀/","link":"","permalink":"http://arvon.top/2015/01/23/windows批量修改文件后缀/","excerpt":"Ren命令可用于修改文件的名称，包括后缀名。比如，将D盘中的1.jpg重命名为2.png，只需在命令提示符中输入下面的命令，然后回车即可。","text":"Ren命令可用于修改文件的名称，包括后缀名。比如，将D盘中的1.jpg重命名为2.png，只需在命令提示符中输入下面的命令，然后回车即可。 ren D:\\1.jpg 2.png注意，如果提示权限不足，那么你需要以管理员身份运行命令提示符。如果要批量修改后缀名，可借助通配符来实现。比如将D盘中的所有文件后缀名改为jpg的命令为： ren D:* *.png如果只想将某一类型文件（具有相同后缀名）修改为另一个类型，比如，将D盘中的所有jpg文件后缀修改为png，其命令为： ren D:*.jpg *.png如果你经常需要批量修改后缀名，可以把下面两条命令复制到txt文本文档中: 12cd /d %~dp0ren * *.jpg 保存后将其后缀txt修改为bat。以后只需要把这个bat文件与需要修改的文件放到同一目录中，然后以管理员身份运行这个bat文件即可将该目录下的所有文件后缀名改为jpg。 以上","categories":[],"tags":[{"name":"其他OS","slug":"其他OS","permalink":"http://arvon.top/tags/其他OS/"}]}]}